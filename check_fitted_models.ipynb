{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "644bfd56-adb4-4df5-962c-366082eb3081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_fitted_models_for_msa(msa_name, min_timestring=None,\n",
    "                                        max_timestring=None,\n",
    "                                        timestrings=None,\n",
    "                                       required_properties=None,\n",
    "                                       required_model_kwargs=None,\n",
    "                                       recompute_losses=False,\n",
    "                                       key_to_sort_by=None,\n",
    "                                       old_directory=False):\n",
    "\n",
    "    \"\"\"\n",
    "    required_properties refers to params that are defined in data_and_model_kwargs, outside of ‘model_kwargs’ and ‘data_kwargs`\n",
    "    E.g., required_properties={'experiment_to_run':'normal_grid_search'})\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    pd.set_option('max_columns', 50)\n",
    "    pd.set_option('display.width', 500)\n",
    "\n",
    "    if required_model_kwargs is None:\n",
    "        required_model_kwargs = {}\n",
    "    if required_properties is None:\n",
    "        required_properties = {}\n",
    "\n",
    "    if timestrings is None:\n",
    "        timestrings = filter_timestrings_for_properties(\n",
    "            required_properties=required_properties,\n",
    "            required_model_kwargs=required_model_kwargs,\n",
    "            required_data_kwargs={'MSA_name':msa_name},\n",
    "            min_timestring=min_timestring,\n",
    "            max_timestring=max_timestring,\n",
    "            old_directory=old_directory)\n",
    "        print('Found %d fitted models for %s' % (len(timestrings), msa_name))\n",
    "    else:\n",
    "        # sometimes we may wish to pass in a list of timestrings to evaluate models\n",
    "        # so we don't have to call filter_timestrings_for_properties a lot.\n",
    "        assert min_timestring is None\n",
    "        assert max_timestring is None\n",
    "        assert required_model_kwargs == {}\n",
    "\n",
    "    if recompute_losses:\n",
    "        nyt_outcomes, _, _, _, _ = get_variables_for_evaluating_msa_model(msa_name)\n",
    "\n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "    for ts in timestrings:\n",
    "        try:\n",
    "            _, kwargs, _, model_results, fast_to_load_results = load_model_and_data_from_timestring(ts,\n",
    "                verbose=False,\n",
    "                load_fast_results_only=(not recompute_losses), old_directory=old_directory)\n",
    "            model_kwargs = kwargs['model_kwargs']\n",
    "            exo_kwargs = model_kwargs['exogenous_model_kwargs']\n",
    "            data_kwargs = kwargs['data_kwargs']\n",
    "            experiment_to_run = kwargs['experiment_to_run']\n",
    "            assert data_kwargs['MSA_name'] == msa_name\n",
    "\n",
    "            if recompute_losses:\n",
    "                fast_to_load_results['loss_dict'] = compare_model_vs_real_num_cases(nyt_outcomes,\n",
    "                                                       model_kwargs['min_datetime'],\n",
    "                                                       model_results=model_results,\n",
    "                                                       make_plot=False)\n",
    "\n",
    "            results_for_ts = {'timestring':ts,\n",
    "                             'data_kwargs':data_kwargs,\n",
    "                             'model_kwargs':model_kwargs,\n",
    "                             'results':model_results,\n",
    "                             'experiment_to_run':experiment_to_run}\n",
    "        except Exception as e:\n",
    "            print(\"Error in load_model_and_data_from_timestring():\", e, ts)\n",
    "            continue\n",
    "\n",
    "        if 'final infected fraction' in fast_to_load_results:\n",
    "            results_for_ts['final infected fraction'] = fast_to_load_results['final infected fraction']\n",
    "\n",
    "        for result_type in ['loss_dict', 'train_loss_dict', 'test_loss_dict', 'ses_race_summary_results', 'estimated_R0', 'clipping_monitor', 'agg_county_loss_dict']:\n",
    "            if (result_type in fast_to_load_results) and (fast_to_load_results[result_type] is not None):\n",
    "                for k in fast_to_load_results[result_type]:\n",
    "                    full_key = result_type + '_' + k\n",
    "                    assert full_key not in results_for_ts\n",
    "                    results_for_ts[full_key] = fast_to_load_results[result_type][k]\n",
    "\n",
    "        for k in exo_kwargs:\n",
    "            assert k not in results_for_ts\n",
    "            results_for_ts[k] = exo_kwargs[k]\n",
    "        for k in model_kwargs:\n",
    "            if k == 'exogenous_model_kwargs':\n",
    "                continue\n",
    "            else:\n",
    "                assert k not in results_for_ts\n",
    "                results_for_ts[k] = model_kwargs[k]\n",
    "        results.append(results_for_ts)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print('Time to load and score all models: %.3fs -> %.3fs per model' %\n",
    "          (end_time-start_time, (end_time-start_time)/len(timestrings)))\n",
    "    results = pd.DataFrame(results)\n",
    "\n",
    "    if key_to_sort_by is not None:\n",
    "        results = results.sort_values(by=key_to_sort_by)\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def load_model_and_data_from_timestring(timestring, verbose=False, load_original_data=False,\n",
    "                                        load_full_model=False, load_fast_results_only=True,\n",
    "                                        load_filtered_data_model_was_fitted_on=False,\n",
    "                                        old_directory=False):\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Loading model from timestring %s\" % timestring)\n",
    "    if old_directory:\n",
    "        model_dir = OLD_FITTED_MODEL_DIR\n",
    "    else:\n",
    "        model_dir = FITTED_MODEL_DIR\n",
    "    f = open(os.path.join(model_dir, 'data_and_model_configs', 'config_%s.pkl' % timestring), 'rb')\n",
    "    data_and_model_kwargs = pickle.load(f)\n",
    "    f.close()\n",
    "    model = None\n",
    "    model_results = None\n",
    "    f = open(os.path.join(model_dir, 'fast_to_load_results_only', 'fast_to_load_results_%s.pkl' % timestring), 'rb')\n",
    "    fast_to_load_results = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    if not load_fast_results_only:\n",
    "        if SAVE_MODEL_RESULTS_SEPARATELY:\n",
    "            f = open(os.path.join(helper.FITTED_MODEL_DIR, 'model_results', 'model_results_%s.pkl' % timestring), 'rb')\n",
    "            model_results = pickle.load(f)\n",
    "            f.close()\n",
    "\n",
    "        if load_full_model:\n",
    "            f = open(os.path.join(model_dir, 'full_models', 'fitted_model_%s.pkl' % timestring), 'rb')\n",
    "            model = pickle.load(f)\n",
    "            f.close()\n",
    "\n",
    "    if load_original_data:\n",
    "        if verbose:\n",
    "            print(\"Loading original data as well...warning, this may take a while\")\n",
    "        d = helper.load_dataframe_for_individual_msa(**data_and_model_kwargs['data_kwargs'])\n",
    "    else:\n",
    "        d = None\n",
    "\n",
    "    if load_filtered_data_model_was_fitted_on:\n",
    "        # if true, return the data after all the filtering, along with the model prior to fitting.\n",
    "        data_kwargs = data_and_model_kwargs['data_kwargs'].copy()\n",
    "        model_kwargs = data_and_model_kwargs['model_kwargs'].copy()\n",
    "        model_kwargs['return_model_and_data_without_fitting'] = True\n",
    "        unfitted_model = fit_and_save_one_model(timestring=None,\n",
    "                                     model_kwargs=model_kwargs,\n",
    "                                     data_kwargs=data_kwargs,\n",
    "                                     train_test_partition=None)\n",
    "        filtered_data = unfitted_model.d\n",
    "        return model, data_and_model_kwargs, d, model_results, fast_to_load_results, filtered_data, unfitted_model\n",
    "\n",
    "    else:\n",
    "        return model, data_and_model_kwargs, d, model_results, fast_to_load_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "160eb8e1-f053-48d9-a28c-07dba2a537cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from covid_constants_and_util import *\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib import ticker as tick\n",
    "from collections import Counter \n",
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import geopandas as gpd\n",
    "# import helper_methods_for_aggregate_data_analysis as helper\n",
    "\n",
    "# from model_experiments import *\n",
    "# from model_results import *\n",
    "\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e845143-8f38-4a24-b20b-f8fcaa977880",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLD_FITTED_MODEL_DIR = r'J:\\extra_safegraph_aggregate_models'\n",
    "FITTED_MODEL_DIR = OLD_FITTED_MODEL_DIR\n",
    "def filter_timestrings_for_properties(required_properties=None,\n",
    "                                      required_model_kwargs=None,\n",
    "                                      required_data_kwargs=None,\n",
    "                                      min_timestring=None,\n",
    "                                      max_timestring=None,\n",
    "                                      return_msa_names=False,\n",
    "                                      old_directory=False):\n",
    "    \"\"\"\n",
    "    required_properties refers to params that are defined in data_and_model_kwargs, outside of ‘model_kwargs’ and ‘data_kwargs\n",
    "    \"\"\"\n",
    "    if required_properties is None:\n",
    "        required_properties = {}\n",
    "    if required_model_kwargs is None:\n",
    "        required_model_kwargs = {}\n",
    "    if required_data_kwargs is None:\n",
    "        required_data_kwargs = {}\n",
    "    if max_timestring is None:\n",
    "        max_timestring = str(datetime.datetime.now()).replace(' ', '_').replace('-', '_').replace('.', '_').replace(':', '_')\n",
    "    print(\"Loading models with timestrings between %s and %s\" % (str(min_timestring), max_timestring))\n",
    "    if old_directory:\n",
    "        config_dir = os.path.join(OLD_FITTED_MODEL_DIR, 'data_and_model_configs')\n",
    "    else:\n",
    "        config_dir = os.path.join(FITTED_MODEL_DIR, 'data_and_model_configs')\n",
    "    matched_timestrings = []\n",
    "    msa_names = []\n",
    "    configs_to_evaluate = os.listdir(config_dir)\n",
    "    print(\"%i files in directory %s\" % (len(configs_to_evaluate), config_dir))\n",
    "    for fn in configs_to_evaluate:\n",
    "        if fn.startswith('config_'):\n",
    "            timestring = fn.lstrip('config_').rstrip('.pkl')\n",
    "            if (timestring <= max_timestring) and (min_timestring is None or timestring >= min_timestring):\n",
    "                f = open(os.path.join(config_dir, fn), 'rb')\n",
    "                data_and_model_kwargs = pickle.load(f)\n",
    "                f.close()\n",
    "                if test_if_kwargs_match(required_properties,\n",
    "                                        required_data_kwargs,\n",
    "                                        required_model_kwargs,\n",
    "                                        data_and_model_kwargs):\n",
    "                    matched_timestrings.append(timestring)\n",
    "                    msa_names.append(data_and_model_kwargs['data_kwargs']['MSA_name'])\n",
    "    if not return_msa_names:\n",
    "        return matched_timestrings\n",
    "    else:\n",
    "        return matched_timestrings, msa_names\n",
    "\n",
    "    return matched_timestrings\n",
    "\n",
    "def test_if_kwargs_match(req_properties, req_data_kwargs,\n",
    "                         req_model_kwargs, test_data_and_model_kwargs):\n",
    "    # check whether direct properties in test_data_and_model_kwargs match\n",
    "    prop_match = all([req_properties[key] == test_data_and_model_kwargs[key] for key in req_properties if key not in ['data_kwargs', 'model_kwargs']])\n",
    "    if not prop_match:\n",
    "        return False\n",
    "\n",
    "    # check whether data kwargs match\n",
    "    test_data_kwargs = test_data_and_model_kwargs['data_kwargs']\n",
    "    data_match = all([req_data_kwargs[key] == test_data_kwargs[key] for key in req_data_kwargs])\n",
    "    if not data_match:\n",
    "        return False\n",
    "\n",
    "    # check if non-dictionary model kwargs match\n",
    "    kwargs_keys = set([key for key in req_model_kwargs if 'kwargs' in key])\n",
    "    test_model_kwargs = test_data_and_model_kwargs['model_kwargs']\n",
    "    model_match = all([req_model_kwargs[key] == test_model_kwargs[key] for key in req_model_kwargs if key not in kwargs_keys])\n",
    "    if not model_match:\n",
    "        return False\n",
    "\n",
    "    # check if elements within dictionary model kwargs match\n",
    "    for kw_key in kwargs_keys:\n",
    "        req_kwargs = req_model_kwargs[kw_key]\n",
    "        test_kwargs = test_model_kwargs[kw_key]\n",
    "        kw_match = all([req_kwargs[k] == test_kwargs[k] for k in req_kwargs])\n",
    "        if not kw_match:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_best_models_for_all_msas(df, key_to_sort_by, only_single_best=False, \n",
    "                                 loss_tolerance=ACCEPTABLE_LOSS_TOLERANCE, verbose=False,\n",
    "                                 max_models_to_keep=100):\n",
    "    model_str = 'model' if only_single_best else 'models'\n",
    "    if verbose:\n",
    "        print('Finding best fit %s based on key=%s, loss tolerance=%2.3f' % (model_str, key_to_sort_by, loss_tolerance))\n",
    "    df = df.sort_values(by=key_to_sort_by)\n",
    "    best_models = []\n",
    "    for msa in df.MSA_name.unique():\n",
    "        msa_df = df[df['MSA_name'] == msa]\n",
    "        if only_single_best:\n",
    "            msa_best_models = msa_df.iloc[:1]\n",
    "        else:\n",
    "            best_loss = msa_df.iloc[0][key_to_sort_by]\n",
    "            msa_best_models = msa_df[msa_df[key_to_sort_by] <= (loss_tolerance * best_loss)]\n",
    "            passed_thresh = len(msa_best_models)\n",
    "            msa_best_models = msa_best_models.iloc[:max_models_to_keep]\n",
    "        if verbose:\n",
    "            print('%s: %d within loss tol -> keeping %d' % (msa, passed_thresh, len(msa_best_models)))\n",
    "        best_models.append(msa_best_models)\n",
    "    best_models = pd.concat(best_models)\n",
    "    return best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "712b08f1-ec65-4e1a-a44b-3e4b38298b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_fit_for_msa(best_models, msa_name, ax, min_datetime, max_datetime,\n",
    "                           mode='cases', train_test_partition=None, additional_kwargs=None):\n",
    "    msa_best_models = best_models[best_models.MSA_name == msa_name]\n",
    "    nyt_outcomes, _, _, _, _ = get_variables_for_evaluating_msa_model(msa_name)  \n",
    "    lag = DEATH_LAG if mode == 'deaths' else DETECTION_LAG\n",
    "    other_plotting_kwargs =  {\n",
    "            'plot_log':False, \n",
    "            'plot_legend':True,\n",
    "            'plot_errorbars':True,\n",
    "            'xticks':[min_datetime + datetime.timedelta(days=lag),\n",
    "                      max_datetime + datetime.timedelta(days=lag)],                                                                             \n",
    "            'x_range':[min_datetime + datetime.timedelta(days=lag),\n",
    "                       max_datetime + datetime.timedelta(days=lag)],        \n",
    "            'plot_daily_not_cumulative':True,\n",
    "            'model_line_label': 'Model simulated',\n",
    "            'true_line_label': 'Reported %s' % mode,\n",
    "            'title_fontsize':30,\n",
    "            'marker_size':5,\n",
    "            'real_data_color':'tab:orange',\n",
    "            'model_color':'tab:blue',\n",
    "            'plot_mode':mode,\n",
    "    }\n",
    "    if additional_kwargs is not None:\n",
    "        for k,v in additional_kwargs.items():\n",
    "            other_plotting_kwargs[k] = v\n",
    "    score_dict = plot_best_models_fit_for_msa(msa_best_models, msa_name, ax, key_to_sort_by=None, train_test_partition=train_test_partition, \n",
    "                                 plotting_kwargs=other_plotting_kwargs, use_given_df=True)\n",
    "    ax.grid(alpha=0.1)\n",
    "    ax.set_ylabel('Daily confirmed %s' % mode, fontsize=20)\n",
    "    return score_dict\n",
    "\n",
    "def compute_rmses(pred_dates, pred_data, real_dates, real_data, train_test_partition=None, normalized=False):\n",
    "    assert pred_data.shape[1] == len(pred_dates)\n",
    "    assert len(real_data) == len(real_dates)\n",
    "    overlap_dates = sorted(set(pred_dates).intersection(set(real_dates)))\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for dt in overlap_dates:\n",
    "        if train_test_partition is None or dt >= train_test_partition:\n",
    "            pred_index = pred_dates.index(dt)\n",
    "            pred_vec = pred_data[:, pred_index]\n",
    "            y_pred.append(pred_vec)\n",
    "            real_index = real_dates.index(dt)\n",
    "            observed = real_data[real_index]\n",
    "            y_true.append(observed)\n",
    "    y_pred = np.array(y_pred).T  # num seeds x dates \n",
    "    y_true = np.array(y_true)\n",
    "    assert y_pred.shape[1] == len(y_true)\n",
    "    \n",
    "    mean_rmse_over_seeds = RMSE(y_true, y_pred)\n",
    "    rmse_of_mean = RMSE(y_true, np.mean(y_pred, axis=0))\n",
    "    if normalized:  # normalize by mean of y_true so that we can compare across time periods / MSAs \n",
    "        mean_rmse_over_seeds = mean_rmse_over_seeds / np.mean(y_true)\n",
    "        rmse_of_mean = rmse_of_mean / np.mean(y_true)\n",
    "    return mean_rmse_over_seeds, rmse_of_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa32700-8d68-45a5-aaa1-0f4e14d91543",
   "metadata": {},
   "source": [
    "## Greenville"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "80ab27b5-3389-4fa2-8373-bab3d5427f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models with timestrings between 2022_01_02_13_55203770 and 2022_01_02_17_22203770\n",
      "6476 files in directory J:\\extra_safegraph_aggregate_models\\data_and_model_configs\n",
      "Found 1058 fitted models for Greenville_Anderson_SC\n",
      "Time to load and score all models: 9.821s -> 0.009s per model\n",
      "Finding best fit models based on key=loss_dict_smoothed_daily_cases_RMSE_time_varying_cdr, loss tolerance=1.200\n",
      "Greenville_Anderson_SC: 2 within loss tol -> keeping 2\n",
      "                                                                                               home_beta  poi_psi\n",
      "258  [0.01496, 0.014773, 0.014586, 0.014399, 0.014211999999999999, 0.014025000000000001, 0.013838, 0....      5.1\n",
      "259  [0.014666666666666666, 0.014483333333333332, 0.0143, 0.014116666666666666, 0.013933333333333332,...      5.0\n",
      "0.00748 0.01496\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestring</th>\n",
       "      <th>data_kwargs</th>\n",
       "      <th>model_kwargs</th>\n",
       "      <th>results</th>\n",
       "      <th>experiment_to_run</th>\n",
       "      <th>final infected fraction</th>\n",
       "      <th>loss_dict_eval_start_time_cases</th>\n",
       "      <th>loss_dict_eval_end_time_cases</th>\n",
       "      <th>loss_dict_cumulative_predicted_cases</th>\n",
       "      <th>loss_dict_cumulative_true_cases</th>\n",
       "      <th>loss_dict_cumulative_cases_RMSE</th>\n",
       "      <th>loss_dict_cumulative_cases_MSE</th>\n",
       "      <th>loss_dict_daily_cases_RMSE</th>\n",
       "      <th>loss_dict_daily_cases_MSE</th>\n",
       "      <th>loss_dict_daily_cases_RMSE_time_varying_cdr</th>\n",
       "      <th>loss_dict_smoothed_daily_cases_RMSE_time_varying_cdr</th>\n",
       "      <th>loss_dict_daily_cases_MRE_thres-1_sum</th>\n",
       "      <th>loss_dict_daily_cases_MRE_thres-10_sum</th>\n",
       "      <th>loss_dict_daily_cases_MRE_thres-20_sum</th>\n",
       "      <th>loss_dict_daily_cases_MRE_thres-50_sum</th>\n",
       "      <th>loss_dict_daily_cases_MRE_thres-100_sum</th>\n",
       "      <th>loss_dict_daily_cases_poisson_NLL_thres-1_logsumexp</th>\n",
       "      <th>loss_dict_daily_cases_poisson_NLL_thres-1_sum</th>\n",
       "      <th>loss_dict_daily_cases_poisson_NLL_thres-10_logsumexp</th>\n",
       "      <th>loss_dict_daily_cases_poisson_NLL_thres-10_sum</th>\n",
       "      <th>...</th>\n",
       "      <th>agg_county_loss_dict_max_daily_deaths_RMSE</th>\n",
       "      <th>agg_county_loss_dict_unweighted_avg_daily_deaths_per_capita_RMSE</th>\n",
       "      <th>agg_county_loss_dict_weighted_avg_daily_deaths_per_capita_RMSE</th>\n",
       "      <th>agg_county_loss_dict_max_daily_deaths_per_capita_RMSE</th>\n",
       "      <th>home_beta</th>\n",
       "      <th>poi_psi</th>\n",
       "      <th>p_sick_at_t0</th>\n",
       "      <th>inter_cbg_gamma</th>\n",
       "      <th>just_compute_r0</th>\n",
       "      <th>min_datetime</th>\n",
       "      <th>max_datetime</th>\n",
       "      <th>model_init_kwargs</th>\n",
       "      <th>simulation_kwargs</th>\n",
       "      <th>poi_attributes_to_clip</th>\n",
       "      <th>include_cbg_prop_out</th>\n",
       "      <th>include_inter_cbg_travel</th>\n",
       "      <th>include_mask_use</th>\n",
       "      <th>counties_to_track</th>\n",
       "      <th>poi_cbg_visits_list</th>\n",
       "      <th>poi_ids</th>\n",
       "      <th>cbg_ids</th>\n",
       "      <th>MSA_name</th>\n",
       "      <th>start_beta</th>\n",
       "      <th>end_beta</th>\n",
       "      <th>beta_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>2022_01_02_14_46_37_084470_normal_grid_search</td>\n",
       "      <td>{'MSA_name': 'Greenville_Anderson_SC', 'nrows': None}</td>\n",
       "      <td>{'min_datetime': 2020-12-20 00:00:00, 'max_datetime': 2021-01-29 23:00:00, 'exogenous_model_kwar...</td>\n",
       "      <td>None</td>\n",
       "      <td>normal_grid_search</td>\n",
       "      <td>[0.15287738119616295, 0.15388530898704586, 0.15446027993266298, 0.15352468891199647, 0.153745422...</td>\n",
       "      <td>2020-12-29</td>\n",
       "      <td>2021-02-07</td>\n",
       "      <td>[[53666.4, 54492.6, 55346.4, 56270.4, 57186.0, 58092.0, 58992.6, 59895.0, 60812.399999999994, 61...</td>\n",
       "      <td>[61660, 62421, 63193, 63193, 64678, 67157, 68077, 68732, 69945, 70993, 72438, 73543, 74818, 7544...</td>\n",
       "      <td>9995.079087</td>\n",
       "      <td>9.998130e+07</td>\n",
       "      <td>403.982556</td>\n",
       "      <td>163221.5367</td>\n",
       "      <td>395.832557</td>\n",
       "      <td>68.689063</td>\n",
       "      <td>0.212114</td>\n",
       "      <td>0.212114</td>\n",
       "      <td>0.212114</td>\n",
       "      <td>0.212114</td>\n",
       "      <td>0.212114</td>\n",
       "      <td>2797.306179</td>\n",
       "      <td>88732.227431</td>\n",
       "      <td>2797.306179</td>\n",
       "      <td>88732.227431</td>\n",
       "      <td>...</td>\n",
       "      <td>6.907881</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>[0.01496, 0.014773, 0.014586, 0.014399, 0.014211999999999999, 0.014025000000000001, 0.013838, 0....</td>\n",
       "      <td>5.1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-20</td>\n",
       "      <td>2021-01-29 23:00:00</td>\n",
       "      <td>{'num_seeds': 30}</td>\n",
       "      <td>{'use_aggregate_mobility': False, 'use_home_proportion_beta': False, 'use_inter_cbg_leak_factor'...</td>\n",
       "      <td>{'clip_areas': True, 'clip_dwell_times': True, 'clip_visits': True}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[45045, 45059, 45077, 45007]</td>\n",
       "      <td>[  (1, 926)\\t0.06209145446126893\\n  (1, 913)\\t0.023949315000219972\\n  (1, 905)\\t0.24201453362046...</td>\n",
       "      <td>[sg:003ac77ae87346e59162e5b657edf9fe, sg:21da056d879c472a8d223de5fdd40e0f, sg:2ae7c73886b5424d85...</td>\n",
       "      <td>[130119704001, 130730303082, 131050001002, 131050001003, 131050002001, 131050002002, 13105000200...</td>\n",
       "      <td>Greenville_Anderson_SC</td>\n",
       "      <td>0.014960</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>2022_01_02_14_46_37_084470_normal_grid_search_normal_grid_search</td>\n",
       "      <td>{'MSA_name': 'Greenville_Anderson_SC', 'nrows': None}</td>\n",
       "      <td>{'min_datetime': 2020-12-20 00:00:00, 'max_datetime': 2021-01-29 23:00:00, 'exogenous_model_kwar...</td>\n",
       "      <td>None</td>\n",
       "      <td>normal_grid_search</td>\n",
       "      <td>[0.15034080724644233, 0.15065834439217168, 0.15169712168799981, 0.1507631263313822, 0.1499583797...</td>\n",
       "      <td>2020-12-29</td>\n",
       "      <td>2021-02-07</td>\n",
       "      <td>[[53666.4, 54496.2, 55367.4, 56239.2, 57185.4, 58060.799999999996, 58955.4, 59839.2, 60713.39999...</td>\n",
       "      <td>[61660, 62421, 63193, 63193, 64678, 67157, 68077, 68732, 69945, 70993, 72438, 73543, 74818, 7544...</td>\n",
       "      <td>10724.299661</td>\n",
       "      <td>1.150996e+08</td>\n",
       "      <td>412.959689</td>\n",
       "      <td>170554.6035</td>\n",
       "      <td>398.480109</td>\n",
       "      <td>74.461909</td>\n",
       "      <td>0.224615</td>\n",
       "      <td>0.224615</td>\n",
       "      <td>0.224615</td>\n",
       "      <td>0.224615</td>\n",
       "      <td>0.224615</td>\n",
       "      <td>2975.423462</td>\n",
       "      <td>94886.231480</td>\n",
       "      <td>2975.423462</td>\n",
       "      <td>94886.231480</td>\n",
       "      <td>...</td>\n",
       "      <td>6.984080</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>[0.014666666666666666, 0.014483333333333332, 0.0143, 0.014116666666666666, 0.013933333333333332,...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-20</td>\n",
       "      <td>2021-01-29 23:00:00</td>\n",
       "      <td>{'num_seeds': 30}</td>\n",
       "      <td>{'use_aggregate_mobility': False, 'use_home_proportion_beta': False, 'use_inter_cbg_leak_factor'...</td>\n",
       "      <td>{'clip_areas': True, 'clip_dwell_times': True, 'clip_visits': True}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[45045, 45059, 45077, 45007]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Greenville_Anderson_SC</td>\n",
       "      <td>0.014667</td>\n",
       "      <td>0.007333</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 194 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           timestring                                            data_kwargs                                                                                         model_kwargs results   experiment_to_run                                                                              final infected fraction loss_dict_eval_start_time_cases loss_dict_eval_end_time_cases  \\\n",
       "258                     2022_01_02_14_46_37_084470_normal_grid_search  {'MSA_name': 'Greenville_Anderson_SC', 'nrows': None}  {'min_datetime': 2020-12-20 00:00:00, 'max_datetime': 2021-01-29 23:00:00, 'exogenous_model_kwar...    None  normal_grid_search  [0.15287738119616295, 0.15388530898704586, 0.15446027993266298, 0.15352468891199647, 0.153745422...                      2020-12-29                    2021-02-07   \n",
       "259  2022_01_02_14_46_37_084470_normal_grid_search_normal_grid_search  {'MSA_name': 'Greenville_Anderson_SC', 'nrows': None}  {'min_datetime': 2020-12-20 00:00:00, 'max_datetime': 2021-01-29 23:00:00, 'exogenous_model_kwar...    None  normal_grid_search  [0.15034080724644233, 0.15065834439217168, 0.15169712168799981, 0.1507631263313822, 0.1499583797...                      2020-12-29                    2021-02-07   \n",
       "\n",
       "                                                                    loss_dict_cumulative_predicted_cases                                                                      loss_dict_cumulative_true_cases  loss_dict_cumulative_cases_RMSE  loss_dict_cumulative_cases_MSE  loss_dict_daily_cases_RMSE  loss_dict_daily_cases_MSE  loss_dict_daily_cases_RMSE_time_varying_cdr  loss_dict_smoothed_daily_cases_RMSE_time_varying_cdr  loss_dict_daily_cases_MRE_thres-1_sum  \\\n",
       "258  [[53666.4, 54492.6, 55346.4, 56270.4, 57186.0, 58092.0, 58992.6, 59895.0, 60812.399999999994, 61...  [61660, 62421, 63193, 63193, 64678, 67157, 68077, 68732, 69945, 70993, 72438, 73543, 74818, 7544...                      9995.079087                    9.998130e+07                  403.982556                163221.5367                                   395.832557                                             68.689063                               0.212114   \n",
       "259  [[53666.4, 54496.2, 55367.4, 56239.2, 57185.4, 58060.799999999996, 58955.4, 59839.2, 60713.39999...  [61660, 62421, 63193, 63193, 64678, 67157, 68077, 68732, 69945, 70993, 72438, 73543, 74818, 7544...                     10724.299661                    1.150996e+08                  412.959689                170554.6035                                   398.480109                                             74.461909                               0.224615   \n",
       "\n",
       "     loss_dict_daily_cases_MRE_thres-10_sum  loss_dict_daily_cases_MRE_thres-20_sum  loss_dict_daily_cases_MRE_thres-50_sum  loss_dict_daily_cases_MRE_thres-100_sum  loss_dict_daily_cases_poisson_NLL_thres-1_logsumexp  loss_dict_daily_cases_poisson_NLL_thres-1_sum  loss_dict_daily_cases_poisson_NLL_thres-10_logsumexp  loss_dict_daily_cases_poisson_NLL_thres-10_sum  ...  agg_county_loss_dict_max_daily_deaths_RMSE  agg_county_loss_dict_unweighted_avg_daily_deaths_per_capita_RMSE  \\\n",
       "258                                0.212114                                0.212114                                0.212114                                 0.212114                                          2797.306179                                   88732.227431                                           2797.306179                                    88732.227431  ...                                    6.907881                                                          0.000023   \n",
       "259                                0.224615                                0.224615                                0.224615                                 0.224615                                          2975.423462                                   94886.231480                                           2975.423462                                    94886.231480  ...                                    6.984080                                                          0.000023   \n",
       "\n",
       "     agg_county_loss_dict_weighted_avg_daily_deaths_per_capita_RMSE  agg_county_loss_dict_max_daily_deaths_per_capita_RMSE                                                                                            home_beta  poi_psi p_sick_at_t0 inter_cbg_gamma just_compute_r0 min_datetime        max_datetime  model_init_kwargs                                                                                    simulation_kwargs  \\\n",
       "258                                                        0.000018                                               0.000037  [0.01496, 0.014773, 0.014586, 0.014399, 0.014211999999999999, 0.014025000000000001, 0.013838, 0....      5.1         None            None           False   2020-12-20 2021-01-29 23:00:00  {'num_seeds': 30}  {'use_aggregate_mobility': False, 'use_home_proportion_beta': False, 'use_inter_cbg_leak_factor'...   \n",
       "259                                                        0.000018                                               0.000037  [0.014666666666666666, 0.014483333333333332, 0.0143, 0.014116666666666666, 0.013933333333333332,...      5.0         None            None           False   2020-12-20 2021-01-29 23:00:00  {'num_seeds': 30}  {'use_aggregate_mobility': False, 'use_home_proportion_beta': False, 'use_inter_cbg_leak_factor'...   \n",
       "\n",
       "                                                  poi_attributes_to_clip  include_cbg_prop_out  include_inter_cbg_travel  include_mask_use             counties_to_track                                                                                  poi_cbg_visits_list                                                                                              poi_ids                                                                                              cbg_ids                MSA_name  \\\n",
       "258  {'clip_areas': True, 'clip_dwell_times': True, 'clip_visits': True}                 False                     False              True  [45045, 45059, 45077, 45007]  [  (1, 926)\\t0.06209145446126893\\n  (1, 913)\\t0.023949315000219972\\n  (1, 905)\\t0.24201453362046...  [sg:003ac77ae87346e59162e5b657edf9fe, sg:21da056d879c472a8d223de5fdd40e0f, sg:2ae7c73886b5424d85...  [130119704001, 130730303082, 131050001002, 131050001003, 131050002001, 131050002002, 13105000200...  Greenville_Anderson_SC   \n",
       "259  {'clip_areas': True, 'clip_dwell_times': True, 'clip_visits': True}                 False                     False              True  [45045, 45059, 45077, 45007]                                                                                                 None                                                                                                 None                                                                                                 None  Greenville_Anderson_SC   \n",
       "\n",
       "     start_beta  end_beta  beta_ratio  \n",
       "258    0.014960  0.007480         0.5  \n",
       "259    0.014667  0.007333         0.5  \n",
       "\n",
       "[2 rows x 194 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# full model\n",
    "msa_name = 'Greenville_Anderson_SC'\n",
    "key_to_sort_by = 'loss_dict_smoothed_daily_cases_RMSE_time_varying_cdr'\n",
    "min_datetime = datetime.datetime(2020, 12, 20)\n",
    "max_datetime = datetime.datetime(2021, 1, 30)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "max_timestring = '2021_11_28_02_44_203770'  # group 2\n",
    "min_timestring = '2021_11_27_23_34_000000'\n",
    "\n",
    "max_timestring = '2021_11_27_23_35_000000' # group 1\n",
    "min_timestring = '2021_11_27_19_45_000000'\n",
    "\n",
    "max_timestring = '2021_11_28_13_00_203770'  # group 3   # 931 , \n",
    "min_timestring = '2021_11_28_09_10_203770'    # 931 2021_11_28_12_33_55_506980_normal_grid_search_normal_grid_search\n",
    "\n",
    "# max_timestring = '2021_11_28_13_15_203770'  # group 4  bad, not use \n",
    "# min_timestring = '2021_11_28_13_05_203770'    \n",
    "\n",
    "# max_timestring = '2021_11_28_13_44_203770'  # group 5  GOOD:  # 2021_11_28_13_18_45_911113_normal_grid_search_normal_grid_search\n",
    "# min_timestring = '2021_11_28_13_23_203770' \n",
    "\n",
    "# max_timestring = '2021_11_28_13_22203770'  # group 4.5 OOD:  # 2021_11_28_13_18_45_911113_normal_grid_search_normal_grid_search\n",
    "# min_timestring = '2021_11_28_13_16203770' \n",
    "\n",
    "max_timestring = '2022_01_02_17_22203770'  # Greenville_Anderson_SC, Best: 2022_01_02_14_46_37_084470_normal_grid_search_normal_grid_search\t\n",
    "min_timestring = '2022_01_02_13_55203770' \n",
    "\n",
    "# max_timestring = '2022_01_03_00_22203770'  # Charleston_North_Charleston_SC, Best: 2022_01_02_23_03_13_321102_normal_grid_search_normal_grid_search\t\n",
    "# min_timestring = '2022_01_02_20_55203770' \n",
    "\n",
    "# max_timestring = '2022_01_03_30_22203770'  # Charleston_North_Charleston_SC, Best: * 1.1, not good.\t\n",
    "# min_timestring = '2022_01_03_00_22203770' \n",
    "\n",
    "# max_timestring = '2022_01_03_08_30203770'  # Charleston_North_Charleston_SC, Best: * 1.02,  good.\t\n",
    "# min_timestring = '2022_01_03_08_22203770' \n",
    "\n",
    "\n",
    "\n",
    "expected_num_jobs = 1050\n",
    "gridsearch_df = evaluate_all_fitted_models_for_msa(msa_name,\n",
    "                                                   min_timestring=min_timestring,\n",
    "                                                   max_timestring=max_timestring, \n",
    "                                                   required_properties={'experiment_to_run':'normal_grid_search'})\n",
    "# assert len(gridsearch_df) == expected_num_jobs, len(gridsearch_df)\n",
    "gridsearch_df['MSA_name'] = gridsearch_df['data_kwargs'].map(lambda x:x['MSA_name'])\n",
    "gridsearch_df['start_beta'] = gridsearch_df['home_beta'].apply(lambda x:x[0])\n",
    "gridsearch_df['end_beta'] = gridsearch_df['home_beta'].apply(lambda x:x[-1])\n",
    "gridsearch_df['beta_ratio'] = gridsearch_df.end_beta.values / gridsearch_df.start_beta.values\n",
    "\n",
    "best_models_df = get_best_models_for_all_msas(gridsearch_df, key_to_sort_by, loss_tolerance=1.2, verbose=True)\n",
    "best_models_df = best_models_df.sort_values(by=key_to_sort_by)\n",
    "\n",
    "print(best_models_df[['home_beta', 'poi_psi']])\n",
    "print(best_models_df['home_beta'].iloc[0].min(), best_models_df['home_beta'].iloc[0].max())\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(7,7))\n",
    "# out = plot_model_fit_for_msa(best_models_df, msa_name, ax, min_datetime, max_datetime)\n",
    "\n",
    "# plt.show()\n",
    "best_models_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4761e1e-3837-4766-81c7-e817723d5669",
   "metadata": {},
   "source": [
    "## Columbia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "aa88d87d-1c1c-489f-ac45-fc367f6899b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variables_for_evaluating_msa_model(msa_name, verbose=False):\n",
    "    PATH_TO_ACS_5YR_DATA =r'H:\\covid_mobility_results\\new_census_data\\ACS_2019_5YR_BG\\ACS_race_cbsa_income_2019.csv'  \n",
    "\n",
    "    acs_data = pd.read_csv(r'H:\\covid_mobility_results\\new_census_data\\ACS_2019_5YR_BG\\ACS_race_cbsa_income_2019.csv')\n",
    "    acs_msas = [msa for msa in acs_data['CBSA Title'].unique() if type(msa) == str]\n",
    "    msa_matches = []\n",
    "    for single_msa in msa_name.split('+'):  # may be combination of multiple MSAs\n",
    "        msa_match = match_msa_name_to_msas_in_acs_data(single_msa, acs_msas)\n",
    "        if msa_match is None:\n",
    "            raise Exception('Could not find ACS match for %s' % single_msa)\n",
    "        if verbose: \n",
    "            print('Found MSA %s in ACS 5-year data' % msa_match)\n",
    "        msa_matches.append(msa_match)\n",
    "    msa_data = acs_data[acs_data['CBSA Title'].isin(msa_matches)].copy()\n",
    "    msa_data['id_to_match_to_safegraph_data'] = msa_data['GEOID'].map(lambda x:x.split(\"US\")[1]).astype(np.int64)\n",
    "    msa_data['id_to_match_to_safegraph_data'] = msa_data['id_to_match_to_safegraph_data'].astype(str).str.zfill(12)\n",
    "    msa_cbgs = msa_data['id_to_match_to_safegraph_data'].values\n",
    "    msa_data['fips'] = get_fips_codes_from_state_and_county_fp(msa_data.STATEFP, msa_data.COUNTYFP)\n",
    "    msa_data['fips'] = msa_data['fips'].astype(str)\n",
    "    msa_counties = list(set(msa_data['fips'].values))\n",
    "    if verbose:\n",
    "        print('Found %d counties and %d CBGs in MSA' % (len(msa_counties), len(msa_cbgs)))\n",
    "    nyt_outcomes = get_nyt_outcomes_over_counties(msa_counties)\n",
    "    nyt_counties = set(nyt_outcomes.fips.unique())\n",
    "    nyt_cbgs = msa_data[msa_data['fips'].isin(nyt_counties)]['id_to_match_to_safegraph_data'].values\n",
    "    if verbose:\n",
    "        print('Found NYT data matching %d counties and %d CBGs' % (len(nyt_counties), len(nyt_cbgs)))\n",
    "    return nyt_outcomes, nyt_counties, nyt_cbgs, msa_counties, msa_cbgs\n",
    "\n",
    "\n",
    "def get_nyt_outcomes_over_counties(counties=None):\n",
    "    outcomes = pd.read_csv(r'H:\\extra_safegraph_aggregate_models\\all_aggregate_data\\us-counties.csv')\n",
    "    outcomes['fips'] = outcomes.fillna(0)['fips'].astype(float).astype(int).astype(str).str.zfill(5)  # \"Huan\n",
    "    if counties is not None:\n",
    "        outcomes = outcomes[outcomes['fips'].isin(counties)]\n",
    "    return outcomes\n",
    "# r'PATH_TO_NYT_DATA = os.path.join(BASE_DIR, 'external_datasets_for_aggregate_analysis/nytimes_coronavirus_data/covid-19-data/us-counties.csv')'\n",
    "\n",
    "def plot_model_fit_for_msa(best_models, msa_name, ax, min_datetime, max_datetime,\n",
    "                           mode='cases', train_test_partition=None, additional_kwargs=None):\n",
    "    msa_best_models = best_models[best_models.MSA_name == msa_name]\n",
    "    nyt_outcomes, _, _, _, _ = get_variables_for_evaluating_msa_model(msa_name)  \n",
    "    lag = DEATH_LAG if mode == 'deaths' else DETECTION_LAG\n",
    "    other_plotting_kwargs =  {\n",
    "            'plot_log':False, \n",
    "            'plot_legend':True,\n",
    "            'plot_errorbars':True,\n",
    "            'xticks':[min_datetime + datetime.timedelta(days=lag),\n",
    "                      max_datetime + datetime.timedelta(days=lag)],                                                                             \n",
    "            'x_range':[min_datetime + datetime.timedelta(days=lag),\n",
    "                       max_datetime + datetime.timedelta(days=lag)],        \n",
    "            'plot_daily_not_cumulative':True,\n",
    "            'model_line_label': 'Model simulated',\n",
    "            'true_line_label': 'Reported %s' % mode,\n",
    "            'title_fontsize':30,\n",
    "            'marker_size':5,\n",
    "            'real_data_color':'tab:orange',\n",
    "            'model_color':'tab:blue',\n",
    "            'plot_mode':mode,\n",
    "    }\n",
    "    if additional_kwargs is not None:\n",
    "        for k,v in additional_kwargs.items():\n",
    "            other_plotting_kwargs[k] = v\n",
    "    score_dict = plot_best_models_fit_for_msa(msa_best_models, msa_name, ax, key_to_sort_by=None, train_test_partition=train_test_partition, \n",
    "                                 plotting_kwargs=other_plotting_kwargs, use_given_df=True)\n",
    "    ax.grid(alpha=0.1)\n",
    "    ax.set_ylabel('Daily confirmed %s' % mode, fontsize=20)\n",
    "    return score_dict\n",
    "\n",
    "def get_best_models_for_all_msas(df, key_to_sort_by, only_single_best=False, \n",
    "                                 loss_tolerance=ACCEPTABLE_LOSS_TOLERANCE, verbose=False,\n",
    "                                 max_models_to_keep=100):\n",
    "    model_str = 'model' if only_single_best else 'models'\n",
    "    if verbose:\n",
    "        print('Finding best fit %s based on key=%s, loss tolerance=%2.3f' % (model_str, key_to_sort_by, loss_tolerance))\n",
    "    df = df.sort_values(by=key_to_sort_by)\n",
    "    best_models = []\n",
    "    for msa in df.MSA_name.unique():\n",
    "        msa_df = df[df['MSA_name'] == msa]\n",
    "        if only_single_best:\n",
    "            msa_best_models = msa_df.iloc[:1]\n",
    "        else:\n",
    "            best_loss = msa_df.iloc[0][key_to_sort_by]\n",
    "            msa_best_models = msa_df[msa_df[key_to_sort_by] <= (loss_tolerance * best_loss)]\n",
    "            passed_thresh = len(msa_best_models)\n",
    "            msa_best_models = msa_best_models.iloc[:max_models_to_keep]\n",
    "        if verbose:\n",
    "            print('%s: %d within loss tol -> keeping %d' % (msa, passed_thresh, len(msa_best_models)))\n",
    "        best_models.append(msa_best_models)\n",
    "    best_models = pd.concat(best_models)\n",
    "    return best_models\n",
    "\n",
    "def plot_best_models_fit_for_msa(df, msa_name, ax, key_to_sort_by, train_test_partition,\n",
    "                                 plotting_kwargs, use_given_df=False, old_directory=False, \n",
    "                                 loss_tol=ACCEPTABLE_LOSS_TOLERANCE, max_models_to_keep=MAX_MODELS_TO_TAKE_PER_MSA):\n",
    "\n",
    "    if use_given_df:\n",
    "        subdf = df\n",
    "        num_models_to_aggregate = len(subdf)\n",
    "        print('Plotting predictions from %d models' % num_models_to_aggregate)\n",
    "    else:\n",
    "        subdf = df[(df['MSA_name'] == msa_name)].copy()\n",
    "        subdf = subdf.sort_values(by=key_to_sort_by)\n",
    "        losses = subdf[key_to_sort_by] / subdf.iloc[0][key_to_sort_by]\n",
    "        num_models_to_aggregate = np.sum(losses <= loss_tol)\n",
    "        print('Found %d best fit models within threshold for %s, keeping %d at most' % (num_models_to_aggregate, MSAS_TO_PRETTY_NAMES[msa_name], max_models_to_keep))\n",
    "        num_models_to_aggregate = min(num_models_to_aggregate, max_models_to_keep)\n",
    "\n",
    "    # Aggregate predictions from best fit models that are within the ACCEPTABLE_LOSS_TOLERANCE\n",
    "    mdl_predictions = []\n",
    "    old_projected_hrs = None\n",
    "    individual_plotting_kwargs = plotting_kwargs.copy()\n",
    "    individual_plotting_kwargs['return_mdl_pred_and_hours'] = True  # don't plot individual models\n",
    "    for model_idx in range(num_models_to_aggregate):\n",
    "        ts = subdf.iloc[model_idx]['timestring']\n",
    "        model, kwargs, _, _, _ = load_model_and_data_from_timestring(\n",
    "            ts,\n",
    "            load_fast_results_only=False, old_directory=old_directory, load_full_model=True)\n",
    "        model_kwargs = kwargs['model_kwargs']\n",
    "        data_kwargs = kwargs['data_kwargs']\n",
    "        mdl_prediction, projected_hrs = plot_model_fit_from_model_and_kwargs(\n",
    "            ax,\n",
    "            model_kwargs,\n",
    "            data_kwargs,\n",
    "            model=model,\n",
    "            plotting_kwargs=individual_plotting_kwargs,\n",
    "            train_test_partition=train_test_partition)\n",
    "        mdl_predictions.append(mdl_prediction)\n",
    "        if old_projected_hrs is not None:\n",
    "            assert projected_hrs == old_projected_hrs\n",
    "        old_projected_hrs = projected_hrs\n",
    "    mdl_predictions = np.concatenate(mdl_predictions, axis=0)\n",
    "\n",
    "    # Plot aggregate predictions\n",
    "    agg_plotting_kwargs = plotting_kwargs.copy()\n",
    "    agg_plotting_kwargs['mdl_prediction'] = mdl_predictions\n",
    "    agg_plotting_kwargs['projected_hrs'] = projected_hrs\n",
    "    score_dict = plot_model_fit_from_model_and_kwargs(\n",
    "        ax,\n",
    "        model_kwargs,\n",
    "        data_kwargs,\n",
    "        plotting_kwargs=agg_plotting_kwargs,\n",
    "        train_test_partition=train_test_partition,\n",
    "    )\n",
    "    ax.grid(False)\n",
    "    return score_dict\n",
    "\n",
    "\n",
    "\n",
    "def plot_model_fit_from_model_and_kwargs(ax,\n",
    "                                         mdl_kwargs,\n",
    "                                         data_kwargs,\n",
    "                                         model=None,\n",
    "                                         train_test_partition=None,\n",
    "                                         model_results=None,\n",
    "                                         plotting_kwargs=None):\n",
    "    msa_name = data_kwargs['MSA_name']\n",
    "    nyt_outcomes, _, _, _, _ = get_variables_for_evaluating_msa_model(msa_name)\n",
    "    min_datetime = mdl_kwargs['min_datetime']\n",
    "    if plotting_kwargs is None:\n",
    "        plotting_kwargs = {}  # could include options like plot_mode, plot_log, etc.\n",
    "    if 'title' not in plotting_kwargs:\n",
    "        plotting_kwargs['title'] = MSAS_TO_PRETTY_NAMES[msa_name] if msa_name in MSAS_TO_PRETTY_NAMES else msa_name\n",
    "    if 'make_plot' not in plotting_kwargs:\n",
    "        plotting_kwargs['make_plot'] = True\n",
    "    score_dict = compare_model_vs_real_num_cases(nyt_outcomes, min_datetime,\n",
    "                                    model=model,\n",
    "                                    model_results=model_results,\n",
    "                                    ax=ax,\n",
    "                                    **plotting_kwargs)\n",
    "    if train_test_partition is not None and plotting_kwargs['make_plot']:\n",
    "        ax.plot_date([train_test_partition, train_test_partition], ax.get_ylim(), color='black', linestyle='-')\n",
    "    return score_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851c15d7-4184-4a71-bffe-f0366978486d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99deef74-4cf3-4088-b38b-71f6f91aa522",
   "metadata": {},
   "source": [
    "### Columbia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1dd443bc-81f8-4abc-a51c-3a5d59e9b014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models with timestrings between 2020_11_28_13_00203770 and 2022_01_02_17_22203770\n",
      "6476 files in directory J:\\extra_safegraph_aggregate_models\\data_and_model_configs\n",
      "Found 4336 fitted models for Columbia_SC\n",
      "Error in load_model_and_data_from_timestring(): [Errno 2] No such file or directory: 'J:\\\\extra_safegraph_aggregate_models\\\\fast_to_load_results_only\\\\fast_to_load_results_2021_11_28_13_04_02_682905_normal_grid_search_normal_grid_search.pkl' 2021_11_28_13_04_02_682905_normal_grid_search_normal_grid_search\n",
      "Time to load and score all models: 10.499s -> 0.002s per model\n",
      "Finding best fit models based on key=loss_dict_smoothed_daily_cases_RMSE_time_varying_cdr, loss tolerance=1.200\n",
      "Columbia_SC: 4 within loss tol -> keeping 4\n",
      "                                                                                                home_beta    poi_psi\n",
      "4279  [0.015333333333333332, 0.01510972222222222, 0.01488611111111111, 0.014662499999999998, 0.0144388...  11.666667\n",
      "4086  [0.014666666666666666, 0.014483333333333332, 0.0143, 0.014116666666666666, 0.013933333333333332,...  11.785714\n",
      "4317  [0.015333333333333332, 0.01517361111111111, 0.015013888888888887, 0.014854166666666665, 0.014694...   8.333333\n",
      "4241  [0.01479, 0.014651343749999999, 0.0145126875, 0.014374031249999999, 0.014235375, 0.0140967187499...   9.180000\n",
      "0.006388888888888888 0.015333333333333332\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestring</th>\n",
       "      <th>data_kwargs</th>\n",
       "      <th>model_kwargs</th>\n",
       "      <th>results</th>\n",
       "      <th>experiment_to_run</th>\n",
       "      <th>final infected fraction</th>\n",
       "      <th>loss_dict_eval_start_time_cases</th>\n",
       "      <th>loss_dict_eval_end_time_cases</th>\n",
       "      <th>loss_dict_cumulative_predicted_cases</th>\n",
       "      <th>loss_dict_cumulative_true_cases</th>\n",
       "      <th>loss_dict_cumulative_cases_RMSE</th>\n",
       "      <th>loss_dict_cumulative_cases_MSE</th>\n",
       "      <th>loss_dict_daily_cases_RMSE</th>\n",
       "      <th>loss_dict_daily_cases_MSE</th>\n",
       "      <th>loss_dict_daily_cases_RMSE_time_varying_cdr</th>\n",
       "      <th>loss_dict_smoothed_daily_cases_RMSE_time_varying_cdr</th>\n",
       "      <th>loss_dict_daily_cases_MRE_thres-1_sum</th>\n",
       "      <th>loss_dict_daily_cases_MRE_thres-10_sum</th>\n",
       "      <th>loss_dict_daily_cases_MRE_thres-20_sum</th>\n",
       "      <th>loss_dict_daily_cases_MRE_thres-50_sum</th>\n",
       "      <th>loss_dict_daily_cases_MRE_thres-100_sum</th>\n",
       "      <th>loss_dict_daily_cases_poisson_NLL_thres-1_logsumexp</th>\n",
       "      <th>loss_dict_daily_cases_poisson_NLL_thres-1_sum</th>\n",
       "      <th>loss_dict_daily_cases_poisson_NLL_thres-10_logsumexp</th>\n",
       "      <th>loss_dict_daily_cases_poisson_NLL_thres-10_sum</th>\n",
       "      <th>...</th>\n",
       "      <th>agg_county_loss_dict_max_daily_deaths_RMSE</th>\n",
       "      <th>agg_county_loss_dict_unweighted_avg_daily_deaths_per_capita_RMSE</th>\n",
       "      <th>agg_county_loss_dict_weighted_avg_daily_deaths_per_capita_RMSE</th>\n",
       "      <th>agg_county_loss_dict_max_daily_deaths_per_capita_RMSE</th>\n",
       "      <th>home_beta</th>\n",
       "      <th>poi_psi</th>\n",
       "      <th>p_sick_at_t0</th>\n",
       "      <th>inter_cbg_gamma</th>\n",
       "      <th>just_compute_r0</th>\n",
       "      <th>min_datetime</th>\n",
       "      <th>max_datetime</th>\n",
       "      <th>model_init_kwargs</th>\n",
       "      <th>simulation_kwargs</th>\n",
       "      <th>poi_attributes_to_clip</th>\n",
       "      <th>include_cbg_prop_out</th>\n",
       "      <th>include_inter_cbg_travel</th>\n",
       "      <th>include_mask_use</th>\n",
       "      <th>counties_to_track</th>\n",
       "      <th>poi_cbg_visits_list</th>\n",
       "      <th>poi_ids</th>\n",
       "      <th>cbg_ids</th>\n",
       "      <th>MSA_name</th>\n",
       "      <th>start_beta</th>\n",
       "      <th>end_beta</th>\n",
       "      <th>beta_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4279</th>\n",
       "      <td>2021_11_28_13_30_17_509206_normal_grid_search_normal_grid_search</td>\n",
       "      <td>{'MSA_name': 'Columbia_SC', 'nrows': None}</td>\n",
       "      <td>{'min_datetime': 2020-12-20 00:00:00, 'max_datetime': 2021-01-29 23:00:00, 'exogenous_model_kwar...</td>\n",
       "      <td>None</td>\n",
       "      <td>normal_grid_search</td>\n",
       "      <td>[0.13768269578953446, 0.13730169142845147, 0.13819385040068433, 0.13711538764307343, 0.136989435...</td>\n",
       "      <td>2020-12-29</td>\n",
       "      <td>2021-02-07</td>\n",
       "      <td>[[43064.4, 43529.4, 44026.2, 44531.4, 45081.0, 45676.799999999996, 46252.2, 46811.4, 47425.2, 48...</td>\n",
       "      <td>[48961, 49481, 50161, 50161, 50790, 51919, 52550, 52920, 53772, 54672, 55610, 56401, 56895, 5735...</td>\n",
       "      <td>7591.910768</td>\n",
       "      <td>5.768706e+07</td>\n",
       "      <td>245.257384</td>\n",
       "      <td>60172.0213</td>\n",
       "      <td>235.526693</td>\n",
       "      <td>48.662176</td>\n",
       "      <td>0.278464</td>\n",
       "      <td>0.278464</td>\n",
       "      <td>0.278464</td>\n",
       "      <td>0.278464</td>\n",
       "      <td>0.278464</td>\n",
       "      <td>1681.522608</td>\n",
       "      <td>56976.163377</td>\n",
       "      <td>1681.522608</td>\n",
       "      <td>56976.163377</td>\n",
       "      <td>...</td>\n",
       "      <td>3.816624</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>[0.015333333333333332, 0.01510972222222222, 0.01488611111111111, 0.014662499999999998, 0.0144388...</td>\n",
       "      <td>11.666667</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-20</td>\n",
       "      <td>2021-01-29 23:00:00</td>\n",
       "      <td>{'num_seeds': 30}</td>\n",
       "      <td>{'use_aggregate_mobility': False, 'use_home_proportion_beta': False, 'use_inter_cbg_leak_factor'...</td>\n",
       "      <td>{'clip_areas': True, 'clip_dwell_times': True, 'clip_visits': True}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[45017, 45079, 45055, 45063, 45081, 45039]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Columbia_SC</td>\n",
       "      <td>0.015333</td>\n",
       "      <td>0.006389</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4086</th>\n",
       "      <td>2021_11_28_12_33_55_506980_normal_grid_search_normal_grid_search</td>\n",
       "      <td>{'MSA_name': 'Columbia_SC', 'nrows': None}</td>\n",
       "      <td>{'min_datetime': 2020-12-20 00:00:00, 'max_datetime': 2021-01-29 23:00:00, 'exogenous_model_kwar...</td>\n",
       "      <td>None</td>\n",
       "      <td>normal_grid_search</td>\n",
       "      <td>[0.13771313415446945, 0.13493064775989377, 0.13778188287527093, 0.13634655652876687, 0.136392738...</td>\n",
       "      <td>2020-12-29</td>\n",
       "      <td>2021-02-07</td>\n",
       "      <td>[[43057.2, 43521.6, 44023.2, 44526.6, 45083.4, 45648.6, 46227.6, 46795.799999999996, 47398.2, 47...</td>\n",
       "      <td>[48961, 49481, 50161, 50161, 50790, 51919, 52550, 52920, 53772, 54672, 55610, 56401, 56895, 5735...</td>\n",
       "      <td>7952.232871</td>\n",
       "      <td>6.331378e+07</td>\n",
       "      <td>248.782602</td>\n",
       "      <td>61922.3150</td>\n",
       "      <td>237.240052</td>\n",
       "      <td>53.534170</td>\n",
       "      <td>0.272384</td>\n",
       "      <td>0.272384</td>\n",
       "      <td>0.272384</td>\n",
       "      <td>0.272384</td>\n",
       "      <td>0.272384</td>\n",
       "      <td>1796.159902</td>\n",
       "      <td>58513.703102</td>\n",
       "      <td>1796.159902</td>\n",
       "      <td>58513.703102</td>\n",
       "      <td>...</td>\n",
       "      <td>3.836764</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>[0.014666666666666666, 0.014483333333333332, 0.0143, 0.014116666666666666, 0.013933333333333332,...</td>\n",
       "      <td>11.785714</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-20</td>\n",
       "      <td>2021-01-29 23:00:00</td>\n",
       "      <td>{'num_seeds': 30}</td>\n",
       "      <td>{'use_aggregate_mobility': False, 'use_home_proportion_beta': False, 'use_inter_cbg_leak_factor'...</td>\n",
       "      <td>{'clip_areas': True, 'clip_dwell_times': True, 'clip_visits': True}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[45081, 45017, 45063, 45055, 45039, 45079]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Columbia_SC</td>\n",
       "      <td>0.014667</td>\n",
       "      <td>0.007333</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4317</th>\n",
       "      <td>2021_11_28_13_39_15_528618_normal_grid_search_normal_grid_search</td>\n",
       "      <td>{'MSA_name': 'Columbia_SC', 'nrows': None}</td>\n",
       "      <td>{'min_datetime': 2020-12-20 00:00:00, 'max_datetime': 2021-01-29 23:00:00, 'exogenous_model_kwar...</td>\n",
       "      <td>None</td>\n",
       "      <td>normal_grid_search</td>\n",
       "      <td>[0.1424326551175813, 0.14086088092826518, 0.14154207054353474, 0.14189473573726444, 0.1435321098...</td>\n",
       "      <td>2020-12-29</td>\n",
       "      <td>2021-02-07</td>\n",
       "      <td>[[43064.4, 43548.0, 44030.4, 44563.2, 45103.799999999996, 45695.4, 46248.6, 46836.0, 47391.0, 47...</td>\n",
       "      <td>[48961, 49481, 50161, 50161, 50790, 51919, 52550, 52920, 53772, 54672, 55610, 56401, 56895, 5735...</td>\n",
       "      <td>7316.691160</td>\n",
       "      <td>5.364775e+07</td>\n",
       "      <td>239.659685</td>\n",
       "      <td>57450.5593</td>\n",
       "      <td>236.200333</td>\n",
       "      <td>57.356438</td>\n",
       "      <td>0.262822</td>\n",
       "      <td>0.262822</td>\n",
       "      <td>0.262822</td>\n",
       "      <td>0.262822</td>\n",
       "      <td>0.262822</td>\n",
       "      <td>1669.151100</td>\n",
       "      <td>53321.389981</td>\n",
       "      <td>1669.151100</td>\n",
       "      <td>53321.389981</td>\n",
       "      <td>...</td>\n",
       "      <td>3.789434</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>[0.015333333333333332, 0.01517361111111111, 0.015013888888888887, 0.014854166666666665, 0.014694...</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-20</td>\n",
       "      <td>2021-01-29 23:00:00</td>\n",
       "      <td>{'num_seeds': 30}</td>\n",
       "      <td>{'use_aggregate_mobility': False, 'use_home_proportion_beta': False, 'use_inter_cbg_leak_factor'...</td>\n",
       "      <td>{'clip_areas': True, 'clip_dwell_times': True, 'clip_visits': True}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[45081, 45063, 45039, 45079, 45055, 45017]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Columbia_SC</td>\n",
       "      <td>0.015333</td>\n",
       "      <td>0.008944</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4241</th>\n",
       "      <td>2021_11_28_13_18_45_911113_normal_grid_search</td>\n",
       "      <td>{'MSA_name': 'Columbia_SC', 'nrows': None}</td>\n",
       "      <td>{'min_datetime': 2020-12-20 00:00:00, 'max_datetime': 2021-01-29 23:00:00, 'exogenous_model_kwar...</td>\n",
       "      <td>None</td>\n",
       "      <td>normal_grid_search</td>\n",
       "      <td>[0.14097528719646915, 0.1399041716303943, 0.1407160362951262, 0.13952894006266106, 0.13946491453...</td>\n",
       "      <td>2020-12-29</td>\n",
       "      <td>2021-02-07</td>\n",
       "      <td>[[43066.799999999996, 43523.4, 44017.2, 44530.2, 45050.4, 45600.0, 46143.6, 46690.799999999996, ...</td>\n",
       "      <td>[48961, 49481, 50161, 50161, 50790, 51919, 52550, 52920, 53772, 54672, 55610, 56401, 56895, 5735...</td>\n",
       "      <td>7697.120120</td>\n",
       "      <td>5.933761e+07</td>\n",
       "      <td>244.676434</td>\n",
       "      <td>59889.5964</td>\n",
       "      <td>238.041421</td>\n",
       "      <td>58.344058</td>\n",
       "      <td>0.267717</td>\n",
       "      <td>0.267717</td>\n",
       "      <td>0.267717</td>\n",
       "      <td>0.267717</td>\n",
       "      <td>0.267717</td>\n",
       "      <td>1716.270990</td>\n",
       "      <td>56020.633224</td>\n",
       "      <td>1716.270990</td>\n",
       "      <td>56020.633224</td>\n",
       "      <td>...</td>\n",
       "      <td>3.816518</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>[0.01479, 0.014651343749999999, 0.0145126875, 0.014374031249999999, 0.014235375, 0.0140967187499...</td>\n",
       "      <td>9.180000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-20</td>\n",
       "      <td>2021-01-29 23:00:00</td>\n",
       "      <td>{'num_seeds': 30}</td>\n",
       "      <td>{'use_aggregate_mobility': False, 'use_home_proportion_beta': False, 'use_inter_cbg_leak_factor'...</td>\n",
       "      <td>{'clip_areas': True, 'clip_dwell_times': True, 'clip_visits': True}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[45039, 45063, 45017, 45079, 45081, 45055]</td>\n",
       "      <td>[  (3, 893)\\t0.2289944516825042\\n  (3, 878)\\t0.47933067639604404\\n  (3, 872)\\t0.2162319608843065...</td>\n",
       "      <td>[sg:003140f5b1dc4f599315dc9f5b1fd623, sg:084a87f9d18a4ab6bd97647f6da414b0, sg:0ce97481f4df412fb5...</td>\n",
       "      <td>[130510107001, 130730301023, 130730303042, 130730303061, 130730303082, 130730304022, 13073030503...</td>\n",
       "      <td>Columbia_SC</td>\n",
       "      <td>0.014790</td>\n",
       "      <td>0.009244</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 194 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            timestring                                 data_kwargs                                                                                         model_kwargs results   experiment_to_run                                                                              final infected fraction loss_dict_eval_start_time_cases loss_dict_eval_end_time_cases  \\\n",
       "4279  2021_11_28_13_30_17_509206_normal_grid_search_normal_grid_search  {'MSA_name': 'Columbia_SC', 'nrows': None}  {'min_datetime': 2020-12-20 00:00:00, 'max_datetime': 2021-01-29 23:00:00, 'exogenous_model_kwar...    None  normal_grid_search  [0.13768269578953446, 0.13730169142845147, 0.13819385040068433, 0.13711538764307343, 0.136989435...                      2020-12-29                    2021-02-07   \n",
       "4086  2021_11_28_12_33_55_506980_normal_grid_search_normal_grid_search  {'MSA_name': 'Columbia_SC', 'nrows': None}  {'min_datetime': 2020-12-20 00:00:00, 'max_datetime': 2021-01-29 23:00:00, 'exogenous_model_kwar...    None  normal_grid_search  [0.13771313415446945, 0.13493064775989377, 0.13778188287527093, 0.13634655652876687, 0.136392738...                      2020-12-29                    2021-02-07   \n",
       "4317  2021_11_28_13_39_15_528618_normal_grid_search_normal_grid_search  {'MSA_name': 'Columbia_SC', 'nrows': None}  {'min_datetime': 2020-12-20 00:00:00, 'max_datetime': 2021-01-29 23:00:00, 'exogenous_model_kwar...    None  normal_grid_search  [0.1424326551175813, 0.14086088092826518, 0.14154207054353474, 0.14189473573726444, 0.1435321098...                      2020-12-29                    2021-02-07   \n",
       "4241                     2021_11_28_13_18_45_911113_normal_grid_search  {'MSA_name': 'Columbia_SC', 'nrows': None}  {'min_datetime': 2020-12-20 00:00:00, 'max_datetime': 2021-01-29 23:00:00, 'exogenous_model_kwar...    None  normal_grid_search  [0.14097528719646915, 0.1399041716303943, 0.1407160362951262, 0.13952894006266106, 0.13946491453...                      2020-12-29                    2021-02-07   \n",
       "\n",
       "                                                                     loss_dict_cumulative_predicted_cases                                                                      loss_dict_cumulative_true_cases  loss_dict_cumulative_cases_RMSE  loss_dict_cumulative_cases_MSE  loss_dict_daily_cases_RMSE  loss_dict_daily_cases_MSE  loss_dict_daily_cases_RMSE_time_varying_cdr  loss_dict_smoothed_daily_cases_RMSE_time_varying_cdr  loss_dict_daily_cases_MRE_thres-1_sum  \\\n",
       "4279  [[43064.4, 43529.4, 44026.2, 44531.4, 45081.0, 45676.799999999996, 46252.2, 46811.4, 47425.2, 48...  [48961, 49481, 50161, 50161, 50790, 51919, 52550, 52920, 53772, 54672, 55610, 56401, 56895, 5735...                      7591.910768                    5.768706e+07                  245.257384                 60172.0213                                   235.526693                                             48.662176                               0.278464   \n",
       "4086  [[43057.2, 43521.6, 44023.2, 44526.6, 45083.4, 45648.6, 46227.6, 46795.799999999996, 47398.2, 47...  [48961, 49481, 50161, 50161, 50790, 51919, 52550, 52920, 53772, 54672, 55610, 56401, 56895, 5735...                      7952.232871                    6.331378e+07                  248.782602                 61922.3150                                   237.240052                                             53.534170                               0.272384   \n",
       "4317  [[43064.4, 43548.0, 44030.4, 44563.2, 45103.799999999996, 45695.4, 46248.6, 46836.0, 47391.0, 47...  [48961, 49481, 50161, 50161, 50790, 51919, 52550, 52920, 53772, 54672, 55610, 56401, 56895, 5735...                      7316.691160                    5.364775e+07                  239.659685                 57450.5593                                   236.200333                                             57.356438                               0.262822   \n",
       "4241  [[43066.799999999996, 43523.4, 44017.2, 44530.2, 45050.4, 45600.0, 46143.6, 46690.799999999996, ...  [48961, 49481, 50161, 50161, 50790, 51919, 52550, 52920, 53772, 54672, 55610, 56401, 56895, 5735...                      7697.120120                    5.933761e+07                  244.676434                 59889.5964                                   238.041421                                             58.344058                               0.267717   \n",
       "\n",
       "      loss_dict_daily_cases_MRE_thres-10_sum  loss_dict_daily_cases_MRE_thres-20_sum  loss_dict_daily_cases_MRE_thres-50_sum  loss_dict_daily_cases_MRE_thres-100_sum  loss_dict_daily_cases_poisson_NLL_thres-1_logsumexp  loss_dict_daily_cases_poisson_NLL_thres-1_sum  loss_dict_daily_cases_poisson_NLL_thres-10_logsumexp  loss_dict_daily_cases_poisson_NLL_thres-10_sum  ...  agg_county_loss_dict_max_daily_deaths_RMSE  agg_county_loss_dict_unweighted_avg_daily_deaths_per_capita_RMSE  \\\n",
       "4279                                0.278464                                0.278464                                0.278464                                 0.278464                                          1681.522608                                   56976.163377                                           1681.522608                                    56976.163377  ...                                    3.816624                                                          0.000023   \n",
       "4086                                0.272384                                0.272384                                0.272384                                 0.272384                                          1796.159902                                   58513.703102                                           1796.159902                                    58513.703102  ...                                    3.836764                                                          0.000023   \n",
       "4317                                0.262822                                0.262822                                0.262822                                 0.262822                                          1669.151100                                   53321.389981                                           1669.151100                                    53321.389981  ...                                    3.789434                                                          0.000023   \n",
       "4241                                0.267717                                0.267717                                0.267717                                 0.267717                                          1716.270990                                   56020.633224                                           1716.270990                                    56020.633224  ...                                    3.816518                                                          0.000023   \n",
       "\n",
       "      agg_county_loss_dict_weighted_avg_daily_deaths_per_capita_RMSE  agg_county_loss_dict_max_daily_deaths_per_capita_RMSE                                                                                            home_beta    poi_psi p_sick_at_t0 inter_cbg_gamma just_compute_r0 min_datetime        max_datetime  model_init_kwargs                                                                                    simulation_kwargs  \\\n",
       "4279                                                        0.000013                                               0.000043  [0.015333333333333332, 0.01510972222222222, 0.01488611111111111, 0.014662499999999998, 0.0144388...  11.666667         None            None           False   2020-12-20 2021-01-29 23:00:00  {'num_seeds': 30}  {'use_aggregate_mobility': False, 'use_home_proportion_beta': False, 'use_inter_cbg_leak_factor'...   \n",
       "4086                                                        0.000013                                               0.000043  [0.014666666666666666, 0.014483333333333332, 0.0143, 0.014116666666666666, 0.013933333333333332,...  11.785714         None            None           False   2020-12-20 2021-01-29 23:00:00  {'num_seeds': 30}  {'use_aggregate_mobility': False, 'use_home_proportion_beta': False, 'use_inter_cbg_leak_factor'...   \n",
       "4317                                                        0.000012                                               0.000043  [0.015333333333333332, 0.01517361111111111, 0.015013888888888887, 0.014854166666666665, 0.014694...   8.333333         None            None           False   2020-12-20 2021-01-29 23:00:00  {'num_seeds': 30}  {'use_aggregate_mobility': False, 'use_home_proportion_beta': False, 'use_inter_cbg_leak_factor'...   \n",
       "4241                                                        0.000012                                               0.000043  [0.01479, 0.014651343749999999, 0.0145126875, 0.014374031249999999, 0.014235375, 0.0140967187499...   9.180000         None            None           False   2020-12-20 2021-01-29 23:00:00  {'num_seeds': 30}  {'use_aggregate_mobility': False, 'use_home_proportion_beta': False, 'use_inter_cbg_leak_factor'...   \n",
       "\n",
       "                                                   poi_attributes_to_clip  include_cbg_prop_out  include_inter_cbg_travel  include_mask_use                           counties_to_track                                                                                  poi_cbg_visits_list                                                                                              poi_ids                                                                                              cbg_ids  \\\n",
       "4279  {'clip_areas': True, 'clip_dwell_times': True, 'clip_visits': True}                 False                     False              True  [45017, 45079, 45055, 45063, 45081, 45039]                                                                                                 None                                                                                                 None                                                                                                 None   \n",
       "4086  {'clip_areas': True, 'clip_dwell_times': True, 'clip_visits': True}                 False                     False              True  [45081, 45017, 45063, 45055, 45039, 45079]                                                                                                 None                                                                                                 None                                                                                                 None   \n",
       "4317  {'clip_areas': True, 'clip_dwell_times': True, 'clip_visits': True}                 False                     False              True  [45081, 45063, 45039, 45079, 45055, 45017]                                                                                                 None                                                                                                 None                                                                                                 None   \n",
       "4241  {'clip_areas': True, 'clip_dwell_times': True, 'clip_visits': True}                 False                     False              True  [45039, 45063, 45017, 45079, 45081, 45055]  [  (3, 893)\\t0.2289944516825042\\n  (3, 878)\\t0.47933067639604404\\n  (3, 872)\\t0.2162319608843065...  [sg:003140f5b1dc4f599315dc9f5b1fd623, sg:084a87f9d18a4ab6bd97647f6da414b0, sg:0ce97481f4df412fb5...  [130510107001, 130730301023, 130730303042, 130730303061, 130730303082, 130730304022, 13073030503...   \n",
       "\n",
       "         MSA_name  start_beta  end_beta  beta_ratio  \n",
       "4279  Columbia_SC    0.015333  0.006389    0.416667  \n",
       "4086  Columbia_SC    0.014667  0.007333    0.500000  \n",
       "4317  Columbia_SC    0.015333  0.008944    0.583333  \n",
       "4241  Columbia_SC    0.014790  0.009244    0.625000  \n",
       "\n",
       "[4 rows x 194 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# full model\n",
    "msa_name = 'Columbia_SC'\n",
    "# msa_name = 'Greenville_Anderson_SC'\n",
    "# msa_name = 'Charleston_North_Charleston_SC'\n",
    "\n",
    "key_to_sort_by = 'loss_dict_smoothed_daily_cases_RMSE_time_varying_cdr'\n",
    "min_datetime = datetime.datetime(2020, 12, 20)\n",
    "max_datetime = datetime.datetime(2021, 1, 30)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "max_timestring = '2021_11_28_02_44_203770'  # group 2\n",
    "min_timestring = '2021_11_27_23_34_000000'\n",
    "\n",
    "max_timestring = '2021_11_27_23_35_000000' # group 1\n",
    "min_timestring = '2021_11_27_19_45_000000'\n",
    "\n",
    "max_timestring = '2021_11_28_13_00_203770'  # group 3   # 931 , \n",
    "min_timestring = '2021_11_28_09_10_203770'    # 931 2021_11_28_12_33_55_506980_normal_grid_search_normal_grid_search\n",
    "\n",
    "# max_timestring = '2021_11_28_13_15_203770'  # group 4  bad, not use \n",
    "# min_timestring = '2021_11_28_13_05_203770'    \n",
    "\n",
    "# max_timestring = '2021_11_28_13_44_203770'  # group 5  GOOD:  # 2021_11_28_13_18_45_911113_normal_grid_search_normal_grid_search\n",
    "# min_timestring = '2021_11_28_13_23_203770' \n",
    "\n",
    "max_timestring = '2021_11_28_13_22203770'  # group 4.5 GOOD:  # 2021_11_28_13_18_45_911113_normal_grid_search_normal_grid_search\n",
    "min_timestring = '2021_11_28_13_16203770' \n",
    "\n",
    "# max_timestring = '2022_01_02_17_22203770'  # Greenville_Anderson_SC, Best: 2022_01_02_14_46_37_084470_normal_grid_search_normal_grid_search\t\n",
    "# min_timestring = '2022_01_02_13_55203770' \n",
    "\n",
    "# max_timestring = '2022_01_03_00_22203770'  # Charleston_North_Charleston_SC, Best: 2022_01_02_23_03_13_321102_normal_grid_search_normal_grid_search\t\n",
    "# min_timestring = '2022_01_02_20_55203770' \n",
    "\n",
    "# max_timestring = '2022_01_03_30_22203770'  # Charleston_North_Charleston_SC, Best: * 1.1, not good.\t\n",
    "# min_timestring = '2022_01_03_00_22203770' \n",
    "\n",
    "# max_timestring = '2022_01_03_08_30203770'  # Charleston_North_Charleston_SC, Best: * 1.02,  good.\t\n",
    "# min_timestring = '2022_01_03_08_22203770' \n",
    "\n",
    "max_timestring = '2022_11_27_23_35_000000' # all\n",
    "min_timestring = '2021_11_28_11_45_000000'\n",
    "\n",
    "max_timestring = '2022_01_02_17_22203770'  # Greenville_Anderson_SC, Best: 2022_01_02_14_46_37_084470_normal_grid_search_normal_grid_search\t\n",
    "min_timestring = '2020_11_28_13_00203770'\n",
    "\n",
    "expected_num_jobs = 1050\n",
    "gridsearch_df = evaluate_all_fitted_models_for_msa(msa_name,\n",
    "                                                   min_timestring=min_timestring,\n",
    "                                                   max_timestring=max_timestring, \n",
    "                                                   required_properties={'experiment_to_run':'normal_grid_search'})\n",
    "# assert len(gridsearch_df) == expected_num_jobs, len(gridsearch_df)\n",
    "gridsearch_df['MSA_name'] = gridsearch_df['data_kwargs'].map(lambda x:x['MSA_name'])\n",
    "gridsearch_df['start_beta'] = gridsearch_df['home_beta'].apply(lambda x:x[0])\n",
    "gridsearch_df['end_beta'] = gridsearch_df['home_beta'].apply(lambda x:x[-1])\n",
    "gridsearch_df['beta_ratio'] = gridsearch_df.end_beta.values / gridsearch_df.start_beta.values\n",
    "\n",
    "best_models_df = get_best_models_for_all_msas(gridsearch_df, key_to_sort_by, loss_tolerance=1.2, verbose=True)\n",
    "best_models_df = best_models_df.sort_values(by=key_to_sort_by)\n",
    "\n",
    "print(best_models_df[['home_beta', 'poi_psi']])\n",
    "print(best_models_df['home_beta'].iloc[0].min(), best_models_df['home_beta'].iloc[0].max())\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(7,7))\n",
    "# out = plot_model_fit_for_msa(best_models_df, msa_name, ax, min_datetime, max_datetime)\n",
    "# plt.show()\n",
    "best_models_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b5f552-2178-4364-b9ba-9849a636441a",
   "metadata": {},
   "source": [
    "## Charleston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "039e6db7-6a6a-4d22-bff3-5709669a57ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models with timestrings between 2020_11_28_09_10_203770 and 2022_11_28_13_00_203770\n",
      "6476 files in directory J:\\extra_safegraph_aggregate_models\\data_and_model_configs\n",
      "Found 1054 fitted models for Charleston_North_Charleston_SC\n",
      "Time to load and score all models: 15.220s -> 0.014s per model\n",
      "Finding best fit models based on key=loss_dict_smoothed_daily_cases_RMSE_time_varying_cdr, loss tolerance=1.200\n",
      "Charleston_North_Charleston_SC: 3 within loss tol -> keeping 3\n",
      "                                                                                                home_beta    poi_psi\n",
      "1053  [0.01496, 0.014773, 0.014586, 0.014399, 0.014211999999999999, 0.014025000000000001, 0.013838, 0....  18.942857\n",
      "722   [0.01496, 0.014773, 0.014586, 0.014399, 0.014211999999999999, 0.014025000000000001, 0.013838, 0....  18.942857\n",
      "0     [0.0154, 0.0152075, 0.015015, 0.0148225, 0.014629999999999999, 0.0144375, 0.014245, 0.0140525, 0...  19.500000\n",
      "0.00748 0.01496\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestring</th>\n",
       "      <th>data_kwargs</th>\n",
       "      <th>model_kwargs</th>\n",
       "      <th>results</th>\n",
       "      <th>experiment_to_run</th>\n",
       "      <th>final infected fraction</th>\n",
       "      <th>loss_dict_eval_start_time_cases</th>\n",
       "      <th>loss_dict_eval_end_time_cases</th>\n",
       "      <th>loss_dict_cumulative_predicted_cases</th>\n",
       "      <th>loss_dict_cumulative_true_cases</th>\n",
       "      <th>loss_dict_cumulative_cases_RMSE</th>\n",
       "      <th>loss_dict_cumulative_cases_MSE</th>\n",
       "      <th>loss_dict_daily_cases_RMSE</th>\n",
       "      <th>loss_dict_daily_cases_MSE</th>\n",
       "      <th>loss_dict_daily_cases_RMSE_time_varying_cdr</th>\n",
       "      <th>loss_dict_smoothed_daily_cases_RMSE_time_varying_cdr</th>\n",
       "      <th>loss_dict_daily_cases_MRE_thres-1_sum</th>\n",
       "      <th>loss_dict_daily_cases_MRE_thres-10_sum</th>\n",
       "      <th>loss_dict_daily_cases_MRE_thres-20_sum</th>\n",
       "      <th>loss_dict_daily_cases_MRE_thres-50_sum</th>\n",
       "      <th>loss_dict_daily_cases_MRE_thres-100_sum</th>\n",
       "      <th>loss_dict_daily_cases_poisson_NLL_thres-1_logsumexp</th>\n",
       "      <th>loss_dict_daily_cases_poisson_NLL_thres-1_sum</th>\n",
       "      <th>loss_dict_daily_cases_poisson_NLL_thres-10_logsumexp</th>\n",
       "      <th>loss_dict_daily_cases_poisson_NLL_thres-10_sum</th>\n",
       "      <th>...</th>\n",
       "      <th>agg_county_loss_dict_max_daily_deaths_RMSE</th>\n",
       "      <th>agg_county_loss_dict_unweighted_avg_daily_deaths_per_capita_RMSE</th>\n",
       "      <th>agg_county_loss_dict_weighted_avg_daily_deaths_per_capita_RMSE</th>\n",
       "      <th>agg_county_loss_dict_max_daily_deaths_per_capita_RMSE</th>\n",
       "      <th>home_beta</th>\n",
       "      <th>poi_psi</th>\n",
       "      <th>p_sick_at_t0</th>\n",
       "      <th>inter_cbg_gamma</th>\n",
       "      <th>just_compute_r0</th>\n",
       "      <th>min_datetime</th>\n",
       "      <th>max_datetime</th>\n",
       "      <th>model_init_kwargs</th>\n",
       "      <th>simulation_kwargs</th>\n",
       "      <th>poi_attributes_to_clip</th>\n",
       "      <th>include_cbg_prop_out</th>\n",
       "      <th>include_inter_cbg_travel</th>\n",
       "      <th>include_mask_use</th>\n",
       "      <th>counties_to_track</th>\n",
       "      <th>poi_cbg_visits_list</th>\n",
       "      <th>poi_ids</th>\n",
       "      <th>cbg_ids</th>\n",
       "      <th>MSA_name</th>\n",
       "      <th>start_beta</th>\n",
       "      <th>end_beta</th>\n",
       "      <th>beta_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>2022_01_03_08_25_normal_grid_search</td>\n",
       "      <td>{'MSA_name': 'Charleston_North_Charleston_SC', 'nrows': None}</td>\n",
       "      <td>{'min_datetime': 2020-12-20 00:00:00, 'max_datetime': 2021-01-29 23:00:00, 'exogenous_model_kwar...</td>\n",
       "      <td>None</td>\n",
       "      <td>normal_grid_search</td>\n",
       "      <td>[0.13292937099671914, 0.13179861311997507, 0.13301170773531704, 0.13262307832913509, 0.133465657...</td>\n",
       "      <td>2020-12-29</td>\n",
       "      <td>2021-02-07</td>\n",
       "      <td>[[37091.4, 37407.6, 37774.2, 38129.4, 38529.0, 38914.799999999996, 39350.4, 39786.6, 40203.6, 40...</td>\n",
       "      <td>[40831, 41058, 41709, 41709, 42238, 43005, 43279, 43516, 44103, 44811, 45625, 46605, 46983, 4740...</td>\n",
       "      <td>5410.904567</td>\n",
       "      <td>2.934974e+07</td>\n",
       "      <td>209.273918</td>\n",
       "      <td>43815.8078</td>\n",
       "      <td>200.024065</td>\n",
       "      <td>46.986551</td>\n",
       "      <td>0.284047</td>\n",
       "      <td>0.284047</td>\n",
       "      <td>0.284047</td>\n",
       "      <td>0.284047</td>\n",
       "      <td>0.284047</td>\n",
       "      <td>1608.063108</td>\n",
       "      <td>53535.818340</td>\n",
       "      <td>1608.063108</td>\n",
       "      <td>53535.818340</td>\n",
       "      <td>...</td>\n",
       "      <td>3.005610</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>[0.01496, 0.014773, 0.014586, 0.014399, 0.014211999999999999, 0.014025000000000001, 0.013838, 0....</td>\n",
       "      <td>18.942857</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-20</td>\n",
       "      <td>2021-01-29 23:00:00</td>\n",
       "      <td>{'num_seeds': 30}</td>\n",
       "      <td>{'use_aggregate_mobility': False, 'use_home_proportion_beta': False, 'use_inter_cbg_leak_factor'...</td>\n",
       "      <td>{'clip_areas': True, 'clip_dwell_times': True, 'clip_visits': True}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[45019, 45015, 45035]</td>\n",
       "      <td>[  (13, 720)\\t0.27553844493055735\\n  (13, 601)\\t0.39470269078646036\\n  (13, 476)\\t0.000679470508...</td>\n",
       "      <td>[sg:0719f423492c4f229548ab9bb46aedb7, sg:09f3799061e744e586b1c9e32b0207cb, sg:1bbe25edd2714a97bf...</td>\n",
       "      <td>[130299203062, 130510107001, 130510108021, 130730303042, 130730303082, 131030303011, 37025041301...</td>\n",
       "      <td>Charleston_North_Charleston_SC</td>\n",
       "      <td>0.01496</td>\n",
       "      <td>0.00748</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>2022_01_02_23_03_13_321102_normal_grid_search</td>\n",
       "      <td>{'MSA_name': 'Charleston_North_Charleston_SC', 'nrows': None}</td>\n",
       "      <td>{'min_datetime': 2020-12-20 00:00:00, 'max_datetime': 2021-01-29 23:00:00, 'exogenous_model_kwar...</td>\n",
       "      <td>None</td>\n",
       "      <td>normal_grid_search</td>\n",
       "      <td>[0.13292937099671914, 0.13179861311997507, 0.13301170773531704, 0.13262307832913509, 0.133465657...</td>\n",
       "      <td>2020-12-29</td>\n",
       "      <td>2021-02-07</td>\n",
       "      <td>[[37091.4, 37407.6, 37774.2, 38129.4, 38529.0, 38914.799999999996, 39350.4, 39786.6, 40203.6, 40...</td>\n",
       "      <td>[40831, 41058, 41709, 41709, 42238, 43005, 43279, 43516, 44103, 44811, 45625, 46605, 46983, 4740...</td>\n",
       "      <td>5410.904567</td>\n",
       "      <td>2.934974e+07</td>\n",
       "      <td>209.273918</td>\n",
       "      <td>43815.8078</td>\n",
       "      <td>200.024065</td>\n",
       "      <td>46.986551</td>\n",
       "      <td>0.284047</td>\n",
       "      <td>0.284047</td>\n",
       "      <td>0.284047</td>\n",
       "      <td>0.284047</td>\n",
       "      <td>0.284047</td>\n",
       "      <td>1608.063108</td>\n",
       "      <td>53535.818340</td>\n",
       "      <td>1608.063108</td>\n",
       "      <td>53535.818340</td>\n",
       "      <td>...</td>\n",
       "      <td>3.005610</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>[0.01496, 0.014773, 0.014586, 0.014399, 0.014211999999999999, 0.014025000000000001, 0.013838, 0....</td>\n",
       "      <td>18.942857</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-20</td>\n",
       "      <td>2021-01-29 23:00:00</td>\n",
       "      <td>{'num_seeds': 30}</td>\n",
       "      <td>{'use_aggregate_mobility': False, 'use_home_proportion_beta': False, 'use_inter_cbg_leak_factor'...</td>\n",
       "      <td>{'clip_areas': True, 'clip_dwell_times': True, 'clip_visits': True}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[45019, 45015, 45035]</td>\n",
       "      <td>[  (13, 720)\\t0.27553844493055735\\n  (13, 601)\\t0.39470269078646036\\n  (13, 476)\\t0.000679470508...</td>\n",
       "      <td>[sg:0719f423492c4f229548ab9bb46aedb7, sg:09f3799061e744e586b1c9e32b0207cb, sg:1bbe25edd2714a97bf...</td>\n",
       "      <td>[130299203062, 130510107001, 130510108021, 130730303042, 130730303082, 131030303011, 37025041301...</td>\n",
       "      <td>Charleston_North_Charleston_SC</td>\n",
       "      <td>0.01496</td>\n",
       "      <td>0.00748</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-08-17_normal_grid_search</td>\n",
       "      <td>{'MSA_name': 'Charleston_North_Charleston_SC', 'nrows': None}</td>\n",
       "      <td>{'min_datetime': 2020-12-20 00:00:00, 'max_datetime': 2021-01-29 23:00:00, 'exogenous_model_kwar...</td>\n",
       "      <td>None</td>\n",
       "      <td>normal_grid_search</td>\n",
       "      <td>[0.13693587669689156, 0.13717026194610019, 0.1373118811364885, 0.13647204640279023, 0.1354236252...</td>\n",
       "      <td>2020-12-29</td>\n",
       "      <td>2021-02-07</td>\n",
       "      <td>[[37091.4, 37419.0, 37753.799999999996, 38133.0, 38514.0, 38911.799999999996, 39331.799999999996...</td>\n",
       "      <td>[40831, 41058, 41709, 41709, 42238, 43005, 43279, 43516, 44103, 44811, 45625, 46605, 46983, 4740...</td>\n",
       "      <td>4766.240533</td>\n",
       "      <td>2.278635e+07</td>\n",
       "      <td>200.223916</td>\n",
       "      <td>40102.4006</td>\n",
       "      <td>198.486553</td>\n",
       "      <td>50.398009</td>\n",
       "      <td>0.269774</td>\n",
       "      <td>0.269774</td>\n",
       "      <td>0.269774</td>\n",
       "      <td>0.269774</td>\n",
       "      <td>0.269774</td>\n",
       "      <td>1511.356599</td>\n",
       "      <td>48198.194238</td>\n",
       "      <td>1511.356599</td>\n",
       "      <td>48198.194238</td>\n",
       "      <td>...</td>\n",
       "      <td>2.987942</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>[0.0154, 0.0152075, 0.015015, 0.0148225, 0.014629999999999999, 0.0144375, 0.014245, 0.0140525, 0...</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-20</td>\n",
       "      <td>2021-01-29 23:00:00</td>\n",
       "      <td>{'num_seeds': 30}</td>\n",
       "      <td>{'use_aggregate_mobility': False, 'use_home_proportion_beta': False, 'use_inter_cbg_leak_factor'...</td>\n",
       "      <td>{'clip_areas': True, 'clip_dwell_times': True, 'clip_visits': True}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[45019, 45015, 45035]</td>\n",
       "      <td>[  (13, 720)\\t0.27553844493055735\\n  (13, 601)\\t0.39470269078646036\\n  (13, 476)\\t0.000679470508...</td>\n",
       "      <td>[sg:0719f423492c4f229548ab9bb46aedb7, sg:09f3799061e744e586b1c9e32b0207cb, sg:1bbe25edd2714a97bf...</td>\n",
       "      <td>[130299203062, 130510107001, 130510108021, 130730303042, 130730303082, 131030303011, 37025041301...</td>\n",
       "      <td>Charleston_North_Charleston_SC</td>\n",
       "      <td>0.01540</td>\n",
       "      <td>0.00770</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 194 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         timestring                                                    data_kwargs                                                                                         model_kwargs results   experiment_to_run                                                                              final infected fraction loss_dict_eval_start_time_cases loss_dict_eval_end_time_cases  \\\n",
       "1053            2022_01_03_08_25_normal_grid_search  {'MSA_name': 'Charleston_North_Charleston_SC', 'nrows': None}  {'min_datetime': 2020-12-20 00:00:00, 'max_datetime': 2021-01-29 23:00:00, 'exogenous_model_kwar...    None  normal_grid_search  [0.13292937099671914, 0.13179861311997507, 0.13301170773531704, 0.13262307832913509, 0.133465657...                      2020-12-29                    2021-02-07   \n",
       "722   2022_01_02_23_03_13_321102_normal_grid_search  {'MSA_name': 'Charleston_North_Charleston_SC', 'nrows': None}  {'min_datetime': 2020-12-20 00:00:00, 'max_datetime': 2021-01-29 23:00:00, 'exogenous_model_kwar...    None  normal_grid_search  [0.13292937099671914, 0.13179861311997507, 0.13301170773531704, 0.13262307832913509, 0.133465657...                      2020-12-29                    2021-02-07   \n",
       "0                     2022-08-17_normal_grid_search  {'MSA_name': 'Charleston_North_Charleston_SC', 'nrows': None}  {'min_datetime': 2020-12-20 00:00:00, 'max_datetime': 2021-01-29 23:00:00, 'exogenous_model_kwar...    None  normal_grid_search  [0.13693587669689156, 0.13717026194610019, 0.1373118811364885, 0.13647204640279023, 0.1354236252...                      2020-12-29                    2021-02-07   \n",
       "\n",
       "                                                                     loss_dict_cumulative_predicted_cases                                                                      loss_dict_cumulative_true_cases  loss_dict_cumulative_cases_RMSE  loss_dict_cumulative_cases_MSE  loss_dict_daily_cases_RMSE  loss_dict_daily_cases_MSE  loss_dict_daily_cases_RMSE_time_varying_cdr  loss_dict_smoothed_daily_cases_RMSE_time_varying_cdr  loss_dict_daily_cases_MRE_thres-1_sum  \\\n",
       "1053  [[37091.4, 37407.6, 37774.2, 38129.4, 38529.0, 38914.799999999996, 39350.4, 39786.6, 40203.6, 40...  [40831, 41058, 41709, 41709, 42238, 43005, 43279, 43516, 44103, 44811, 45625, 46605, 46983, 4740...                      5410.904567                    2.934974e+07                  209.273918                 43815.8078                                   200.024065                                             46.986551                               0.284047   \n",
       "722   [[37091.4, 37407.6, 37774.2, 38129.4, 38529.0, 38914.799999999996, 39350.4, 39786.6, 40203.6, 40...  [40831, 41058, 41709, 41709, 42238, 43005, 43279, 43516, 44103, 44811, 45625, 46605, 46983, 4740...                      5410.904567                    2.934974e+07                  209.273918                 43815.8078                                   200.024065                                             46.986551                               0.284047   \n",
       "0     [[37091.4, 37419.0, 37753.799999999996, 38133.0, 38514.0, 38911.799999999996, 39331.799999999996...  [40831, 41058, 41709, 41709, 42238, 43005, 43279, 43516, 44103, 44811, 45625, 46605, 46983, 4740...                      4766.240533                    2.278635e+07                  200.223916                 40102.4006                                   198.486553                                             50.398009                               0.269774   \n",
       "\n",
       "      loss_dict_daily_cases_MRE_thres-10_sum  loss_dict_daily_cases_MRE_thres-20_sum  loss_dict_daily_cases_MRE_thres-50_sum  loss_dict_daily_cases_MRE_thres-100_sum  loss_dict_daily_cases_poisson_NLL_thres-1_logsumexp  loss_dict_daily_cases_poisson_NLL_thres-1_sum  loss_dict_daily_cases_poisson_NLL_thres-10_logsumexp  loss_dict_daily_cases_poisson_NLL_thres-10_sum  ...  agg_county_loss_dict_max_daily_deaths_RMSE  agg_county_loss_dict_unweighted_avg_daily_deaths_per_capita_RMSE  \\\n",
       "1053                                0.284047                                0.284047                                0.284047                                 0.284047                                          1608.063108                                   53535.818340                                           1608.063108                                    53535.818340  ...                                    3.005610                                                          0.000008   \n",
       "722                                 0.284047                                0.284047                                0.284047                                 0.284047                                          1608.063108                                   53535.818340                                           1608.063108                                    53535.818340  ...                                    3.005610                                                          0.000008   \n",
       "0                                   0.269774                                0.269774                                0.269774                                 0.269774                                          1511.356599                                   48198.194238                                           1511.356599                                    48198.194238  ...                                    2.987942                                                          0.000007   \n",
       "\n",
       "      agg_county_loss_dict_weighted_avg_daily_deaths_per_capita_RMSE  agg_county_loss_dict_max_daily_deaths_per_capita_RMSE                                                                                            home_beta    poi_psi p_sick_at_t0 inter_cbg_gamma just_compute_r0 min_datetime        max_datetime  model_init_kwargs                                                                                    simulation_kwargs  \\\n",
       "1053                                                        0.000008                                               0.000008  [0.01496, 0.014773, 0.014586, 0.014399, 0.014211999999999999, 0.014025000000000001, 0.013838, 0....  18.942857         None            None           False   2020-12-20 2021-01-29 23:00:00  {'num_seeds': 30}  {'use_aggregate_mobility': False, 'use_home_proportion_beta': False, 'use_inter_cbg_leak_factor'...   \n",
       "722                                                         0.000008                                               0.000008  [0.01496, 0.014773, 0.014586, 0.014399, 0.014211999999999999, 0.014025000000000001, 0.013838, 0....  18.942857         None            None           False   2020-12-20 2021-01-29 23:00:00  {'num_seeds': 30}  {'use_aggregate_mobility': False, 'use_home_proportion_beta': False, 'use_inter_cbg_leak_factor'...   \n",
       "0                                                           0.000007                                               0.000008  [0.0154, 0.0152075, 0.015015, 0.0148225, 0.014629999999999999, 0.0144375, 0.014245, 0.0140525, 0...  19.500000         None            None           False   2020-12-20 2021-01-29 23:00:00  {'num_seeds': 30}  {'use_aggregate_mobility': False, 'use_home_proportion_beta': False, 'use_inter_cbg_leak_factor'...   \n",
       "\n",
       "                                                   poi_attributes_to_clip  include_cbg_prop_out  include_inter_cbg_travel  include_mask_use      counties_to_track                                                                                  poi_cbg_visits_list                                                                                              poi_ids                                                                                              cbg_ids                        MSA_name  \\\n",
       "1053  {'clip_areas': True, 'clip_dwell_times': True, 'clip_visits': True}                 False                     False              True  [45019, 45015, 45035]  [  (13, 720)\\t0.27553844493055735\\n  (13, 601)\\t0.39470269078646036\\n  (13, 476)\\t0.000679470508...  [sg:0719f423492c4f229548ab9bb46aedb7, sg:09f3799061e744e586b1c9e32b0207cb, sg:1bbe25edd2714a97bf...  [130299203062, 130510107001, 130510108021, 130730303042, 130730303082, 131030303011, 37025041301...  Charleston_North_Charleston_SC   \n",
       "722   {'clip_areas': True, 'clip_dwell_times': True, 'clip_visits': True}                 False                     False              True  [45019, 45015, 45035]  [  (13, 720)\\t0.27553844493055735\\n  (13, 601)\\t0.39470269078646036\\n  (13, 476)\\t0.000679470508...  [sg:0719f423492c4f229548ab9bb46aedb7, sg:09f3799061e744e586b1c9e32b0207cb, sg:1bbe25edd2714a97bf...  [130299203062, 130510107001, 130510108021, 130730303042, 130730303082, 131030303011, 37025041301...  Charleston_North_Charleston_SC   \n",
       "0     {'clip_areas': True, 'clip_dwell_times': True, 'clip_visits': True}                 False                     False              True  [45019, 45015, 45035]  [  (13, 720)\\t0.27553844493055735\\n  (13, 601)\\t0.39470269078646036\\n  (13, 476)\\t0.000679470508...  [sg:0719f423492c4f229548ab9bb46aedb7, sg:09f3799061e744e586b1c9e32b0207cb, sg:1bbe25edd2714a97bf...  [130299203062, 130510107001, 130510108021, 130730303042, 130730303082, 131030303011, 37025041301...  Charleston_North_Charleston_SC   \n",
       "\n",
       "      start_beta  end_beta  beta_ratio  \n",
       "1053     0.01496   0.00748         0.5  \n",
       "722      0.01496   0.00748         0.5  \n",
       "0        0.01540   0.00770         0.5  \n",
       "\n",
       "[3 rows x 194 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# full model\n",
    "msa_name = 'Charleston_North_Charleston_SC'\n",
    "key_to_sort_by = 'loss_dict_smoothed_daily_cases_RMSE_time_varying_cdr'\n",
    "min_datetime = datetime.datetime(2020, 12, 20)\n",
    "max_datetime = datetime.datetime(2021, 1, 30)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "max_timestring = '2021_11_28_02_44_203770'  # group 2\n",
    "min_timestring = '2021_11_27_23_34_000000'\n",
    "\n",
    "max_timestring = '2021_11_27_23_35_000000' # group 1\n",
    "min_timestring = '2021_11_27_19_45_000000'\n",
    "\n",
    "\n",
    "\n",
    "# max_timestring = '2021_11_28_13_15_203770'  # group 4  bad, not use \n",
    "# min_timestring = '2021_11_28_13_05_203770'    \n",
    "\n",
    "# max_timestring = '2021_11_28_13_44_203770'  # group 5  GOOD:  # 2021_11_28_13_18_45_911113_normal_grid_search_normal_grid_search\n",
    "# min_timestring = '2021_11_28_13_23_203770' \n",
    "\n",
    "# max_timestring = '2021_11_28_13_22203770'  # group 4.5 OOD:  # 2021_11_28_13_18_45_911113_normal_grid_search_normal_grid_search\n",
    "# min_timestring = '2021_11_28_13_16203770' \n",
    "\n",
    "max_timestring = '2022_01_02_17_22203770'  # Greenville_Anderson_SC, Best: 2022_01_02_14_46_37_084470_normal_grid_search_normal_grid_search\t\n",
    "min_timestring = '2022_01_02_13_55203770' \n",
    "\n",
    "max_timestring = '2022_01_03_00_22203770'  # Charleston_North_Charleston_SC, Best: 2022_01_02_23_03_13_321102_normal_grid_search_normal_grid_search\t\n",
    "min_timestring = '2022_01_02_20_55203770' \n",
    "\n",
    "# max_timestring = '2022_01_03_30_22203770'  # Charleston_North_Charleston_SC, Best: * 1.1, not good.\t\n",
    "min_timestring = '2022_01_03_00_22203770' \n",
    "\n",
    "max_timestring = '2022_01_03_08_30203770'  # Charleston_North_Charleston_SC, Best: * 1.02,  good.\t\n",
    "min_timestring = '2022_01_03_08_22203770' \n",
    "\n",
    "max_timestring = '2022_11_28_13_00_203770'  # group 3   # 931 , \n",
    "min_timestring = '2020_11_28_09_10_203770'    # 931 2021_11_28_12_33_55_506980_normal_grid_search_normal_grid_search\n",
    "\n",
    "expected_num_jobs = 1050\n",
    "gridsearch_df = evaluate_all_fitted_models_for_msa(msa_name,\n",
    "                                                   min_timestring=min_timestring,\n",
    "                                                   max_timestring=max_timestring, \n",
    "                                                   required_properties={'experiment_to_run':'normal_grid_search'})\n",
    "# assert len(gridsearch_df) == expected_num_jobs, len(gridsearch_df)\n",
    "gridsearch_df['MSA_name'] = gridsearch_df['data_kwargs'].map(lambda x:x['MSA_name'])\n",
    "gridsearch_df['start_beta'] = gridsearch_df['home_beta'].apply(lambda x:x[0])\n",
    "gridsearch_df['end_beta'] = gridsearch_df['home_beta'].apply(lambda x:x[-1])\n",
    "gridsearch_df['beta_ratio'] = gridsearch_df.end_beta.values / gridsearch_df.start_beta.values\n",
    "\n",
    "best_models_df = get_best_models_for_all_msas(gridsearch_df, key_to_sort_by, loss_tolerance=1.2, verbose=True)\n",
    "best_models_df = best_models_df.sort_values(by=key_to_sort_by)\n",
    "\n",
    "print(best_models_df[['home_beta', 'poi_psi']])\n",
    "print(best_models_df['home_beta'].iloc[0].min(), best_models_df['home_beta'].iloc[0].max())\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(7,7))\n",
    "# out = plot_model_fit_for_msa(best_models_df, msa_name, ax, min_datetime, max_datetime)\n",
    "\n",
    "# plt.show()\n",
    "best_models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939faf3f-8e7b-4e20-b9ae-16b591a74ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f59e83b1-e168-4efb-9684-b02893dd222b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-69-c25ba99aab11>, line 2492)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-69-c25ba99aab11>\"\u001b[1;36m, line \u001b[1;32m2492\u001b[0m\n\u001b[1;33m    PATH_TO_ACS_5YR_DATA =r'H:\\covid_mobility_results\\new_census_data\\ACS_2019_5YR_BG\\ACS_race_cbsa_income_2019.csv\u001b[0m\n\u001b[1;37m                                                                                                                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "\n",
    "# from covid_constants_and_util import *\n",
    "from disease_model import Model\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import cm\n",
    "# import helper_methods_for_aggregate_data_analysis as helper\n",
    "# import seaborn as sns\n",
    "import copy\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "import getpass\n",
    "from traceback import print_exc\n",
    "import socket\n",
    "# import psutil\n",
    "import json\n",
    "import subprocess\n",
    "import multiprocessing\n",
    "import IPython\n",
    "from scipy.stats import scoreatpercentile, poisson, binom\n",
    "from scipy.special import logsumexp\n",
    "# from psutil._common import bytes2human\n",
    "from scipy.stats import ttest_ind, rankdata\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import argparse\n",
    "import getpass\n",
    "from collections import Counter\n",
    "\n",
    "###################################################\n",
    "# Loss functions\n",
    "###################################################\n",
    "def MRE(y_true, y_pred):\n",
    "    '''\n",
    "    Computes the median relative error (MRE). y_true and y_pred should\n",
    "    both be numpy arrays.\n",
    "    If y_true and y_pred are 1D, the MRE is returned.\n",
    "    If y_true and y_pred are 2D, e.g., predictions over multiple seeds,\n",
    "    the MRE is computed per row, then averaged.\n",
    "    '''\n",
    "    abs_err = np.absolute(y_true - y_pred)\n",
    "    rel_err = abs_err / y_true\n",
    "    if len(abs_err.shape) == 1:  # this implies y_true and y_pred are 1D\n",
    "        mre = np.median(rel_err)\n",
    "    else:  # this implies at least one of them is 2D\n",
    "        mre = np.mean(np.median(rel_err, axis=1))\n",
    "    return mre\n",
    "\n",
    "def RMSE(y_true, y_pred):\n",
    "    '''\n",
    "    Computes the root mean squared error (RMSE). y_true and y_pred should\n",
    "    both be numpy arrays.\n",
    "    If y_true and y_pred are 1D, the RMSE is returned.\n",
    "    If y_true and y_pred are 2D, e.g., predictions over multiple seeds,\n",
    "    the RMSE is computed per row, then averaged.\n",
    "    '''\n",
    "    sq_err = (y_true - y_pred) ** 2\n",
    "    if len(sq_err.shape) == 1:  # this implies y_true and y_pred are 1D\n",
    "        rmse = np.sqrt(np.mean(sq_err))\n",
    "    else:  # this implies at least one of them is 2D\n",
    "        rmse = np.sqrt(np.mean(sq_err, axis=1))\n",
    "        rmse = np.mean(rmse)\n",
    "    return rmse\n",
    "\n",
    "def MSE(y_true, y_pred):\n",
    "    '''\n",
    "    Computes the mean squared error (MSE). y_true and y_pred should\n",
    "    both be numpy arrays.\n",
    "    '''\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def poisson_NLL(y_true, y_pred, sum_or_logsumexp):\n",
    "    # We clip variance to a min of 4, similar to Li et al. (2020)\n",
    "    # First sum log-likelihoods over days\n",
    "    variance = np.clip(y_pred, 4, None)\n",
    "    ll = np.sum(poisson.logpmf(y_true, variance), axis=1)\n",
    "    # Then sum or logsumexp over seeds\n",
    "    ll = sum_or_logsumexp(ll)\n",
    "    return -ll\n",
    "\n",
    "###################################################\n",
    "# Code for running one model\n",
    "###################################################\n",
    "def fit_disease_model_on_real_data(d,\n",
    "                                   min_datetime,\n",
    "                                   max_datetime,\n",
    "                                   exogenous_model_kwargs,\n",
    "                                   poi_attributes_to_clip,\n",
    "                                   msa_name=None,\n",
    "                                   preload_poi_visits_list_filename=None,\n",
    "                                   poi_cbg_visits_list=None,\n",
    "                                   poi_ids=None,\n",
    "                                   cbg_ids=None,\n",
    "                                   cbg_init_mode='cases',\n",
    "                                   correct_poi_visits=True,\n",
    "                                   multiply_poi_visit_counts_by_census_ratio=True,\n",
    "                                   aggregate_home_cbg_col='aggregated_cbg_population_adjusted_visitor_home_cbgs',\n",
    "                                   poi_hourly_visits_cutoff='all', \n",
    "                                   cbg_count_cutoff=10,\n",
    "                                   cbgs_to_filter_for=None,\n",
    "                                   cbg_groups_to_track=None,\n",
    "                                   counties_to_track=None,\n",
    "                                   include_cbg_prop_out=False,\n",
    "                                   include_inter_cbg_travel=False,\n",
    "                                   # include_mask_use=True,\n",
    "                                   include_mask_use=False,  # Huan\n",
    "                                   model_init_kwargs=None,\n",
    "                                   simulation_kwargs=None,\n",
    "                                   counterfactual_poi_opening_experiment_kwargs=None,\n",
    "                                   counterfactual_retrospective_experiment_kwargs=None,\n",
    "                                   return_model_without_fitting=False,\n",
    "                                   attach_data_to_model=False,\n",
    "                                   model_quality_dict=None,\n",
    "                                   verbose=True):\n",
    "    \"\"\"\n",
    "    Function to prepare data as input for the disease model, and to run the disease simulation on formatted data.\n",
    "    d: pandas DataFrame; POI data from SafeGraph\n",
    "    min_datetime, max_datetime: DateTime objects; the first and last hour to simulate\n",
    "    exogenous_model_kwargs: dict; extra arguments for Model.init_exogenous_variables()\n",
    "        required keys: p_sick_at_t0, poi_psi, and home_beta\n",
    "    poi_attributes_to_clip: dict; which POI attributes to clip\n",
    "        required keys: clip_areas, clip_dwell_times, clip_visits\n",
    "    preload_poi_visits_list_filename: str; name of file from which to load precomputed hourly networks\n",
    "    poi_cbg_visits_list: list of sparse matrices; precomputed hourly networks\n",
    "    correct_poi_visits: bool; whether to correct hourly visit counts with dwell time\n",
    "    multiply_poi_visit_counts_by_census_ratio: bool; whether to upscale visit counts by a constant factor\n",
    "        derived using Census data to try to get real visit volumes\n",
    "    aggregate_col_to_use: str; the field that holds the aggregated CBG proportions for each POI\n",
    "    cbg_count_cutoff: int; the minimum number of POIs a CBG must visit to be included in the model\n",
    "    cbgs_to_filter_for: list; only model CBGs in this list\n",
    "    cbg_groups_to_track: dict; maps group name to CBGs, will track their disease trajectories during simulation\n",
    "    counties_to_track: list; names of counties, will track their disease trajectories during simulation\n",
    "    include_cbg_prop_out: bool; whether to adjust the POI-CBG network based on Social Distancing Metrics (SDM);\n",
    "        should only be used if precomputed poi_cbg_visits_list is not in use\n",
    "    model_init_kwargs: dict; extra arguments for initializing Model\n",
    "    simulation_kwargs: dict; extra arguments for Model.simulate_disease_spread()\n",
    "    counterfactual_poi_opening_experiment_kwargs: dict; arguments for POI category reopening experiments\n",
    "    counterfactual_retrospective_experiment_kwargs: dict; arguments for counterfactual mobility reduction experiment\n",
    "    \"\"\"\n",
    "    if counties_to_track is not None:\n",
    "        counties_to_track = [str(s).zfill(5) for s in counties_to_track]\n",
    "\n",
    "\n",
    "    assert min_datetime <= max_datetime\n",
    "    assert all([k in exogenous_model_kwargs for k in ['poi_psi', 'home_beta']])\n",
    "    assert all([k in poi_attributes_to_clip for k in ['clip_areas', 'clip_dwell_times', 'clip_visits']])\n",
    "    assert all([k in d.columns for k in ['region', 'sub_category', 'safegraph_computed_area_in_square_feet']])\n",
    "    assert aggregate_home_cbg_col in ['aggregated_cbg_population_adjusted_visitor_home_cbgs',\n",
    "                                      'aggregated_visitor_home_cbgs']\n",
    "    if cbg_groups_to_track is None:\n",
    "        cbg_groups_to_track = {}\n",
    "    if model_init_kwargs is None:\n",
    "        model_init_kwargs = {}\n",
    "    if simulation_kwargs is None:\n",
    "        simulation_kwargs = {}\n",
    "\n",
    "    simulation_kwargs['groups_to_track_num_cases_per_poi'] = ['all']\n",
    "\n",
    "    if preload_poi_visits_list_filename is not None:\n",
    "        f = open(preload_poi_visits_list_filename, 'rb')\n",
    "        poi_cbg_visits_list = pickle.load(f)\n",
    "        f.close()\n",
    "    \n",
    "    t0 = time.time()\n",
    "    print('1. Processing SafeGraph data...')\n",
    "    # get hours and check hourly visit info\n",
    "    all_hours = helper.list_hours_in_range(min_datetime, max_datetime)\n",
    "    print(\"Found %d hours in all (%s to %s)\" % (len(all_hours),\n",
    "         get_datetime_hour_as_string(min_datetime),\n",
    "         get_datetime_hour_as_string(max_datetime)))\n",
    "    if poi_cbg_visits_list is not None:\n",
    "        assert len(poi_cbg_visits_list) == len(all_hours)\n",
    "    hour_cols = ['hourly_visits_%s' % get_datetime_hour_as_string(dt) for dt in all_hours]\n",
    "    if poi_cbg_visits_list is None:  # don't need hourly visits in dataframe otherwise\n",
    "\n",
    "        for col in hour_cols:   # Huan\n",
    "            if col not in d.columns:\n",
    "                print(f\"hour_cols {col} is not in d.columns, exit!\")\n",
    "                print(\"d.columns:\\n\", d.columns)\n",
    "                assert (all([col in d.columns for col in hour_cols]))\n",
    "    model_days = helper.list_datetimes_in_range(min_datetime, max_datetime)\n",
    "    home_beta = exogenous_model_kwargs['home_beta']\n",
    "    if type(home_beta) in {np.ndarray, list}:\n",
    "        if len(home_beta) == 2:  # start and end points\n",
    "            home_beta = np.linspace(home_beta[0], home_beta[1], len(model_days))  # increment daily\n",
    "            exogenous_model_kwargs['home_beta'] = home_beta\n",
    "        else:  # should be daily\n",
    "            assert len(home_beta) == len(model_days)  # daily home_beta\n",
    "\n",
    "    # aggregate median_dwell time over weeks\n",
    "    if 'avg_median_dwell' not in d.columns:\n",
    "        weekly_median_dwell_pattern = re.compile('202.*_.*_of_median_dwell')\n",
    "        median_dwell_cols = [col for col in d.columns if re.match(weekly_median_dwell_pattern, col)]\n",
    "        print('Taking median over median_dwell from %s to %s' % (median_dwell_cols[0], median_dwell_cols[-1]))\n",
    "        # note: this may trigger \"RuntimeWarning: All-NaN slice encountered\" if a POI has all nans for median_dwell;\n",
    "        # this is not a problem and will be addressed during clipping and/or POI dropping \n",
    "        d['avg_median_dwell'] = d[median_dwell_cols].median(axis=1).values\n",
    "\n",
    "    # clip before dropping data so we have more POIs as basis for percentiles\n",
    "    # this will also drop POIs whose sub and top categories are too small for clipping\n",
    "    poi_attributes_to_clip = poi_attributes_to_clip.copy()  # copy in case we need to modify\n",
    "    if poi_cbg_visits_list is not None:\n",
    "        poi_attributes_to_clip['clip_visits'] = False\n",
    "        print('Precomputed POI-CBG networks were passed in; will NOT be clipping hourly visits in dataframe')\n",
    "    if poi_attributes_to_clip['clip_areas'] or poi_attributes_to_clip['clip_dwell_times'] or poi_attributes_to_clip['clip_visits']:\n",
    "        d, categories_to_clip, cols_to_clip, thresholds, medians = clip_poi_attributes_in_msa_df(\n",
    "            d, min_datetime, max_datetime, **poi_attributes_to_clip)\n",
    "        print('After clipping, %i POIs' % len(d))\n",
    "\n",
    "    # filter POIs\n",
    "    if poi_ids is None:\n",
    "        #weekly_median_dwell_pattern = re.compile('202*median_dwell')  # raw:   re.compile('202.*_.*_of_median_dwell')   Huan\n",
    "        #median_dwell_cols = [col for col in d.columns if re.match(weekly_median_dwell_pattern, col)] # Huan\n",
    "        #median_dwell_cols = [col for col in d.columns if col.endswith('median_dwell')]\n",
    "        #aggregate_home_cbg_col = median_dwell_cols[0][:18] + aggregate_home_cbg_col\n",
    "        d = d.loc[d[aggregate_home_cbg_col].map(lambda x:len(x.keys()) > 0)]\n",
    "        if verbose: print(\"After dropping for missing CBG home data, %i POIs\" % len(d))\n",
    "        d = d.dropna(subset=['avg_median_dwell'])\n",
    "        if verbose: print(\"After dropping for missing avg_median_dwell, %i POIs\" % len(d))\n",
    "        d = d.dropna(subset=['safegraph_computed_area_in_square_feet'])\n",
    "        if verbose: print(\"After dropping for missing area, %i POIs\" % len(d))\n",
    "        curr_num_visits = np.nansum(d[hour_cols].values)\n",
    "        if poi_hourly_visits_cutoff == 'all' or poi_hourly_visits_cutoff >= len(hour_cols):  # POI must have non-missing hourly visits data for every hour to be included\n",
    "            d = d.dropna(subset=hour_cols)\n",
    "            new_num_visits = np.sum(d[hour_cols].values)\n",
    "            if verbose: print(\"After dropping for missing any hours, %i POIs; kept %.2f%% of visits\" % \n",
    "                              (len(d), 100. * new_num_visits / curr_num_visits))\n",
    "        else:  # cutoff based on simulation hours\n",
    "            assert poi_hourly_visits_cutoff >= 0\n",
    "            num_nonnan_hours = np.sum(~pd.isnull(d[hour_cols]), axis=1)\n",
    "            poi_passes = num_nonnan_hours >= poi_hourly_visits_cutoff\n",
    "            d = d.loc[poi_passes]\n",
    "            fill_with_0 = {k:0 for k in hour_cols}\n",
    "            d = d.fillna(value=fill_with_0)\n",
    "            new_num_visits = np.sum(d[hour_cols].values)\n",
    "            if verbose: print(\"After dropping for having less than %d hours of data, %i POIs; kept %.2f%% of visits\" % \n",
    "                              (poi_hourly_visits_cutoff, len(d), 100. * new_num_visits / curr_num_visits))\n",
    "    else:\n",
    "        pois_in_df = set(d.index)\n",
    "        n_missing = len(set(poi_ids) - pois_in_df)\n",
    "        print('Received %d pre-specified POI ids -> missing %d in dataframe' % (len(poi_ids), n_missing))\n",
    "        assert n_missing == 0  # all poi_ids should be in df \n",
    "        d = d.loc[poi_ids]\n",
    "        assert len(d) == len(poi_ids)\n",
    "        if poi_cbg_visits_list is None:\n",
    "            is_null = pd.isnull(d[hour_cols]).values\n",
    "            print('%d / %d hours are null -> filling with 0' % (np.sum(is_null), len(hour_cols) * len(d)))\n",
    "            fill_with_0 = {k:0 for k in hour_cols}\n",
    "            d = d.fillna(value=fill_with_0)\n",
    "        else:\n",
    "            assert poi_cbg_visits_list[0].shape[0] == len(poi_ids)\n",
    "    M = len(d)  # POI counts\n",
    "\n",
    "    # filter CBGs\n",
    "    poi_cbg_proportions = d[aggregate_home_cbg_col].values\n",
    "    # an array of dicts; each dict represents CBG distribution for POI\n",
    "\n",
    "    acs_d = helper.load_and_reconcile_multiple_acs_data()\n",
    "    acs_d['GEOID'] = acs_d['GEOID'].str[-12:]\n",
    "    acs_d['county_code'] = acs_d['GEOID'].str[:5]\n",
    "    acs_d['census_block_group'] = acs_d['GEOID']\n",
    "    cbgs_to_census_pops = dict(zip(acs_d['GEOID'].values,\n",
    "                                   acs_d['total_cbg_population_2019_5YR'].values))  # use most recent population data\n",
    "    if cbg_ids is None:\n",
    "        all_cbgs = [a for b in poi_cbg_proportions for a in b.keys()]\n",
    "        cbg_counts = Counter(all_cbgs).most_common()\n",
    "        cbg_counts = [(str(cgb).zfill(12), count) for cgb, count in cbg_counts]\n",
    "        all_unique_cbgs = [cbg for cbg, count in cbg_counts if count >= cbg_count_cutoff]\n",
    "        # only keep CBGs that have visited at least this many POIs\n",
    "\n",
    "        if verbose: print(\"After dropping CBGs that appear in < %i POIs, %i CBGs (%2.1f%%)\" %\n",
    "              (cbg_count_cutoff, len(all_unique_cbgs), 100.*len(all_unique_cbgs)/len(cbg_counts)))\n",
    "        if cbgs_to_filter_for is not None:\n",
    "            all_unique_cbgs = [a for a in all_unique_cbgs if a in cbgs_to_filter_for]\n",
    "            print(\"After filtering for CBGs in MSA, %i CBGs\" % len(all_unique_cbgs))\n",
    "        all_unique_cbgs2 = []\n",
    "        for idx, cbg in enumerate(all_unique_cbgs):\n",
    "            cgb_populoation = cbgs_to_census_pops.get(cbg, False)  # cbgs_to_census_pops. All 202k groupblock in the U.S>\n",
    "            if cgb_populoation:\n",
    "                if cgb_populoation > 0:\n",
    "                    all_unique_cbgs2.append(str(int(cbg)))\n",
    "        all_unique_cbgs  = all_unique_cbgs2\n",
    "        #all_unique_cbgs = [cbg for cbg in all_unique_cbgs if cbgs_to_census_pops[cbg] > 0]\n",
    "        if verbose: print('After dropping CBGs with population size 0 in ACS data, %i CBGs' % len(all_unique_cbgs))\n",
    "        all_unique_cbgs = sorted(all_unique_cbgs)      # order CBGs lexicographically\n",
    "    else:\n",
    "        print('Received %d pre-specified CBG ids' % len(cbg_ids))\n",
    "        all_unique_cbgs = cbg_ids\n",
    "    N = len(all_unique_cbgs)\n",
    "    all_unique_cbgs = [str(s).zfill(12) for s in all_unique_cbgs]\n",
    "    cbgs_to_idxs = dict(zip(all_unique_cbgs, range(N)))\n",
    "    print(\"\\ncbgs_to_idxs:\\n\", cbgs_to_idxs)\n",
    "    print('FINAL: number of CBGs (N) = %d, number of POIs (M) = %d' % (N, M))\n",
    "\n",
    "    # convert data structures with CBG names to CBG indices\n",
    "    # fill the matric using proportions\n",
    "    poi_cbg_proportions_mat = np.zeros((M, N))\n",
    "    # for each POI\n",
    "    for poi_idx, old_dict in enumerate(poi_cbg_proportions):\n",
    "        for string_key, prop in old_dict.items():\n",
    "            string_key = str(string_key).zfill(12)\n",
    "            if string_key in cbgs_to_idxs:\n",
    "                int_key = cbgs_to_idxs[string_key]\n",
    "                # poi_cbg_proportions_mat[poi_idx, int_key] = prop\n",
    "                poi_cbg_proportions_mat[poi_idx, int_key] = prop\n",
    "    E = np.sum(poi_cbg_proportions_mat > 0)\n",
    "    print('Num connected POI-CBG pairs (E) = %d, network density (E/N) = %.3f' %\n",
    "          (E, E / N))  # avg num adjacent POIs per CBG\n",
    "    if poi_cbg_visits_list is not None:\n",
    "        expected_M, expected_N = poi_cbg_visits_list[0].shape\n",
    "        assert M == expected_M\n",
    "        assert N == expected_N\n",
    "\n",
    "    cbg_idx_groups_to_track = {}\n",
    "    for group in cbg_groups_to_track:\n",
    "        cbg_idx_groups_to_track[group] = [\n",
    "            cbgs_to_idxs[a] for a in cbg_groups_to_track[group] if a in cbgs_to_idxs]\n",
    "        if verbose: print(f'{len(cbg_groups_to_track[group])} CBGs in {group} -> matched {len(cbg_idx_groups_to_track[group])} ({(len(cbg_idx_groups_to_track[group]) / len(cbg_groups_to_track[group])):.3f})')\n",
    "\n",
    "    # get POI-related variables\n",
    "    all_states = sorted(list(set(d['region'].dropna())))\n",
    "    poi_subcategory_types = d['sub_category'].values\n",
    "    poi_areas = d['safegraph_computed_area_in_square_feet'].values\n",
    "    poi_dwell_times = d['avg_median_dwell'].values\n",
    "    poi_dwell_time_correction_factors = (poi_dwell_times / (poi_dwell_times+60)) ** 2\n",
    "    print('Dwell time correction factors: mean = %.2f, min = %.2f, max = %.2f' %\n",
    "          (np.mean(poi_dwell_time_correction_factors), min(poi_dwell_time_correction_factors), max(poi_dwell_time_correction_factors)))\n",
    "    if poi_cbg_visits_list is None:\n",
    "        poi_time_counts = d[hour_cols].values\n",
    "    else:\n",
    "        poi_time_counts = None  # don't need poi_time_counts if precomputed is provided\n",
    "    \n",
    "    if correct_poi_visits:  # applying correction to visits so that they represent number of visitors present per hour, \n",
    "        # not number of visits arriving per hour \n",
    "        if poi_cbg_visits_list is not None:\n",
    "            print('Precomputed POI-CBG networks were passed in; will NOT be applying dwell-time-based correction to hourly visits in dataframe')\n",
    "        else:\n",
    "            print('Correcting POI hourly visit vectors...')\n",
    "            new_poi_time_counts = []\n",
    "            for i, (visit_vector, dwell_time) in enumerate(list(zip(poi_time_counts, poi_dwell_times))):\n",
    "                new_poi_time_counts.append(correct_visit_vector(visit_vector, dwell_time))\n",
    "            poi_time_counts = np.array(new_poi_time_counts)\n",
    "            d[hour_cols] = poi_time_counts\n",
    "            new_hourly_visit_count = np.sum(poi_time_counts)\n",
    "            print('After correcting, %.2f hourly visits' % new_hourly_visit_count)\n",
    "    if multiply_poi_visit_counts_by_census_ratio:  # scale visits based on undersampling\n",
    "        if poi_cbg_visits_list is not None:\n",
    "            print('Precomputed POI-CBG networks were passed in; will NOT be applying undersampling correction to hourly visits in dataframe')\n",
    "        else:\n",
    "            # Get overall undersampling factor.\n",
    "            # Basically we take ratio of ACS US population to SafeGraph population in Feb 2020.\n",
    "            # SafeGraph thinks this is reasonable.\n",
    "            # https://safegraphcovid19.slack.com/archives/C0109NPA543/p1586801883190800?thread_ts=1585770817.335800&cid=C0109NPA543\n",
    "            total_us_population_in_50_states_plus_dc = acs_d.loc[acs_d['state_code'].map(lambda x:x in FIPS_CODES_FOR_50_STATES_PLUS_DC), 'total_cbg_population_2019_5YR'].sum()\n",
    "            safegraph_visitor_count_df = pd.read_csv('/media/gpu/easystore/Safegraph/Weekly Places Patterns (for data from 2020-11-30 to Present)/visit_panel_summary/2021/02/03/21/visit_panel_summary.csv')\n",
    "            safegraph_visitor_count = safegraph_visitor_count_df.loc[safegraph_visitor_count_df['state'] == 'ALL_STATES', 'num_unique_visitors'].iloc[0]\n",
    "\n",
    "            # remove a few safegraph visitors from non-US states.\n",
    "            two_letter_codes_for_states = set([a.lower() for a in codes_to_states if codes_to_states[a] in JUST_50_STATES_PLUS_DC])\n",
    "            safegraph_visitor_count_to_non_states = safegraph_visitor_count_df.loc[safegraph_visitor_count_df['state'].map(lambda x:x not in two_letter_codes_for_states and x != 'ALL_STATES'), 'num_unique_visitors'].sum()\n",
    "            if verbose:\n",
    "                print(\"Removing %2.3f%% of people from SafeGraph count who are not in 50 states or DC\" %\n",
    "                    (100. * safegraph_visitor_count_to_non_states/safegraph_visitor_count))\n",
    "            safegraph_visitor_count = safegraph_visitor_count - safegraph_visitor_count_to_non_states\n",
    "            correction_factor = 1. * total_us_population_in_50_states_plus_dc / safegraph_visitor_count\n",
    "            if verbose:\n",
    "                print(\"Total US population from ACS: %i; total safegraph visitor count: %i; correction factor for POI visits is %2.3f\" %\n",
    "                    (total_us_population_in_50_states_plus_dc,\n",
    "                    safegraph_visitor_count,\n",
    "                    correction_factor))\n",
    "            # use South Carolina population to obtain the correction factor.\n",
    "            state_population = 5149000\n",
    "            state_unique_visitors = 415034   # 2021-02-03\n",
    "            correction_factor = state_population / state_unique_visitors\n",
    "            print(\n",
    "                \"Total state population of South Carolina: %i; total safegraph visitor count in South Carolina: %i; correction factor for POI visits is %2.3f\" %\n",
    "                (state_population,\n",
    "                 state_unique_visitors,\n",
    "                 correction_factor))\n",
    "\n",
    "            poi_time_counts = poi_time_counts * correction_factor\n",
    "            d[hour_cols] = poi_time_counts\n",
    "\n",
    "    # get CBG-related variables from census data\n",
    "    print('2. Processing ACS data...')\n",
    "    all_unique_cbgs = [str(round(float(s))).zfill(12) for s in all_unique_cbgs]\n",
    "    print(\"all_unique_cbgs[0]:\", type(all_unique_cbgs[0]), all_unique_cbgs[0])\n",
    "    cbg_sizes = np.array([cbgs_to_census_pops[a] for a in all_unique_cbgs])\n",
    "    assert np.sum(np.isnan(cbg_sizes)) == 0\n",
    "    if verbose:\n",
    "        print('CBGs: median population size = %d, sum of population sizes = %d' %\n",
    "          (np.median(cbg_sizes), np.sum(cbg_sizes)))\n",
    "        \n",
    "    if counties_to_track is not None:  # extract blockgroups for each county\n",
    "        print('Found %d counties to track...' % len(counties_to_track))\n",
    "        county2cbgs = {}\n",
    "        for county in counties_to_track:\n",
    "            county_cbgs = acs_d[acs_d['county_code'] == county]['census_block_group'].values\n",
    "            orig_len = len(county_cbgs)\n",
    "            county_cbgs = sorted(set(county_cbgs).intersection(set(all_unique_cbgs)))\n",
    "            if orig_len > 0:\n",
    "                coverage = len(county_cbgs) / orig_len\n",
    "                if coverage < 0.8:\n",
    "                    print('Low coverage warning: only modeling %d/%d (%.1f%%) of the CBGs in %s' %\n",
    "                          (len(county_cbgs), orig_len, 100. * coverage, county))\n",
    "            if len(county_cbgs) > 0:\n",
    "                county_cbg_idx = np.array([cbgs_to_idxs[a] for a in county_cbgs])\n",
    "                county2cbgs[county] = (county_cbgs, county_cbg_idx)\n",
    "                cbg_idx_groups_to_track[county] = county_cbg_idx\n",
    "        print('Tracking infection trajectories from %d of the counties' % len(county2cbgs))\n",
    "    else:\n",
    "        county2cbgs = None\n",
    "\n",
    "    # turn off warnings temporarily so that using > or <= on np.nan does not cause warnings\n",
    "    np.warnings.filterwarnings('ignore')\n",
    "    cbg_idx_to_track = set(range(N))  # include all CBGs\n",
    "    for attribute in ['p_black', 'p_white', 'median_household_income']:\n",
    "        # attr_col_name = '%s_2017_5YR' % attribute  # using 5-year ACS data for attributes bc less noisy\n",
    "        attr_col_name = attribute #  Huan\n",
    "        assert attr_col_name in acs_d.columns\n",
    "        acs_d['census_block_group'] = acs_d['GEOID']  # Huan\n",
    "        mapper_d = dict(zip(acs_d['census_block_group'].values, acs_d[attr_col_name].values))\n",
    "        attribute_vals = np.array([mapper_d[a] if a in mapper_d and cbgs_to_idxs[a] in cbg_idx_to_track else np.nan for a in all_unique_cbgs])\n",
    "        non_nan_vals = attribute_vals[~np.isnan(attribute_vals)]\n",
    "        median_cutoff = np.median(non_nan_vals)\n",
    "        if verbose:\n",
    "            print(\"Attribute %s: was able to compute for %2.1f%% out of %i CBGs, median is %2.3f\" %\n",
    "                (attribute, 100. * len(non_nan_vals) / len(cbg_idx_to_track),\n",
    "                 len(cbg_idx_to_track), median_cutoff))\n",
    "\n",
    "        cbg_idx_groups_to_track[f'{attribute}_above_median'] = list(set(np.where(attribute_vals > median_cutoff)[0]).intersection(cbg_idx_to_track))\n",
    "        cbg_idx_groups_to_track[f'{attribute}_below_median'] = list(set(np.where(attribute_vals <= median_cutoff)[0]).intersection(cbg_idx_to_track))\n",
    "\n",
    "        top_decile = scoreatpercentile(non_nan_vals, 90)\n",
    "        bottom_decile = scoreatpercentile(non_nan_vals, 10)\n",
    "        cbg_idx_groups_to_track[f'{attribute}_top_decile'] = list(set(np.where(attribute_vals >= top_decile)[0]).intersection(cbg_idx_to_track))\n",
    "        cbg_idx_groups_to_track[f'{attribute}_bottom_decile'] = list(set(np.where(attribute_vals <= bottom_decile)[0]).intersection(cbg_idx_to_track))\n",
    "\n",
    "        if county2cbgs is not None:\n",
    "            above_median_in_county = []\n",
    "            below_median_in_county = []\n",
    "            for county in county2cbgs:\n",
    "                county_cbgs, cbg_idx = county2cbgs[county]\n",
    "                attribute_vals = np.array([mapper_d[a] if a in mapper_d and cbgs_to_idxs[a] in cbg_idx_to_track else np.nan for a in county_cbgs])\n",
    "                non_nan_vals = attribute_vals[~np.isnan(attribute_vals)]\n",
    "                median_cutoff = np.median(non_nan_vals)\n",
    "                above_median_idx = cbg_idx[np.where(attribute_vals > median_cutoff)[0]]\n",
    "                above_median_idx = list(set(above_median_idx).intersection(cbg_idx_to_track))\n",
    "                above_median_in_county.extend(above_median_idx)\n",
    "                below_median_idx = cbg_idx[np.where(attribute_vals <= median_cutoff)[0]]\n",
    "                below_median_idx = list(set(below_median_idx).intersection(cbg_idx_to_track))\n",
    "                below_median_in_county.extend(below_median_idx)\n",
    "            cbg_idx_groups_to_track[f'{attribute}_above_median_in_own_county'] = above_median_in_county\n",
    "            cbg_idx_groups_to_track[f'{attribute}_below_median_in_own_county'] = below_median_in_county\n",
    "    np.warnings.resetwarnings()\n",
    "\n",
    "    cbg_day_prop_out = None\n",
    "    inter_cbg_travel = None\n",
    "    if include_cbg_prop_out or include_inter_cbg_travel:\n",
    "        sdm_df = helper.load_social_distancing_metrics(model_days)\n",
    "        sdm_df = sdm_df.loc[all_unique_cbgs]\n",
    "        cols_to_keep = ['%s.%s.%s' % (dt.year, dt.month, dt.day) for dt in model_days]\n",
    "        if include_cbg_prop_out:\n",
    "            print('Giving model daily proportion out for %s to %s' % (cols_to_keep[0], cols_to_keep[-1]))\n",
    "            cbg_day_prop_out = helper.compute_cbg_day_prop_out(sdm_df)\n",
    "            assert all([c1 == c2 for c1, c2 in zip(cbg_day_prop_out['census_block_group'].values, all_unique_cbgs)])\n",
    "            assert((len(cols_to_keep) * 24) == len(hour_cols))\n",
    "            cbg_day_prop_out = cbg_day_prop_out[cols_to_keep].values     \n",
    "        if include_inter_cbg_travel:\n",
    "            print('Giving model inter-CBG travel for %s to %s' % (cols_to_keep[0], cols_to_keep[-1]))\n",
    "            inter_cbg_travel = helper.compute_daily_inter_cbg_travel(sdm_df, cbg_sizes, model_days)\n",
    "            # num_cbgs x num_days; avg num visits to other CBGs per capita\n",
    "            inter_cbg_travel = (inter_cbg_travel.values.T / (cbg_sizes+1)).T\n",
    "    include_mask_use = False # Huan\n",
    "    if include_mask_use:\n",
    "        day_strs = [dt.strftime('%Y-%m-%d') for dt in model_days]\n",
    "        most_common_state = d['region'].value_counts().idxmax()\n",
    "        print('Loading mask use data for state=%s' % most_common_state)\n",
    "        mask_df = helper.load_mask_use_data(most_common_state)\n",
    "        mask_df = mask_df[mask_df.date.isin(day_strs)]\n",
    "        assert len(mask_df) == len(model_days)\n",
    "        mask_data = mask_df['mask use'].values / 100\n",
    "        assert all((mask_data >= 0) & (mask_data <= 1))\n",
    "    else:\n",
    "        mask_data = None\n",
    "\n",
    "    # Given a starting date and the associated initial S, E, I, R rate. See the initialized_seir.ipynb\n",
    "    if 'p_sick_at_t0' not in exogenous_model_kwargs or exogenous_model_kwargs['p_sick_at_t0'] is None:\n",
    "        fn = os.path.join(PATH_TO_SEIR_INIT, 'all_cbgs_s=%s.csv' % (min_datetime.strftime('%Y-%m-%d')))\n",
    "        assert os.path.isfile(fn),  \"Cannot find file: %s\" % fn\n",
    "        cbg_init_shrinkage_alpha = 0.5 if min_datetime < datetime.datetime(2020, 4, 1) else 0.1  # if early, we trust estimates less, want to shrink more\n",
    "        print('Loading CBG init data; basing inferred SEIR on %s and applying shrinkage of %s' % (cbg_init_mode, cbg_init_shrinkage_alpha))\n",
    "        init_df = pd.read_csv(fn)\n",
    "        init_df['census_block_group'] = init_df['census_block_group'].astype(str).str.zfill(12)\n",
    "        init_df['county_fips'] = init_df['county_fips'].astype(str).str.zfill(5)\n",
    "        # print('init_df.dtypes')\n",
    "        # print(init_df.dtypes)\n",
    "\n",
    "        # print('init_df.head(5)')\n",
    "        # print(init_df.head(5))\n",
    "\n",
    "        init_df = init_df.set_index('census_block_group')\n",
    "        # print('all_unique_cbgs')\n",
    "        # print(all_unique_cbgs)\n",
    "        init_df = init_df.loc[all_unique_cbgs]\n",
    "        is_null = pd.isnull(init_df['county_fips']).values\n",
    "        # print('init_df.head(5)')\n",
    "        # print('init_df.index')\n",
    "        # print(init_df.index)\n",
    "        assert np.sum(is_null) == 0\n",
    "        states_to_init = ['E', 'I', 'R']\n",
    "        eir_cols = ['%s_%s' % (cbg_init_mode, state) for state in states_to_init]\n",
    "        initial_conditions = init_df[eir_cols].values\n",
    "        for idx, state in enumerate(states_to_init):\n",
    "            curr_prop = initial_conditions[:, idx] / cbg_sizes\n",
    "            mean_prop = np.mean(curr_prop)\n",
    "            shrunken_prop = (cbg_init_shrinkage_alpha * mean_prop) + ((1 - cbg_init_shrinkage_alpha) * curr_prop)  # shrink to mean\n",
    "            invalid_prop = shrunken_prop > 1\n",
    "            print('Found %d CBGs with inferred proportion in %s > 1 -> clipping' % (np.sum(invalid_prop), state))\n",
    "            shrunken_prop = np.clip(shrunken_prop, None, 1)\n",
    "            print('Proportion in %s: min = %.4f, 25th = %.4f, median = %.4f, 75th = %.4f, max = %.4f' % \n",
    "                  (state, np.min(shrunken_prop), np.percentile(shrunken_prop, 25), \n",
    "                   np.percentile(shrunken_prop, 50), np.percentile(shrunken_prop, 75),\n",
    "                   np.max(shrunken_prop)))\n",
    "            initial_conditions[:, idx] = np.round(shrunken_prop * cbg_sizes, 0).astype(int)\n",
    "    else:\n",
    "        initial_conditions = None\n",
    "        \n",
    "    # If trying to get the counterfactual where social activity doesn't change, just repeat first week of dataset.\n",
    "    # We put this in exogenous_model_kwargs because it actually affects how the model runs, not just the data input.\n",
    "    if 'just_compute_r0' in exogenous_model_kwargs and exogenous_model_kwargs['just_compute_r0']:\n",
    "        print('Running model to compute r0 -> looping first week visit counts')\n",
    "        # simulate out 15 weeks just so we are sure all cases are gone.\n",
    "        max_datetime = min_datetime + datetime.timedelta(hours=(168*15)-1)\n",
    "        all_hours = helper.list_hours_in_range(min_datetime, max_datetime)\n",
    "        print(\"Extending time period; simulation now ends at %s (%d hours)\" % (max(all_hours), len(all_hours)))\n",
    "        if poi_cbg_visits_list is not None:\n",
    "            assert len(poi_cbg_visits_list) >= 168  # ensure that we have at least a week to model\n",
    "            new_visits_list = []\n",
    "            for i in range(168 * 15):\n",
    "                first_week_idx = i % 168  # map to corresponding hour in first week\n",
    "                new_visits_list.append(poi_cbg_visits_list[first_week_idx].copy())\n",
    "            poi_cbg_visits_list = new_visits_list\n",
    "            assert len(poi_cbg_visits_list) == len(all_hours)\n",
    "        else:\n",
    "            assert poi_time_counts.shape[1] >= 168  # ensure that we have at least a week to model\n",
    "            first_week = poi_time_counts[:, :168]\n",
    "            poi_time_counts = np.tile(first_week, (1, 15))\n",
    "            if cbg_day_prop_out is not None:\n",
    "                assert cbg_day_prop_out.shape[1] >= 7\n",
    "                first_week = cbg_day_prop_out[:, :7]\n",
    "                cbg_day_prop_out = np.tile(first_week, (1, 15))\n",
    "            assert poi_time_counts.shape[1] == len(all_hours)\n",
    "        assert cbg_day_prop_out is None  # R0 calibration should be simplest version of model \n",
    "        assert inter_cbg_travel is None\n",
    "        assert not type(home_beta) in {np.ndarray, list}  # should run R0 calibration with constant beta (only first week)\n",
    "        if mask_data is not None:  # shape: 1 x num_days\n",
    "            avg_first_week_mask = np.mean(mask_data[:7])\n",
    "            print('Average mask use in first week of March: %.3f' % avg_first_week_mask)\n",
    "            mask_data = np.ones(7 * 15) * avg_first_week_mask  # use constant mask wearing for single week\n",
    "\n",
    "    # If we want to run counterfactual reopening simulations\n",
    "    intervention_cost = None\n",
    "    if counterfactual_poi_opening_experiment_kwargs is not None:\n",
    "        if poi_cbg_visits_list is None:\n",
    "            raise Exception('Missing poi_cbg_visits_list; reopening experiments should be run with IPF output')\n",
    "        extra_weeks_to_simulate = counterfactual_poi_opening_experiment_kwargs['extra_weeks_to_simulate']\n",
    "        assert extra_weeks_to_simulate >= 0\n",
    "        intervention_datetime = counterfactual_poi_opening_experiment_kwargs['intervention_datetime']\n",
    "        version = counterfactual_poi_opening_experiment_kwargs['version']\n",
    "        \n",
    "        if cbg_day_prop_out is not None:  # shape: num_cbgs x num_days\n",
    "            to_concat = [cbg_day_prop_out.copy()]\n",
    "            for w in range(extra_weeks_to_simulate):\n",
    "                to_concat.append(cbg_day_prop_out[:, -7:].copy())  # loop final week\n",
    "            cbg_day_prop_out = np.concatenate(to_concat, axis=1)  # concatenate along rows\n",
    "        if inter_cbg_travel is not None:  # shape: num_cbgs x num_days\n",
    "            to_concat = [inter_cbg_travel.copy()]\n",
    "            for w in range(extra_weeks_to_simulate):\n",
    "                to_concat.append(inter_cbg_travel[:, -7:].copy())  # loop final week\n",
    "            inter_cbg_travel = np.concatenate(to_concat, axis=1)  # concatenate along rows\n",
    "        if mask_data is not None:  # shape: 1 x num_days\n",
    "            to_concat = [mask_data.copy()]\n",
    "            for w in range(extra_weeks_to_simulate):\n",
    "                to_concat.append(mask_data[-7:].copy())  # loop final week\n",
    "            mask_data = np.concatenate(to_concat, axis=0)  # concatenate along rows\n",
    "        if type(home_beta) in {np.ndarray, list}:\n",
    "            additional_home_beta = np.ones(7 * extra_weeks_to_simulate) * home_beta[-1]  # keep final home beta, repeat\n",
    "            home_beta = np.concatenate([home_beta, additional_home_beta], axis=0)\n",
    "            exogenous_model_kwargs['home_beta'] = home_beta\n",
    "        \n",
    "        # v1 is from Nature paper, uses beginning of March as full reopening, only allows one category to be \n",
    "        # modified at a time\n",
    "        if version == 'v1':  \n",
    "            orig_num_hours = len(all_hours)\n",
    "            all_hours = helper.list_hours_in_range(min_datetime, max_datetime + datetime.timedelta(hours=168 * extra_weeks_to_simulate))\n",
    "            print(\"Extending time period; simulation now ends at %s (%d hours)\" % (max(all_hours), len(all_hours)))\n",
    "            assert(intervention_datetime in all_hours)\n",
    "            intervention_hour_idx = all_hours.index(intervention_datetime)\n",
    "            if 'top_category' in counterfactual_poi_opening_experiment_kwargs:\n",
    "                top_category = counterfactual_poi_opening_experiment_kwargs['top_category']\n",
    "            else:\n",
    "                top_category = None\n",
    "            if 'sub_category' in counterfactual_poi_opening_experiment_kwargs:\n",
    "                sub_category = counterfactual_poi_opening_experiment_kwargs['sub_category']\n",
    "            else:\n",
    "                sub_category = None\n",
    "            poi_categories = d[['top_category', 'sub_category']]\n",
    "\n",
    "            # must have one but not both of these arguments\n",
    "            assert (('alpha' in counterfactual_poi_opening_experiment_kwargs) + ('full_activity_alpha' in counterfactual_poi_opening_experiment_kwargs)) == 1\n",
    "            # the original alpha - post-intervention is interpolation between no reopening and full activity\n",
    "            if 'alpha' in counterfactual_poi_opening_experiment_kwargs:\n",
    "                alpha = counterfactual_poi_opening_experiment_kwargs['alpha']\n",
    "                assert alpha >= 0 and alpha <= 1\n",
    "                poi_cbg_visits_list, intervention_cost = apply_interventions_to_poi_cbg_matrices(poi_cbg_visits_list,\n",
    "                                            poi_categories, poi_areas, all_hours, intervention_hour_idx,\n",
    "                                            alpha, extra_weeks_to_simulate, top_category, sub_category, interpolate=True)\n",
    "            # post-intervention is alpha-percent of full activity (no interpolation)\n",
    "            else:\n",
    "                alpha = counterfactual_poi_opening_experiment_kwargs['full_activity_alpha']\n",
    "                assert alpha >= 0 and alpha <= 1\n",
    "                poi_cbg_visits_list, intervention_cost = apply_interventions_to_poi_cbg_matrices(poi_cbg_visits_list,\n",
    "                                            poi_categories, poi_areas, all_hours, intervention_hour_idx,\n",
    "                                            alpha, extra_weeks_to_simulate, top_category, sub_category, interpolate=False)\n",
    "\n",
    "            # should be used in tandem with alpha or full_activity_alpha, since the timeseries is extended\n",
    "            # in those blocks; this part just caps post-intervention visits to alpha-percent of max capacity\n",
    "            if 'max_capacity_alpha' in counterfactual_poi_opening_experiment_kwargs:\n",
    "                max_capacity_alpha = counterfactual_poi_opening_experiment_kwargs['max_capacity_alpha']\n",
    "                assert max_capacity_alpha >= 0 and max_capacity_alpha <= 1\n",
    "                poi_visits = np.zeros((M, orig_num_hours))   # num pois x num hours\n",
    "                for t, poi_cbg_visits in enumerate(poi_cbg_visits_list[:orig_num_hours]):\n",
    "                    poi_visits[:, t] = poi_cbg_visits @ np.ones(N)\n",
    "                max_per_poi = np.max(poi_visits, axis=1)  # get historical max capacity per POI\n",
    "                alpha_max_per_poi = np.clip(max_capacity_alpha * max_per_poi, 1e-10, None)  # so that we don't divide by 0\n",
    "                orig_total_activity = 0\n",
    "                capped_total_activity = 0\n",
    "                for t in range(intervention_hour_idx, len(poi_cbg_visits_list)):\n",
    "                    poi_cbg_visits = poi_cbg_visits_list[t]\n",
    "                    num_visits_per_poi = poi_cbg_visits @ np.ones(N)\n",
    "                    orig_total_activity += np.sum(num_visits_per_poi)\n",
    "                    ratio_per_poi = num_visits_per_poi / alpha_max_per_poi\n",
    "                    clipping_idx = ratio_per_poi > 1  # identify which POIs need to be clipped\n",
    "                    poi_multipliers = np.ones(M)\n",
    "                    poi_multipliers[clipping_idx] = 1 / ratio_per_poi[clipping_idx]\n",
    "                    adjusted_poi_cbg_visits = poi_cbg_visits.transpose().multiply(poi_multipliers).transpose().tocsr()\n",
    "                    capped_total_activity += np.sum(adjusted_poi_cbg_visits @ np.ones(N))\n",
    "                    poi_cbg_visits_list[t] = adjusted_poi_cbg_visits\n",
    "                print('Finished capping visits at %.1f%% of max capacity -> kept %.4f%% of visits' %\n",
    "                      (100. * max_capacity_alpha, 100 * capped_total_activity / orig_total_activity))\n",
    "                intervention_cost['total_activity_after_max_capacity_capping'] = capped_total_activity\n",
    "        \n",
    "        # v2 was implemented post-Nature paper, uses 2019 IPF output as full reopening, takes in dictionary of category\n",
    "        # to alpha where alpha represents the percentage of 2019 activity to keep for category\n",
    "        else:\n",
    "            assert msa_name is not None\n",
    "            category2alpha = counterfactual_poi_opening_experiment_kwargs['category_to_alpha']\n",
    "            poi_categories = d.sub_category.values\n",
    "            all_hours, poi_cbg_visits_list, total_post_intervention_visits = apply_different_percentages_of_2019_levels(\n",
    "                msa_name, category2alpha, poi_cbg_visits_list, poi_categories, all_hours, intervention_datetime, \n",
    "                extra_weeks_to_simulate, agg_poi_cbg_visits=poi_cbg_proportions_mat)\n",
    "            print('Total post intervention visits: %.3fM' % (total_post_intervention_visits / 1000000))\n",
    "            intervention_cost = {}\n",
    "            intervention_cost['total_num_visits_post_intervention'] = total_post_intervention_visits\n",
    "\n",
    "    if counterfactual_retrospective_experiment_kwargs is not None:\n",
    "        # must have one but not both of these arguments\n",
    "        assert (('distancing_degree' in counterfactual_retrospective_experiment_kwargs) + ('shift_in_days' in counterfactual_retrospective_experiment_kwargs)) == 1\n",
    "        if poi_cbg_visits_list is None:\n",
    "            raise Exception('Retrospective experiments are only implemented for when poi_cbg_visits_list is precomputed')\n",
    "        if 'distancing_degree' in counterfactual_retrospective_experiment_kwargs:\n",
    "            distancing_degree = counterfactual_retrospective_experiment_kwargs['distancing_degree']\n",
    "            poi_cbg_visits_list = apply_distancing_degree(poi_cbg_visits_list, distancing_degree)\n",
    "            print('Modified poi_cbg_visits_list for retrospective experiment: distancing_degree = %s.' % distancing_degree)\n",
    "        else:\n",
    "            shift_in_days = counterfactual_retrospective_experiment_kwargs['shift_in_days']\n",
    "            poi_cbg_visits_list = apply_shift_in_days(poi_cbg_visits_list, shift_in_days)\n",
    "            print('Modified poi_cbg_visits_list for retrospective experiment: shifted by %d days.' % shift_in_days)\n",
    "    \n",
    "    print('Total time to prep data: %.3fs' % (time.time() - t0))\n",
    "\n",
    "    # feed everything into model.\n",
    "    m = Model(**model_init_kwargs)\n",
    "    # just init, no calculation\n",
    "    m.init_exogenous_variables(poi_cbg_proportions=poi_cbg_proportions_mat,\n",
    "                               poi_time_counts=poi_time_counts,\n",
    "                               poi_areas=poi_areas,\n",
    "                               poi_dwell_time_correction_factors=poi_dwell_time_correction_factors,\n",
    "                               cbg_sizes=cbg_sizes,  # block group population\n",
    "                               all_unique_cbgs=all_unique_cbgs,\n",
    "                               cbgs_to_idxs=cbgs_to_idxs,\n",
    "                               all_states=all_states,\n",
    "                               poi_cbg_visits_list=poi_cbg_visits_list,\n",
    "                               all_hours=all_hours,\n",
    "                               initial_conditions=initial_conditions,\n",
    "                               cbg_idx_groups_to_track=cbg_idx_groups_to_track,\n",
    "                               cbg_day_prop_out=cbg_day_prop_out,\n",
    "                               inter_cbg_travel=inter_cbg_travel,\n",
    "                               daily_mask_use=mask_data,\n",
    "                               intervention_cost=intervention_cost,\n",
    "                               poi_subcategory_types=poi_subcategory_types,\n",
    "                               **exogenous_model_kwargs,\n",
    "                               is_save_infection_rate=True,\n",
    "                               )\n",
    "    m.init_endogenous_variables()\n",
    "    if attach_data_to_model:\n",
    "        m.d = d\n",
    "    if return_model_without_fitting:\n",
    "        return m\n",
    "    m.simulate_disease_spread(**simulation_kwargs)\n",
    "    return m\n",
    "\n",
    "def correct_visit_vector(v, median_dwell_in_minutes):\n",
    "    \"\"\"\n",
    "    Given an original hourly visit vector v and a dwell time in minutes,\n",
    "    return a new hourly visit vector which accounts for spillover.\n",
    "    \"\"\"\n",
    "    v = np.array(v)\n",
    "    d = median_dwell_in_minutes/60.\n",
    "    new_v = v.copy().astype(float)\n",
    "    max_shift = math.floor(d + 1) # maximum hours we can spill over to.\n",
    "    for i in range(1, max_shift + 1):\n",
    "        if i < max_shift:\n",
    "            new_v[i:] += v[:-i] # this hour is fully occupied\n",
    "        else:\n",
    "            new_v[i:] += (d - math.floor(d)) * v[:-i] # this hour only gets part of the visits.\n",
    "    return new_v\n",
    "\n",
    "def clip_poi_attributes_in_msa_df(d, min_datetime, max_datetime,\n",
    "                                  clip_areas, clip_dwell_times, clip_visits,\n",
    "                                  area_below=AREA_CLIPPING_BELOW,\n",
    "                                  area_above=AREA_CLIPPING_ABOVE,\n",
    "                                  dwell_time_above=DWELL_TIME_CLIPPING_ABOVE,\n",
    "                                  visits_above=HOURLY_VISITS_CLIPPING_ABOVE,\n",
    "                                  subcat_cutoff=SUBCATEGORY_CLIPPING_THRESH,\n",
    "                                  topcat_cutoff=TOPCATEGORY_CLIPPING_THRESH):\n",
    "    '''\n",
    "    Deal with POI outliers by clipping their hourly visits, dwell times, and physical areas\n",
    "    to some percentile of the corresponding distribution for each POI category.\n",
    "    '''\n",
    "    attr_cols = []\n",
    "    if clip_areas:\n",
    "        attr_cols.append('safegraph_computed_area_in_square_feet')\n",
    "    if clip_dwell_times:\n",
    "        attr_cols.append('avg_median_dwell')\n",
    "    if clip_visits:\n",
    "        all_hours = helper.list_hours_in_range(min_datetime, max_datetime)\n",
    "        hour_cols = ['hourly_visits_%s' % get_datetime_hour_as_string(dt) for dt in all_hours]\n",
    "        attr_cols.extend(hour_cols)\n",
    "    assert all([col in d.columns for col in attr_cols])\n",
    "    print('Clipping areas: %s (below=%d, above=%d), clipping dwell times: %s (above=%d), clipping visits: %s (above=%d)' %\n",
    "          (clip_areas, area_below, area_above, clip_dwell_times, dwell_time_above, clip_visits, visits_above))\n",
    "\n",
    "    indices_covered = []\n",
    "    subcats = []\n",
    "    subcategory2idx = d.groupby('sub_category').indices\n",
    "    for cat, idx in subcategory2idx.items():\n",
    "        if len(idx) >= subcat_cutoff:\n",
    "            subcats.append(cat)\n",
    "            indices_covered.extend(idx)\n",
    "\n",
    "    # group by top_category for POIs whose sub_category's are too small\n",
    "    topcats = []\n",
    "    topcategory2idx = d.groupby('top_category').indices\n",
    "    for cat, idx in topcategory2idx.items():\n",
    "        if len(idx) >= topcat_cutoff:\n",
    "            new_idx = np.array(list(set(idx) - set(indices_covered)))  # POIs that are not covered by sub_category clipping\n",
    "            if len(new_idx) > 0:\n",
    "                topcats.append(cat)\n",
    "                topcategory2idx[cat] = (idx, new_idx)\n",
    "                indices_covered.extend(new_idx)\n",
    "    print('Found %d sub-categories with >= %d POIs and %d top categories with >= %d POIs -> covers %d POIs' %\n",
    "          (len(subcats), subcat_cutoff, len(topcats), topcat_cutoff, len(indices_covered)))\n",
    "    lost_pois = len(d) - len(indices_covered)\n",
    "    print('Could not cover %d/%d POIs (%.1f%% POIs) -> dropping these POIs' %\n",
    "          (lost_pois, len(d), 100. * lost_pois/len(d)))\n",
    "    if lost_pois / len(d) > .05:\n",
    "        raise Exception('Dropping too many POIs during clipping phase')\n",
    "\n",
    "    all_cats = topcats + subcats  # process top categories first so sub categories will compute percentiles on raw data\n",
    "    new_data = np.array(d[attr_cols].copy().values)  # n_pois x n_cols_to_clip\n",
    "    thresholds = np.zeros((len(all_cats), len(attr_cols)+1))  # clipping thresholds for category x attribute\n",
    "    medians = np.zeros((len(all_cats), len(attr_cols)))  # medians for category x attribute\n",
    "    indices_processed = []\n",
    "    for i, cat in enumerate(all_cats):\n",
    "        if i < len(topcats):\n",
    "            cat_idx, new_idx = topcategory2idx[cat]\n",
    "        else:\n",
    "            cat_idx = subcategory2idx[cat]\n",
    "            new_idx = cat_idx\n",
    "        indices_processed.extend(new_idx)\n",
    "        first_col_idx = 0  # index of first column for this attribute\n",
    "\n",
    "        if clip_areas:\n",
    "            cat_areas = new_data[cat_idx, first_col_idx]  # compute percentiles on entire category\n",
    "            min_area = np.nanpercentile(cat_areas, area_below)\n",
    "            max_area = np.nanpercentile(cat_areas, area_above)\n",
    "            median_area = np.nanmedian(cat_areas)\n",
    "            thresholds[i][first_col_idx] = min_area\n",
    "            thresholds[i][first_col_idx+1] = max_area\n",
    "            medians[i][first_col_idx] = median_area\n",
    "            new_data[new_idx, first_col_idx] = np.clip(new_data[new_idx, first_col_idx], min_area, max_area)\n",
    "            first_col_idx += 1\n",
    "\n",
    "        if clip_dwell_times:\n",
    "            cat_dwell_times = new_data[cat_idx, first_col_idx]\n",
    "            max_dwell_time = np.nanpercentile(cat_dwell_times, dwell_time_above)\n",
    "            median_dwell_time = np.nanmedian(cat_dwell_times)\n",
    "            thresholds[i][first_col_idx+1] = max_dwell_time\n",
    "            medians[i][first_col_idx] = median_dwell_time\n",
    "            new_data[new_idx, first_col_idx] = np.clip(new_data[new_idx, first_col_idx], None, max_dwell_time)\n",
    "            first_col_idx += 1\n",
    "\n",
    "        if clip_visits:\n",
    "            col_idx = np.arange(first_col_idx, first_col_idx+len(hour_cols))\n",
    "            assert col_idx[-1] == (len(attr_cols)-1)\n",
    "            orig_visits = new_data[cat_idx][:, col_idx].copy()  # need to copy bc will modify\n",
    "            orig_visits[orig_visits == 0] = np.nan  # want percentile over positive visits\n",
    "            # can't take percentile of col if it is all 0's or all nan's\n",
    "            cols_to_process = col_idx[np.sum(~np.isnan(orig_visits), axis=0) > 0]\n",
    "            max_visits_per_hour = np.nanpercentile(orig_visits[:, cols_to_process-first_col_idx], visits_above, axis=0)\n",
    "            assert np.sum(np.isnan(max_visits_per_hour)) == 0\n",
    "            thresholds[i][cols_to_process + 1] = max_visits_per_hour\n",
    "            medians[i][cols_to_process] = np.nanmedian(orig_visits[:, cols_to_process-first_col_idx], axis=0)\n",
    "\n",
    "            orig_visit_sum = np.nansum(new_data[new_idx][:, col_idx])\n",
    "            orig_attributes = new_data[new_idx]  # return to un-modified version\n",
    "            orig_attributes[:, cols_to_process] = np.clip(orig_attributes[:, cols_to_process], None, max_visits_per_hour)\n",
    "            new_data[new_idx] = orig_attributes\n",
    "            new_visit_sum = np.nansum(new_data[new_idx][:, col_idx])\n",
    "            print('%s -> has %d POIs, processed %d POIs, %d visits before clipping, %d visits after clipping' %\n",
    "              (cat, len(cat_idx), len(new_idx), orig_visit_sum, new_visit_sum))\n",
    "        else:\n",
    "            print('%s -> has %d POIs, processed %d POIs' % (cat, len(cat_idx), len(new_idx)))\n",
    "\n",
    "    assert len(indices_processed) == len(set(indices_processed))  # double check that we only processed each POI once\n",
    "    assert set(indices_processed) == set(indices_covered)  # double check that we processed the POIs we expected to process\n",
    "    new_d = d.iloc[indices_covered].copy()\n",
    "    new_d[attr_cols] = new_data[indices_covered]\n",
    "    return new_d, all_cats, attr_cols, thresholds, medians\n",
    "    \n",
    "def apply_interventions_to_poi_cbg_matrices(poi_cbg_visits_list, poi_categories, poi_areas,\n",
    "                                            new_all_hours, intervention_hour_idx,\n",
    "                                            alpha, extra_weeks_to_simulate,\n",
    "                                            top_category=None, sub_category=None,\n",
    "                                            interpolate=True):\n",
    "    '''\n",
    "    Simulates hypothetical mobility patterns by editing visit matrices.\n",
    "    '''\n",
    "    # find POIs of interest\n",
    "    if top_category is not None:\n",
    "        if type(top_category) == list:\n",
    "            top_category_poi_idx = np.zeros(len(poi_categories)).astype(bool)\n",
    "            for cat in top_category:\n",
    "                top_category_poi_idx = top_category_poi_idx | (poi_categories['top_category'] == cat).values\n",
    "        else:\n",
    "            top_category_poi_idx = (poi_categories['top_category'] == top_category).values\n",
    "    else:\n",
    "        top_category = 'any'\n",
    "        top_category_poi_idx = np.ones(len(poi_categories)).astype(bool)\n",
    "    \n",
    "    if sub_category is not None:\n",
    "        if type(sub_category) == list:\n",
    "            sub_category_poi_idx = np.zeros(len(poi_categories)).astype(bool)\n",
    "            for cat in sub_category:\n",
    "                sub_category_poi_idx = sub_category_poi_idx | (poi_categories['sub_category'] == cat).values\n",
    "        else:\n",
    "            sub_category_poi_idx = (poi_categories['sub_category'] == sub_category).values\n",
    "    else:\n",
    "        sub_category = 'any'\n",
    "        sub_category_poi_idx = np.ones(len(poi_categories)).astype(bool)\n",
    "    intervened_poi_idx = top_category_poi_idx & sub_category_poi_idx  # poi indices to intervene on\n",
    "    assert intervened_poi_idx.sum() > 0\n",
    "    print(\"Intervening on POIs with top_category=%s, sub_category=%s (n=%i)\" % (top_category, sub_category, intervened_poi_idx.sum()))\n",
    "\n",
    "    # extend matrix list to extra weeks, loop final week for now\n",
    "    num_pois, num_cbgs = poi_cbg_visits_list[0].shape\n",
    "    new_matrix_list = [m.copy() for m in poi_cbg_visits_list]\n",
    "    for i in range(extra_weeks_to_simulate * 168):\n",
    "        matrix_idx = -168 + (i % 168)  # get corresponding matrix from final week\n",
    "        new_matrix_list.append(poi_cbg_visits_list[matrix_idx].copy())\n",
    "        assert new_matrix_list[-1].shape == (num_pois, num_cbgs), len(new_matrix_list)-1\n",
    "    assert len(new_matrix_list) == len(new_all_hours)\n",
    "\n",
    "    if top_category == 'any' and sub_category == 'any':  # apply intervention to all POIs\n",
    "        full_activity_sum = 0\n",
    "        simulated_activity_sum = 0\n",
    "        for i in range(intervention_hour_idx, len(new_all_hours)):\n",
    "            no_reopening = new_matrix_list[i]\n",
    "            full_reopening = new_matrix_list[i % 168]\n",
    "            full_activity_sum += full_reopening.sum()\n",
    "            if alpha == 1:\n",
    "                new_matrix_list[i] = full_reopening.copy()\n",
    "                simulated_activity_sum = full_activity_sum\n",
    "            else:\n",
    "                if interpolate:\n",
    "                    new_matrix_list[i] = full_reopening.multiply(alpha) + no_reopening.multiply(1-alpha)\n",
    "                else:\n",
    "                    new_matrix_list[i] = full_reopening.multiply(alpha)\n",
    "                simulated_activity_sum += new_matrix_list[i].sum()\n",
    "        diff = full_activity_sum - simulated_activity_sum\n",
    "        overall_cost = (100. * diff / full_activity_sum)\n",
    "        print('Overall Cost (%% of full activity): %2.3f%%' % overall_cost)\n",
    "        return new_matrix_list, {'overall_cost':overall_cost, 'cost_within_intervened_pois':overall_cost}\n",
    "\n",
    "    # full activity based on first week of visits\n",
    "    range_end = max(intervention_hour_idx + 168, len(poi_cbg_visits_list))\n",
    "    full_activity = [poi_cbg_visits_list[i % 168] for i in range(intervention_hour_idx, range_end)]  # get corresponding matrix in first week\n",
    "    full_activity = hstack(full_activity, format='csr')\n",
    "    orig_activity = hstack(new_matrix_list[intervention_hour_idx:range_end], format='csr')\n",
    "    assert full_activity.shape == orig_activity.shape\n",
    "    print('Computed hstacks of sparse matrices [shape=(%d, %d)]' % full_activity.shape)\n",
    "\n",
    "    # take mixture of full activity and original activity for POIs of interest\n",
    "    indicator_vec = np.zeros(num_pois)\n",
    "    indicator_vec[intervened_poi_idx] = 1.0\n",
    "    alpha_vec = alpha * indicator_vec\n",
    "    scaled_full_activity = full_activity.transpose().multiply(alpha_vec).transpose()\n",
    "    if interpolate:\n",
    "        non_alpha_vec = 1.0 - alpha_vec   # intervened POIs will have alpha*full + (1-alpha)*closed\n",
    "    else:\n",
    "        non_alpha_vec = 1.0 - indicator_vec  # intervened POIs will have alpha*full\n",
    "    scaled_orig_activity = orig_activity.transpose().multiply(non_alpha_vec).transpose()\n",
    "    activity_mixture = scaled_full_activity + scaled_orig_activity\n",
    "    print('Computed mixture of full and original activity')\n",
    "\n",
    "    # compute costs\n",
    "    full_overall_sum = full_activity.sum()\n",
    "    mixture_overall_sum = activity_mixture.sum()\n",
    "    overall_diff = full_overall_sum - mixture_overall_sum\n",
    "    overall_cost = (100. * overall_diff / full_overall_sum)\n",
    "    print('Overall Cost (%% of full activity): %2.3f%%' % overall_cost)\n",
    "    full_intervened_sum = full_activity.transpose().multiply(indicator_vec).sum()\n",
    "    mixture_intervened_sum = activity_mixture.transpose().multiply(indicator_vec).sum()\n",
    "    intervened_diff = full_intervened_sum - mixture_intervened_sum\n",
    "    cost_within_intervened_pois = (100. * intervened_diff / full_intervened_sum)\n",
    "    print('Cost within intervened POIs: %2.3f%%' % cost_within_intervened_pois)\n",
    "\n",
    "    print('Redistributing stacked matrix into hourly pieces...')\n",
    "    ts = time.time()\n",
    "    looping = False\n",
    "    for i in range(intervention_hour_idx, len(new_all_hours)):\n",
    "        matrix_idx = i - intervention_hour_idx\n",
    "        if i >= len(poi_cbg_visits_list) and matrix_idx >= 168:\n",
    "            # once we are operating past the length of real data, the \"original\" matrix\n",
    "            # is just the matrix from the last week of the real data for the corresponding\n",
    "            # day, and if matrix_idx > 168, then the mixture for that corresponding day\n",
    "            # has been computed already\n",
    "            new_matrix_list[i] = new_matrix_list[i - 168].copy()\n",
    "            if looping is False:\n",
    "                print('Entering looping phase at matrix %d!' % matrix_idx)\n",
    "                looping = True\n",
    "        else:\n",
    "            matrix_start = matrix_idx * num_cbgs\n",
    "            matrix_end = matrix_start + num_cbgs\n",
    "            new_matrix_list[i] = activity_mixture[:, matrix_start:matrix_end]\n",
    "        assert new_matrix_list[i].shape == (num_pois, num_cbgs), 'intervention idx = %d, overall idx = %d [found size = (%d, %d)]' % (matrix_idx, i, new_matrix_list[i].shape[0], new_matrix_list[i].shape[1])\n",
    "        if matrix_idx % 24 == 0:\n",
    "            te = time.time()\n",
    "            print('Finished matrix %d: time so far per hourly matrix = %.2fs' % (matrix_idx, (te-ts)/(matrix_idx+1)))\n",
    "    return new_matrix_list, {'overall_cost':overall_cost, 'cost_within_intervened_pois':cost_within_intervened_pois}\n",
    "\n",
    "def get_corresponding_2019_datetime(date):\n",
    "    if date.year == 2019:\n",
    "        return date \n",
    "    exact_date_2019 = datetime.datetime(2019, date.month, date.day, date.hour)\n",
    "    if date.strftime('%m-%d') in FIXED_HOLIDAY_DATES:  # is a holiday, need to return exact same date\n",
    "        return exact_date_2019\n",
    "    offset_date_2019 = datetime.datetime(2019, date.month, date.day, date.hour)\n",
    "    diff = date.weekday() - offset_date_2019.weekday()\n",
    "    if diff > 0:\n",
    "        if diff <= 3:  # go forward 1-3 days\n",
    "            offset_date_2019 = offset_date_2019 + datetime.timedelta(days=diff)\n",
    "        else:  # go back 1-3 days\n",
    "            assert diff <= 6\n",
    "            back_steps = 7-diff\n",
    "            offset_date_2019 = offset_date_2019 + datetime.timedelta(days=-back_steps)\n",
    "    elif diff < 0:\n",
    "        if diff >= -3:  # go back 1-3 days\n",
    "            offset_date_2019 = offset_date_2019 + datetime.timedelta(days=diff)\n",
    "        else:  # go forward 1-3 days\n",
    "            assert diff >= -6\n",
    "            forward_steps = 7 + diff\n",
    "            offset_date_2019 = offset_date_2019 + datetime.timedelta(days=forward_steps)\n",
    "    assert offset_date_2019.weekday() == date.weekday()\n",
    "    if offset_date_2019.strftime('%m-%d') in FIXED_HOLIDAY_DATES:  # can't map to holiday\n",
    "        return exact_date_2019\n",
    "    return offset_date_2019\n",
    "    \n",
    "def get_corresponding_2019_visits(all_poi_visits, all_hours, start_hour, end_hour):\n",
    "    print('Finding corresponding 2019 hours for %s to %s' % (start_hour.strftime('%Y-%m-%d'), \n",
    "                                                             end_hour.strftime('%Y-%m-%d')))\n",
    "    hours = list_hours_in_range(start_hour, end_hour)\n",
    "    corresponding_visits = np.zeros((all_poi_visits.shape[0], len(hours)))\n",
    "    for i, hr in enumerate(hours):\n",
    "        date_2019 = get_corresponding_2019_datetime(hr)\n",
    "        assert date_2019.hour == hr.hour\n",
    "        index_2019 = all_hours.index(date_2019)\n",
    "        corresponding_visits[:, i] = all_poi_visits[:, index_2019]\n",
    "    return corresponding_visits\n",
    "\n",
    "def apply_different_percentages_of_2019_levels(msa_name, category2alpha, poi_cbg_visits_list, poi_categories, \n",
    "                                               orig_hours, intervention_datetime, extra_weeks_to_simulate,\n",
    "                                               all_poi_visits=None, all_poi_visits_hours=None, \n",
    "                                               agg_poi_cbg_visits=None):\n",
    "    '''\n",
    "    V2 of constructing hypothetical mobility patterns. category2alpha maps category to level of opening, as a fraction\n",
    "    of 2019 mobility levels. For POIs whose categories do not appear in category2alpha, they continue their current\n",
    "    levels of mobility.\n",
    "    '''\n",
    "    num_pois, num_cbgs = poi_cbg_visits_list[0].shape\n",
    "    model_hours =list_hours_in_range(orig_hours[0], orig_hours[-1] + datetime.timedelta(hours=168*extra_weeks_to_simulate))\n",
    "    assert intervention_datetime in model_hours\n",
    "    model_intervention_idx = model_hours.index(intervention_datetime)\n",
    "    assert model_intervention_idx <= len(poi_cbg_visits_list)\n",
    "    num_hours_post_intervention = len(model_hours[model_intervention_idx:])\n",
    "    print('Found %d hours post-intervention' % num_hours_post_intervention)\n",
    "    \n",
    "    # get average proportion of home CBGs per POI\n",
    "    if agg_poi_cbg_visits is None:\n",
    "        agg_poi_cbg_visits = poi_cbg_visits_list[0]\n",
    "        for t in range(1, model_intervention_idx):\n",
    "            agg_poi_cbg_visits = agg_poi_cbg_visits + poi_cbg_visits_list[t]\n",
    "    assert agg_poi_cbg_visits.shape == (num_pois, num_cbgs)\n",
    "    agg_poi_cbg_visits = csr_matrix(agg_poi_cbg_visits)\n",
    "    row_sums = agg_poi_cbg_visits @ np.ones(num_cbgs)\n",
    "    zero_visits = row_sums == 0\n",
    "    print('Found %d POIs with 0 visits in aggregate POI CBG visits matrix' % np.sum(zero_visits))\n",
    "    row_sums[zero_visits] = 1e-10  # the proportions will remain 0 bc numerator\n",
    "    agg_poi_cbg_props = agg_poi_cbg_visits.transpose().multiply(1/row_sums).transpose().tocsr()\n",
    "    # make extended poi_cbg_visits_list with proportions matrix post-interventions\n",
    "    new_matrix_list = [m.copy() for m in poi_cbg_visits_list[:model_intervention_idx]]\n",
    "    post_intervention_list = [agg_poi_cbg_props.copy() for t in range(model_intervention_idx, len(model_hours))]\n",
    "    new_matrix_list += post_intervention_list\n",
    "    assert len(new_matrix_list) == len(model_hours)\n",
    "\n",
    "    # compute current vs 2019 scaling for each POI\n",
    "    if all_poi_visits is None or all_poi_visits_hours is None:\n",
    "        all_poi_visits, _, all_poi_visits_hours = helper.load_all_poi_visits_for_msa(msa_name)\n",
    "    assert len(all_poi_visits) == num_pois\n",
    "    # compare last 4 weeks before intervention to corresponding weeks in 2019\n",
    "    visits_intervention_idx = all_poi_visits_hours.index(intervention_datetime)\n",
    "    four_weeks_visits = all_poi_visits[:, visits_intervention_idx-(168*4):visits_intervention_idx]\n",
    "    start_hour = intervention_datetime + datetime.timedelta(hours=-(168*4))\n",
    "    end_hour = intervention_datetime + datetime.timedelta(hours=-1)\n",
    "    four_weeks_visits_2019 = get_corresponding_2019_visits(all_poi_visits, all_poi_visits_hours, start_hour, end_hour)\n",
    "    assert four_weeks_visits_2019.shape == four_weeks_visits.shape\n",
    "    scaling_factors = (np.sum(four_weeks_visits, axis=1)+1) / (np.sum(four_weeks_visits_2019, axis=1)+1)\n",
    "    \n",
    "    # check which POIs should use computed POI scaling\n",
    "    valid_factors = (scaling_factors >= 0.1) & (scaling_factors <= 2)  # scaling assumptions break at extremes\n",
    "    num_nonzero_hours_2019 = np.sum(four_weeks_visits_2019 > 0, axis=1)\n",
    "    num_nonzero_hours = np.sum(four_weeks_visits > 0, axis=1)\n",
    "    ratio_of_nonzero_hours = (num_nonzero_hours + 1) / (num_nonzero_hours_2019 + 1)\n",
    "    valid_ratios = (ratio_of_nonzero_hours >= 0.5) & (ratio_of_nonzero_hours <= 1.5)\n",
    "    use_scaled_2019 = valid_factors & valid_ratios\n",
    "    print('%.2f%% of POIs have valid scaling factors, %.2f%% have valid non-zero ratios, %.2f%% have both' % \n",
    "          (100. * np.sum(valid_factors) / num_pois, 100. * np.sum(valid_ratios) / num_pois,\n",
    "           100. * np.sum(use_scaled_2019) / num_pois))\n",
    "    print('Scaling factors over valid POIs: min = %.3f, 25th = %.3f, median = %.3f, 75th = %.3f, max = %.3f' % \n",
    "           (np.min(scaling_factors[use_scaled_2019]), np.percentile(scaling_factors[use_scaled_2019], 25), \n",
    "            np.median(scaling_factors[use_scaled_2019]), np.percentile(scaling_factors[use_scaled_2019], 75),\n",
    "            np.max(scaling_factors[use_scaled_2019])))\n",
    "    for cat, alpha in category2alpha.items():\n",
    "        in_cat = poi_categories == cat\n",
    "        assert np.sum(in_cat) > 0  # sanity check to make sure category names aren't misspelled\n",
    "        scaling_factors[in_cat] = alpha\n",
    "        use_scaled_2019[in_cat] = 1  # must use scaled 2019 when alpha is fixed\n",
    "\n",
    "    # compute expected row sums for POIs -- first use average from last 4 weeks, looped\n",
    "    avg_week_to_loop = np.zeros((num_pois, 168))\n",
    "    for i in range(0, four_weeks_visits.shape[1], 168):\n",
    "        avg_week_to_loop += four_weeks_visits[:, i:i+168]\n",
    "    avg_week_to_loop = avg_week_to_loop / 4\n",
    "    num_weeks_post_intervention = math.ceil(num_hours_post_intervention/168)\n",
    "    post_intervention_visits = [avg_week_to_loop.copy() for w in range(num_weeks_post_intervention)]\n",
    "    post_intervention_visits = np.concatenate(post_intervention_visits, axis=1)[:, :num_hours_post_intervention]\n",
    "    # try using visits from 2019 period corresponding to post-intervention period\n",
    "    start_hour = intervention_datetime\n",
    "    end_hour = intervention_datetime + datetime.timedelta(hours=num_hours_post_intervention-1)\n",
    "    post_intervention_visits_2019 = get_corresponding_2019_visits(all_poi_visits, all_poi_visits_hours, start_hour, end_hour)\n",
    "    assert post_intervention_visits_2019.shape == post_intervention_visits.shape\n",
    "    scaled_post_intervention_visits_2019 = (post_intervention_visits_2019.T * scaling_factors).T\n",
    "    post_intervention_visits[use_scaled_2019] = scaled_post_intervention_visits_2019[use_scaled_2019]\n",
    "    # cap predicted POI visits to historical max occupancy of POI\n",
    "    historical_max_occ = np.max(all_poi_visits, axis=1)\n",
    "    post_intervention_visits = np.clip(post_intervention_visits.T, None, historical_max_occ).T\n",
    "    total_post_intervention_visits = np.sum(post_intervention_visits)\n",
    "\n",
    "    # scale every post-intervention hour to match expected POI sums\n",
    "    ts = time.time()\n",
    "    prop_row_sums = agg_poi_cbg_props @ np.ones(num_cbgs)\n",
    "    for t in range(model_intervention_idx, len(model_hours)):\n",
    "        expected_row_sums = post_intervention_visits[:, t-model_intervention_idx]\n",
    "        mat_scaling_factors = expected_row_sums / np.clip(prop_row_sums, 1e-10, None)\n",
    "        new_matrix_list[t] = new_matrix_list[t].transpose().multiply(mat_scaling_factors).transpose().tocsr()\n",
    "        computed_row_sums = new_matrix_list[t] @ np.ones(num_cbgs)\n",
    "        err = np.absolute(computed_row_sums - expected_row_sums)\n",
    "        if np.max(err) > 1:\n",
    "            print('%d. error median = %.3f, 90th = %.3f, max = %.3f' % \n",
    "                  (t, np.median(err), np.percentile(err, 90), np.max(err)))\n",
    "    return model_hours, new_matrix_list, total_post_intervention_visits\n",
    "\n",
    "def apply_distancing_degree(poi_cbg_visits_list, distancing_degree):\n",
    "    \"\"\"\n",
    "    After the first week of March, assume that activity is an interpolation between true activity and first-week-of-March activity\n",
    "    \"\"\"\n",
    "    new_visits_list = []\n",
    "    for i, m in enumerate(poi_cbg_visits_list):\n",
    "        if i < 168:  # first week\n",
    "            new_visits_list.append(m.copy())\n",
    "        else:\n",
    "            first_week_m = poi_cbg_visits_list[i % 168]\n",
    "            mixture = first_week_m.multiply(1-distancing_degree) + m.multiply(distancing_degree)\n",
    "            new_visits_list.append(mixture.copy())\n",
    "    return new_visits_list\n",
    "\n",
    "def apply_shift_in_days(poi_cbg_visits_list, shift_in_days):\n",
    "    \"\"\"\n",
    "    Shift entire visits timeline shift_in_days days forward or backward,\n",
    "    filling in the beginning or end as necessary with data from the first or last week.\n",
    "    \"\"\"\n",
    "    new_visits_list = []\n",
    "    shift_in_hours = shift_in_days * 24\n",
    "    if shift_in_hours <= 0:  # shift earlier\n",
    "        new_visits_list = [m.copy() for m in poi_cbg_visits_list[abs(shift_in_hours):]]\n",
    "        current_length = len(new_visits_list)\n",
    "        assert current_length >= 168\n",
    "        last_week = new_visits_list[-168:]\n",
    "        for i in range(current_length, len(poi_cbg_visits_list)):\n",
    "            last_week_counterpart = last_week[i % 168].copy()\n",
    "            new_visits_list.append(last_week_counterpart)\n",
    "    else:  # shift later\n",
    "        for i in range(len(poi_cbg_visits_list)):\n",
    "            if i-shift_in_hours < 0:\n",
    "                # fill in with the last part of the first week.\n",
    "                # so eg if shift_in_hours is 72, we take the last 72 hours of the first week.\n",
    "                first_week_idx = (168 - shift_in_hours + i) % 168\n",
    "\n",
    "                # alternate, more complex computation as sanity check.\n",
    "                distance_from_start = (shift_in_hours - i) % 168\n",
    "                first_week_idx_2 = (168 - distance_from_start) % 168\n",
    "\n",
    "                assert first_week_idx_2 == first_week_idx\n",
    "                new_visits_list.append(poi_cbg_visits_list[first_week_idx].copy())\n",
    "            else:\n",
    "                new_visits_list.append(poi_cbg_visits_list[i-shift_in_hours].copy())\n",
    "    assert len(new_visits_list) == len(poi_cbg_visits_list)\n",
    "    return new_visits_list\n",
    "\n",
    "def get_ipf_filename(msa_name, min_datetime, max_datetime, clip_visits, correct_visits=True):\n",
    "    \"\"\"\n",
    "    Get the filename matching these parameters of IPF.\n",
    "    \"\"\"\n",
    "    fn = '%s_%s_to_%s_clip_visits_%s' % (msa_name,\n",
    "                                min_datetime.strftime('%Y-%m-%d'),\n",
    "                                max_datetime.strftime('%Y-%m-%d'),\n",
    "                                clip_visits)\n",
    "    if correct_visits:\n",
    "        fn += '_correct_visits_True'\n",
    "    filename = os.path.join(PATH_TO_IPF_OUTPUT, '%s.pkl' % fn)\n",
    "    return filename\n",
    "\n",
    "def sanity_check_error_metrics(fast_to_load_results):\n",
    "    \"\"\"\n",
    "    Make sure train and test loss sum to total loss in the way we would expect.\n",
    "    \"\"\"\n",
    "    n_train_days = len(helper.list_datetimes_in_range(\n",
    "        fast_to_load_results['train_loss_dict']['eval_start_time_cases'],\n",
    "        fast_to_load_results['train_loss_dict']['eval_end_time_cases']))\n",
    "\n",
    "    n_test_days = len(helper.list_datetimes_in_range(\n",
    "        fast_to_load_results['test_loss_dict']['eval_start_time_cases'],\n",
    "        fast_to_load_results['test_loss_dict']['eval_end_time_cases']))\n",
    "\n",
    "    n_total_days = len(helper.list_datetimes_in_range(\n",
    "        fast_to_load_results['loss_dict']['eval_start_time_cases'],\n",
    "        fast_to_load_results['loss_dict']['eval_end_time_cases']))\n",
    "\n",
    "    assert n_train_days + n_test_days == n_total_days\n",
    "    assert fast_to_load_results['loss_dict']['eval_end_time_cases'] == fast_to_load_results['test_loss_dict']['eval_end_time_cases']\n",
    "    assert fast_to_load_results['loss_dict']['eval_start_time_cases'] == fast_to_load_results['train_loss_dict']['eval_start_time_cases']\n",
    "    for key in ['daily_cases_MSE', 'cumulative_cases_MSE']:\n",
    "        if 'RMSE' in key:\n",
    "            train_plus_test_loss = (n_train_days * fast_to_load_results['train_loss_dict'][key] ** 2 +\n",
    "                 n_test_days * fast_to_load_results['test_loss_dict'][key] ** 2)\n",
    "\n",
    "            overall_loss = n_total_days * fast_to_load_results['loss_dict'][key] ** 2\n",
    "        else:\n",
    "            train_plus_test_loss = (n_train_days * fast_to_load_results['train_loss_dict'][key] +\n",
    "                 n_test_days * fast_to_load_results['test_loss_dict'][key])\n",
    "\n",
    "            overall_loss = n_total_days * fast_to_load_results['loss_dict'][key]\n",
    "\n",
    "        assert np.allclose(train_plus_test_loss, overall_loss, rtol=1e-6)\n",
    "    print(\"Sanity check error metrics passed\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fit_and_save_one_model(timestring,\n",
    "                           model_kwargs,\n",
    "                           data_kwargs,\n",
    "                           d=None,\n",
    "                           experiment_to_run=None,\n",
    "                           train_test_partition=None,\n",
    "                           filter_for_cbgs_in_msa=False,\n",
    "                           version='v2',\n",
    "                           existing_config=None):\n",
    "    '''\n",
    "    Fits one model, saves its results and evaluations of the results.\n",
    "    timestring: str; to use in filenames to identify the model and its config;\n",
    "        if None, then the model is not saved\n",
    "    model_kwargs: dict; arguments to use for fit_disease_model_on_real_data\n",
    "        required keys: min_datetime, max_datetime, exogenous_model_kwargs, poi_attributes_to_clip\n",
    "    data_kwargs: dict; arguments for the data; required to have key 'MSA_name'\n",
    "    d: pandas DataFrame; the dataframe for the MSA pois; if None, then the dataframe is loaded\n",
    "        within the function\n",
    "    experiment_to_run: str; name of experiment to run\n",
    "    train_test_partition: DateTime object; the first hour of test; if included, then losses are saved\n",
    "        separately for train and test dates\n",
    "    filter_for_cbgs_in_msa: bool; whether to only model CBGs in the MSA\n",
    "    version: str; either v1 (data + infrastructure from our Nature paper) or v2 (updated data + infrastructure)\n",
    "    '''\n",
    "\n",
    "\n",
    "    assert all([k in model_kwargs for k in ['min_datetime', 'max_datetime', 'exogenous_model_kwargs',\n",
    "                                            'poi_attributes_to_clip']])\n",
    "    assert 'MSA_name' in data_kwargs\n",
    "    assert version in ['v1', 'v2']\n",
    "    t0 = time.time()\n",
    "    return_without_saving = False\n",
    "    if timestring is None:\n",
    "        print(\"Fitting single model. Timestring is none so not saving model and just returning fitted model.\")\n",
    "        return_without_saving = True\n",
    "    else:\n",
    "        print(\"Fitting single model. Results will be saved using timestring %s\" % timestring)\n",
    "    if d is None:  # load data\n",
    "        if version == 'v1':\n",
    "            d = helper.load_dataframe_for_individual_msa(data_kwargs['MSA_name'], version=version)\n",
    "        else:\n",
    "            d = helper.prep_msa_df_for_model_experiments(data_kwargs['MSA_name'], time_period_strings=[])\n",
    "            # load POI attributes.\n",
    "\n",
    "    nyt_outcomes, nyt_counties, nyt_cbgs, msa_counties, msa_cbgs = get_variables_for_evaluating_msa_model(data_kwargs['MSA_name'])\n",
    "    msa_counties = [str(s).zfill(5) for s in msa_counties]  # county fips\n",
    "    nyt_cbgs = [str(s).zfill(12) for s in nyt_cbgs]  # block groups\n",
    "    if 'counties_to_track' not in model_kwargs:\n",
    "        model_kwargs['counties_to_track'] = msa_counties\n",
    "    cbg_groups_to_track = {}\n",
    "    cbg_groups_to_track['nyt'] = nyt_cbgs  # block groups\n",
    "    if filter_for_cbgs_in_msa:\n",
    "        print(\"Filtering for %i CBGs within MSA %s\" % (len(msa_cbgs), data_kwargs['MSA_name']))\n",
    "        cbgs_to_filter_for = set(msa_cbgs) # filter for CBGs within MSA\n",
    "    else:\n",
    "        cbgs_to_filter_for = None\n",
    "\n",
    "    correct_visits = model_kwargs['correct_visits'] if 'correct_visits' in model_kwargs else True  # default to True\n",
    "    if experiment_to_run == 'just_save_ipf_output':\n",
    "        # If we're saving IPF output, don't try to reload file.\n",
    "        print('Running experiment to generate IPF, will not load IPF from file')\n",
    "    elif 'poi_cbg_visits_list' in model_kwargs and model_kwargs['poi_cbg_visits_list'] is not None:\n",
    "        print('Passing in poi_cbg_visits_list, will not load IPF from file')\n",
    "    elif version == 'v1':  # try to load IPF output from v1\n",
    "        preload_poi_visits_list_filename = get_ipf_filename(msa_name=data_kwargs['MSA_name'],\n",
    "            min_datetime=model_kwargs['min_datetime'],\n",
    "            max_datetime=model_kwargs['max_datetime'],\n",
    "            clip_visits=model_kwargs['poi_attributes_to_clip']['clip_visits'],\n",
    "            correct_visits=correct_visits)\n",
    "        if os.path.exists(preload_poi_visits_list_filename):\n",
    "            print(\"Reloading POI visits from %s\" % preload_poi_visits_list_filename)\n",
    "            model_kwargs['preload_poi_visits_list_filename'] = preload_poi_visits_list_filename\n",
    "        else:\n",
    "            print('Warning: path %s does not exist, will not load IPF from file' % preload_poi_visits_list_filename)           \n",
    "    else:  # try to load IPF output from v2\n",
    "        min_datetime = model_kwargs['min_datetime']\n",
    "        max_datetime = model_kwargs['max_datetime']\n",
    "        try:\n",
    "            poi_cbg_visits_list, poi_ids, cbg_ids = helper.load_ipf_output_for_hours(\n",
    "                data_kwargs['MSA_name'], min_datetime, max_datetime, return_ids=True)\n",
    "            # poi_cbg_visits_list: hourly garph\n",
    "            model_kwargs['poi_cbg_visits_list'] = poi_cbg_visits_list\n",
    "            model_kwargs['poi_ids'] = poi_ids\n",
    "            model_kwargs['cbg_ids'] = cbg_ids\n",
    "            print('Successfully loaded IPF output and POI/CBG ids for %s to %s' % \n",
    "                 (min_datetime.strftime('%Y-%m-%d-%H'), max_datetime.strftime('%Y-%m-%d-%H')))\n",
    "        except:\n",
    "            print('Warning: could not load V2 IPF output for %s to %s' % \n",
    "                  (min_datetime.strftime('%Y-%m-%d-%H'), max_datetime.strftime('%Y-%m-%d-%H')))\n",
    "    if 'counterfactual_poi_opening_experiment_kwargs' in model_kwargs:\n",
    "        model_kwargs['counterfactual_poi_opening_experiment_kwargs']['version'] = version\n",
    "    \n",
    "    # fit model\n",
    "    fitted_model = fit_disease_model_on_real_data(\n",
    "        d,\n",
    "        cbg_groups_to_track=cbg_groups_to_track,\n",
    "        cbgs_to_filter_for=cbgs_to_filter_for,\n",
    "        msa_name=data_kwargs['MSA_name'],\n",
    "        **model_kwargs)\n",
    "\n",
    "    if experiment_to_run == 'just_save_ipf_output':\n",
    "        assert version == 'v1'  # this is not implemented for v2 yet\n",
    "        pickle_start_time = time.time()\n",
    "        ipf_filename = get_ipf_filename(msa_name=data_kwargs['MSA_name'],\n",
    "            min_datetime=model_kwargs['min_datetime'],\n",
    "            max_datetime=model_kwargs['max_datetime'],\n",
    "            clip_visits=model_kwargs['poi_attributes_to_clip']['clip_visits'],\n",
    "            correct_visits=correct_visits)\n",
    "        print('Saving IPF output in', ipf_filename)\n",
    "        ipf_file = open(ipf_filename, 'wb')\n",
    "        pickle.dump(fitted_model.poi_cbg_visit_history, ipf_file)\n",
    "        ipf_file.close()\n",
    "        print('Time to save pickle = %.2fs' % (time.time() - pickle_start_time))\n",
    "        print('Size of pickle: %.2f MB' % (os.path.getsize(ipf_filename) / (1024**2)))\n",
    "        return\n",
    "    \n",
    "    if return_without_saving:\n",
    "        return fitted_model\n",
    "    \n",
    "    # Save kwargs\n",
    "    keys_to_delete = ['poi_cbg_visits_list', 'poi_ids', 'cbg_ids']  # their values are too large, we don't want to save\n",
    "    keys_to_delete = []  # Huan\n",
    "\n",
    "    for k in keys_to_delete:\n",
    "        if k in model_kwargs:\n",
    "            model_kwargs[k] = None\n",
    "    data_and_model_kwargs = {'model_kwargs':model_kwargs, 'data_kwargs':data_kwargs, \n",
    "                             'experiment_to_run':experiment_to_run, 'version':version}\n",
    "    file = open(os.path.join(FITTED_MODEL_DIR, 'data_and_model_configs', 'config_%s.pkl' % timestring), 'wb')\n",
    "    pickle.dump(data_and_model_kwargs, file)\n",
    "    file.close()\n",
    "    \n",
    "    # for this experiment, just save forecast results, not entire model or losses\n",
    "    if experiment_to_run == 'test_category_combinations_for_dashboard':\n",
    "        assert 'counterfactual_poi_opening_experiment_kwargs' in model_kwargs\n",
    "        assert 'total_num_visits_post_intervention' in fitted_model.INTERVENTION_COST\n",
    "        extra_weeks_to_simulate = model_kwargs['counterfactual_poi_opening_experiment_kwargs']['extra_weeks_to_simulate']\n",
    "        intervention_datetime = model_kwargs['counterfactual_poi_opening_experiment_kwargs']['intervention_datetime']\n",
    "        all_hours = fitted_model.all_hours\n",
    "        intervention_hour_idx = all_hours.index(intervention_datetime)\n",
    "        ir = fitted_model.history['nyt']['infected'] + fitted_model.history['nyt']['removed']\n",
    "        intervention_ir = ir[:, intervention_hour_idx-1]\n",
    "        week2forecast = {}\n",
    "        for week in range(1, extra_weeks_to_simulate+1):\n",
    "            end_hour_idx = intervention_hour_idx - 1 + (168 * week)  # final hour of the week\n",
    "            delta_ir = ir[:, end_hour_idx] - intervention_ir\n",
    "            week2forecast[week] = delta_ir\n",
    "        to_pickle = (model_kwargs['counterfactual_poi_opening_experiment_kwargs']['category_to_alpha'],\n",
    "                     fitted_model.INTERVENTION_COST['total_num_visits_post_intervention'],\n",
    "                     week2forecast)\n",
    "        \n",
    "        results_path = os.path.join(helper.FITTED_MODEL_DIR, 'model_results', 'dashboard_results_%s.pkl' % timestring)\n",
    "        print('Saving results at %s...' % results_path)\n",
    "        file = open(results_path, 'wb')\n",
    "        pickle.dump(to_pickle, file)\n",
    "        file.close()\n",
    "        print(\"Successfully fitted and saved model results; total time taken %2.3f seconds\" % (time.time() - t0))\n",
    "        return fitted_model\n",
    "\n",
    "    # Save model\n",
    "    mdl_path = os.path.join(FITTED_MODEL_DIR, 'full_models', 'fitted_model_%s.pkl' % timestring)\n",
    "    print(\"Saving model at %s...\" % mdl_path)\n",
    "    file = open(mdl_path, 'wb')\n",
    "    fitted_model.save(file)\n",
    "    file.close()\n",
    "\n",
    "    model_results_to_save_separately = {}\n",
    "    for attr_to_save_separately in ['history', 'CBGS_TO_IDXS']:\n",
    "        model_results_to_save_separately[attr_to_save_separately] = getattr(fitted_model, attr_to_save_separately)\n",
    "\n",
    "    if SAVE_MODEL_RESULTS_SEPARATELY:\n",
    "        # Save some smaller model results for quick(er) loading. For really fast stuff, like losses (numerical results only) we store separately.\n",
    "        print(\"Saving model results...\")\n",
    "        file = open(os.path.join(helper.FITTED_MODEL_DIR, 'model_results', 'model_results_%s.pkl' % timestring), 'wb')\n",
    "        pickle.dump(model_results_to_save_separately, file)\n",
    "        file.close()\n",
    "\n",
    "    # evaluate model fit to cases and save loss separately as well.\n",
    "    # Everything saved in this data structure should be a summary result - small and fast to load, numbers only!\n",
    "    loss_dict = compare_model_vs_real_num_cases(nyt_outcomes,\n",
    "                                           model_kwargs['min_datetime'],\n",
    "                                           model_results=model_results_to_save_separately)\n",
    "    fast_to_load_results = {'loss_dict':loss_dict}\n",
    "    if train_test_partition is not None:\n",
    "        print('Computing train/test loss...')\n",
    "        train_max = train_test_partition + datetime.timedelta(hours=-1)\n",
    "        train_loss_dict = compare_model_vs_real_num_cases(nyt_outcomes,\n",
    "                                           model_kwargs['min_datetime'],\n",
    "                                           compare_end_time = train_max,\n",
    "                                           model_results=model_results_to_save_separately)\n",
    "        fast_to_load_results['train_loss_dict'] = train_loss_dict\n",
    "        test_loss_dict = compare_model_vs_real_num_cases(nyt_outcomes,\n",
    "                                           model_kwargs['min_datetime'],\n",
    "                                           compare_start_time = train_test_partition,\n",
    "                                           model_results=model_results_to_save_separately)\n",
    "        fast_to_load_results['test_loss_dict'] = test_loss_dict\n",
    "        fast_to_load_results['train_test_date_cutoff'] = train_test_partition\n",
    "        # sanity_check_error_metrics(fast_to_load_results)\n",
    "    mdl_summary, per_county, _ = compare_model_vs_real_num_cases_per_county(nyt_outcomes, fitted_model, \n",
    "                                                                            model_kwargs['min_datetime']) \n",
    "    fast_to_load_results['agg_county_loss_dict'] = mdl_summary  # losses aggregated over counties\n",
    "    fast_to_load_results['per_county_loss_dict'] = per_county  # losses per county\n",
    "\n",
    "    fast_to_load_results['clipping_monitor'] = fitted_model.clipping_monitor\n",
    "    fast_to_load_results['final infected fraction'] = (fitted_model.cbg_infected + fitted_model.cbg_removed + fitted_model.cbg_latent).sum(axis=1)/fitted_model.CBG_SIZES.sum()\n",
    "    fast_to_load_results['estimated_R0'] = fitted_model.estimated_R0\n",
    "    fast_to_load_results['intervention_cost'] = fitted_model.INTERVENTION_COST\n",
    "\n",
    "    file = open(os.path.join(FITTED_MODEL_DIR, 'fast_to_load_results_only', 'fast_to_load_results_%s.pkl' % timestring), 'wb')\n",
    "    pickle.dump(fast_to_load_results, file)\n",
    "    file.close()\n",
    "    print(\"Successfully fitted and saved model and data_and_model_kwargs; total time taken %2.3f seconds\" % (time.time() - t0))\n",
    "    return fitted_model\n",
    "\n",
    "def load_model_and_data_from_timestring(timestring, verbose=False, load_original_data=False,\n",
    "                                        load_full_model=False, load_fast_results_only=True,\n",
    "                                        load_filtered_data_model_was_fitted_on=False,\n",
    "                                        old_directory=False):\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Loading model from timestring %s\" % timestring)\n",
    "    if old_directory:\n",
    "        model_dir = OLD_FITTED_MODEL_DIR\n",
    "    else:\n",
    "        model_dir = FITTED_MODEL_DIR\n",
    "    f = open(os.path.join(model_dir, 'data_and_model_configs', 'config_%s.pkl' % timestring), 'rb')\n",
    "    data_and_model_kwargs = pickle.load(f)\n",
    "    f.close()\n",
    "    model = None\n",
    "    model_results = None\n",
    "    f = open(os.path.join(model_dir, 'fast_to_load_results_only', 'fast_to_load_results_%s.pkl' % timestring), 'rb')\n",
    "    fast_to_load_results = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    if not load_fast_results_only:\n",
    "        if SAVE_MODEL_RESULTS_SEPARATELY:\n",
    "            f = open(os.path.join(helper.FITTED_MODEL_DIR, 'model_results', 'model_results_%s.pkl' % timestring), 'rb')\n",
    "            model_results = pickle.load(f)\n",
    "            f.close()\n",
    "\n",
    "        if load_full_model:\n",
    "            f = open(os.path.join(model_dir, 'full_models', 'fitted_model_%s.pkl' % timestring), 'rb')\n",
    "            model = pickle.load(f)\n",
    "            f.close()\n",
    "\n",
    "    if load_original_data:\n",
    "        if verbose:\n",
    "            print(\"Loading original data as well...warning, this may take a while\")\n",
    "        d = helper.load_dataframe_for_individual_msa(**data_and_model_kwargs['data_kwargs'])\n",
    "    else:\n",
    "        d = None\n",
    "\n",
    "    if load_filtered_data_model_was_fitted_on:\n",
    "        # if true, return the data after all the filtering, along with the model prior to fitting.\n",
    "        data_kwargs = data_and_model_kwargs['data_kwargs'].copy()\n",
    "        model_kwargs = data_and_model_kwargs['model_kwargs'].copy()\n",
    "        model_kwargs['return_model_and_data_without_fitting'] = True\n",
    "        unfitted_model = fit_and_save_one_model(timestring=None,\n",
    "                                     model_kwargs=model_kwargs,\n",
    "                                     data_kwargs=data_kwargs,\n",
    "                                     train_test_partition=None)\n",
    "        filtered_data = unfitted_model.d\n",
    "        return model, data_and_model_kwargs, d, model_results, fast_to_load_results, filtered_data, unfitted_model\n",
    "\n",
    "    else:\n",
    "        return model, data_and_model_kwargs, d, model_results, fast_to_load_results\n",
    "\n",
    "def get_full_activity_num_visits(msa, intervention_datetime, extra_weeks_to_simulate, min_datetime, max_datetime):\n",
    "    \"\"\"\n",
    "    Get the total number of visits post-intervention date assuming we just looped activity from the first week\n",
    "    \"\"\"\n",
    "    fn = get_ipf_filename(msa, min_datetime, max_datetime, True, True)\n",
    "    f = open(fn, 'rb')\n",
    "    poi_cbg_visits_list = pickle.load(f)\n",
    "    f.close()\n",
    "    all_hours = helper.list_hours_in_range(min_datetime, max_datetime + datetime.timedelta(hours=168 * extra_weeks_to_simulate))\n",
    "    assert(intervention_datetime in all_hours)\n",
    "    intervention_hour_idx = all_hours.index(intervention_datetime)\n",
    "    full_total = 0\n",
    "    for t in range(intervention_hour_idx, len(all_hours)):\n",
    "        full_activity_matrix = poi_cbg_visits_list[t % 168]\n",
    "        full_total += full_activity_matrix.sum()\n",
    "    return full_total, intervention_hour_idx\n",
    "\n",
    "def get_lir_checkpoints_and_prop_visits_lost(timestring, intervention_hour_idx,\n",
    "                                             full_activity_num_visits=None, group='all', normalize=True):\n",
    "    \"\"\"\n",
    "    Returns the fraction of the population in state L+I+R at two checkpoints: at the point of reopening,\n",
    "    and at the end of the simulation. Also returns the proportion of visits lost after the reopening,\n",
    "    compared to full reopening.\n",
    "    \"\"\"\n",
    "    model, kwargs, _, _, fast_to_load_results = load_model_and_data_from_timestring(timestring,\n",
    "                                                                 load_fast_results_only=False,\n",
    "                                                                 load_full_model=True)\n",
    "    group_history = model.history[group]\n",
    "    lir = group_history['latent'] + group_history['infected'] + group_history['removed']\n",
    "    pop_size = group_history['total_pop']\n",
    "    if normalize:\n",
    "        intervention_lir = lir[:, intervention_hour_idx] / pop_size\n",
    "        final_lir = lir[:, -1] / pop_size\n",
    "    else:\n",
    "        intervention_lir = lir[:, intervention_hour_idx]\n",
    "        final_lir = lir[:, -1]\n",
    "    intervention_cost = fast_to_load_results['intervention_cost']\n",
    "    if 'total_activity_after_max_capacity_capping' in intervention_cost:\n",
    "        # the max_capacity_capping and uniform reduction experiments save different activity measures\n",
    "        # the max_capacity_capping experiments save 'total_activity_after_max_capacity_capping'\n",
    "        # which needs to be translated into prop visits lost\n",
    "        # the uniform reduction experiments save 'overall_cost' which is the percentage of visits lost\n",
    "        # so it needs to be divided by 100 to be a decimal\n",
    "        assert full_activity_num_visits is not None\n",
    "        num_visits = intervention_cost['total_activity_after_max_capacity_capping']\n",
    "        visits_lost = (full_activity_num_visits - num_visits) / full_activity_num_visits\n",
    "    else:\n",
    "        assert 'overall_cost' in intervention_cost\n",
    "        visits_lost = intervention_cost['overall_cost'] / 100\n",
    "    return intervention_lir, final_lir, visits_lost\n",
    "\n",
    "def get_uniform_proportions_per_msa(min_timestring=None, max_cap_df=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Get the proportion of visits kept for each max capacity experiment, so that we can run the corresponding\n",
    "    experiment with uniform reduction.\n",
    "    \"\"\"\n",
    "    assert not(min_timestring is None and max_cap_df is None)\n",
    "    if max_cap_df is None:\n",
    "        max_cap_df = evaluate_all_fitted_models_for_experiment('test_max_capacity_clipping',\n",
    "                                                       min_timestring=min_timestring)\n",
    "    max_cap_df['MSA_name'] = max_cap_df['data_kwargs'].map(lambda x:x['MSA_name'])\n",
    "    k = 'max_capacity_alpha'\n",
    "    max_cap_df['counterfactual_%s' % k] = max_cap_df['counterfactual_poi_opening_experiment_kwargs'].map(lambda x:x[k])\n",
    "    extra_weeks_to_simulate = max_cap_df.iloc[0]['counterfactual_poi_opening_experiment_kwargs']['extra_weeks_to_simulate']\n",
    "    intervention_datetime = max_cap_df.iloc[0]['counterfactual_poi_opening_experiment_kwargs']['intervention_datetime']\n",
    "    min_datetime = max_cap_df.iloc[0]['model_kwargs']['min_datetime']\n",
    "    max_datetime = max_cap_df.iloc[0]['model_kwargs']['max_datetime']\n",
    "\n",
    "    msa2proportions = {}\n",
    "    for msa in max_cap_df.MSA_name.unique():\n",
    "        full_activity, intervention_idx = get_full_activity_num_visits(msa,\n",
    "                                               intervention_datetime=intervention_datetime,\n",
    "                                               extra_weeks_to_simulate=extra_weeks_to_simulate,\n",
    "                                               min_datetime=min_datetime,\n",
    "                                               max_datetime=max_datetime)\n",
    "        msa_df = max_cap_df[max_cap_df['MSA_name'] == msa]\n",
    "        values = sorted(msa_df['counterfactual_max_capacity_alpha'].unique())\n",
    "        proportions = []\n",
    "        for v in values:\n",
    "            first_ts = msa_df[msa_df.counterfactual_max_capacity_alpha == v].iloc[0].timestring\n",
    "            _, _, visits_lost = get_lir_checkpoints_and_prop_visits_lost(first_ts,\n",
    "                        intervention_idx, group='all', full_activity_num_visits=full_activity)\n",
    "            proportions.append(np.round(1 - visits_lost, 5))\n",
    "        msa2proportions[msa] = proportions\n",
    "        if verbose:\n",
    "            print(msa, proportions)\n",
    "    return msa2proportions\n",
    "\n",
    "###################################################\n",
    "# Code for running many models in parallel\n",
    "###################################################\n",
    "def generate_data_and_model_configs(config_idx_to_start_at=None,\n",
    "                                    skip_previously_fitted_kwargs=False,\n",
    "                                    min_timestring=None,\n",
    "                                    min_timestring_to_load_best_fit_models=MIN_TIMESTRING_TO_LOAD_BEST_FIT_MODELS,\n",
    "                                    max_timestring_to_load_best_fit_models=MAX_TIMESTRING_TO_LOAD_BEST_FIT_MODELS,\n",
    "                                    experiment_to_run='normal_grid_search',\n",
    "                                    how_to_select_best_grid_search_models=None,\n",
    "                                    min_datetime=MIN_DATETIME,\n",
    "                                    max_datetime=MAX_DATETIME,\n",
    "                                    max_models_to_take_per_msa=MAX_MODELS_TO_TAKE_PER_MSA,\n",
    "                                    acceptable_loss_tolerance=ACCEPTABLE_LOSS_TOLERANCE,\n",
    "                                    version='v2'):\n",
    "    \"\"\"\n",
    "    Generates the set of parameter configurations for a given experiment.\n",
    "    MSAs to fit: how many MSAs we will focus on.\n",
    "    config_idx_to_start_at: how many configs we should skip.\n",
    "    \"\"\"\n",
    "    # this controls what parameters we search over.\n",
    "    config_generation_start_time = time.time()\n",
    "\n",
    "    if skip_previously_fitted_kwargs:\n",
    "        assert min_timestring is not None\n",
    "        previously_fitted_timestrings = filter_timestrings_for_properties(min_timestring=min_timestring)\n",
    "        previously_fitted_data_and_model_kwargs = [pickle.load(open(os.path.join(FITTED_MODEL_DIR, 'data_and_model_configs', 'config_%s.pkl' % timestring), 'rb')) for timestring in previously_fitted_timestrings]\n",
    "        print(\"Filtering out %i previously generated configs\" % len(previously_fitted_data_and_model_kwargs))\n",
    "    else:\n",
    "        previously_fitted_data_and_model_kwargs = []\n",
    "    if version == 'v1':\n",
    "        msas = MSAS\n",
    "    else:\n",
    "        msas = MSAS_IMPLEMENTED_FOR_V2   # Columbia_SC\n",
    "    print('Running experiment=%s for these MSAs:' % experiment_to_run, msas)\n",
    "    data_kwargs = [{'MSA_name':msa_name, 'nrows':None} for msa_name in msas]\n",
    "    # Generate model kwargs. How exactly we do this depends on which experiments we're running.\n",
    "    num_seeds = 30   # Compute 30 times for each MSA.  # Huan ?\n",
    "    configs_with_changing_params = []\n",
    "    # will 1050 configs: 15 * 10 * 7\n",
    "\n",
    "    if experiment_to_run == 'just_save_ipf_output':\n",
    "        model_kwargs = [{'min_datetime':min_datetime,\n",
    "                         'max_datetime':max_datetime,\n",
    "                         'exogenous_model_kwargs': {  # could be anything, will not affect IPF\n",
    "                            'home_beta':1e-2,\n",
    "                            'poi_psi':1000,\n",
    "                            'p_sick_at_t0':1e-4,\n",
    "                            'just_compute_r0':False,\n",
    "                          },\n",
    "                          'simulation_kwargs': {\n",
    "                              'do_ipf':True,\n",
    "                          },\n",
    "                          'poi_attributes_to_clip':{\n",
    "                              'clip_areas':True,\n",
    "                              'clip_dwell_times':True,\n",
    "                              'clip_visits':True\n",
    "                          },\n",
    "                          'model_init_kwargs':{\n",
    "                            'ipf_final_match':'poi',\n",
    "                            'ipf_num_iter':100,\n",
    "                            'num_seeds':2,  # don't need more than 1 seed for IPF, use 2 to be safe\n",
    "                          },\n",
    "                          'include_cbg_prop_out':True}]\n",
    "        \n",
    "    elif experiment_to_run == 'normal_grid_search' or experiment_to_run == 'grid_search_no_mask_data':\n",
    "        # sample the poi_psi and home_beta from plausible range.\n",
    "        poi_psi_num = 4\n",
    "        home_beta_num = 4\n",
    "        beta_multipliers_num = 4\n",
    "        poi_psis = np.linspace(BETA_AND_PSI_PLAUSIBLE_RANGE_SINGLE['min_poi_psi'],\n",
    "                               BETA_AND_PSI_PLAUSIBLE_RANGE_SINGLE['max_poi_psi'], poi_psi_num)\n",
    "        home_betas = np.linspace(BETA_AND_PSI_PLAUSIBLE_RANGE_SINGLE['min_home_beta'],\n",
    "                                 BETA_AND_PSI_PLAUSIBLE_RANGE_SINGLE['max_home_beta'], home_beta_num)\n",
    "\n",
    "        # ?? do not understand\n",
    "        # fine tuning the home_betasj, 15*10*7 = 1050\n",
    "        beta_multipliers = np.linspace(BETA_PLAUSIBLE_RANGE_SINGLE[0], BETA_PLAUSIBLE_RANGE_SINGLE[1], beta_multipliers_num)\n",
    "        for poi_psi in poi_psis:  # iter: 15\n",
    "            for start_beta in home_betas:   # iter: 10\n",
    "                for multiplier in beta_multipliers:  # iter: 7\n",
    "                    end_beta = multiplier * start_beta\n",
    "                    configs_with_changing_params.append({'poi_psi':poi_psi, \n",
    "                                                         'home_beta':[start_beta, end_beta],\n",
    "                                                         #end_beta = multiplier * start_beta\n",
    "                                                         'p_sick_at_t0':None})\n",
    "    \n",
    "    elif experiment_to_run == 'grid_search_fixed_beta':\n",
    "        poi_psis = np.linspace(BETA_AND_PSI_PLAUSIBLE_RANGE['min_poi_psi'], \n",
    "                               BETA_AND_PSI_PLAUSIBLE_RANGE['max_poi_psi'], 15)\n",
    "        home_betas = np.linspace(BETA_AND_PSI_PLAUSIBLE_RANGE['min_home_beta'],\n",
    "                                 BETA_AND_PSI_PLAUSIBLE_RANGE['max_home_beta'], 30)  # finer grid since one less param\n",
    "        for poi_psi in poi_psis:\n",
    "            for start_beta in home_betas:\n",
    "                configs_with_changing_params.append({'poi_psi':poi_psi, \n",
    "                                                     'home_beta':start_beta, \n",
    "                                                     'p_sick_at_t0':None})\n",
    "                    \n",
    "    elif experiment_to_run == 'grid_search_no_mobility':\n",
    "        # expanded range since we removed psi\n",
    "        home_betas = np.linspace(BETA_AND_PSI_PLAUSIBLE_RANGE['min_home_beta'],\n",
    "                                 BETA_AND_PSI_PLAUSIBLE_RANGE['max_home_beta']*1.5, 40)  # finer grid since one less param\n",
    "        beta_multipliers = np.linspace(BETA_PLAUSIBLE_RANGE[0], BETA_PLAUSIBLE_RANGE[1], 7)\n",
    "        for start_beta in home_betas:\n",
    "            for multiplier in beta_multipliers:\n",
    "                end_beta = multiplier * start_beta\n",
    "                configs_with_changing_params.append({'poi_psi':0,\n",
    "                                                     'home_beta':[start_beta, end_beta], \n",
    "                                                     'p_sick_at_t0':None})\n",
    "                \n",
    "    elif experiment_to_run == 'grid_search_aggregate_mobility':\n",
    "        p_sicks = P0_SICK_RANGE\n",
    "        beta_and_psi_plausible_range_for_aggregate_mobility = {\"min_home_beta\": 0.0011982272027079982,\n",
    "                                        \"max_home_beta\": 0.023964544054159966,\n",
    "                                        \"max_poi_psi\": 0.25,\n",
    "                                        \"min_poi_psi\": 2.5}\n",
    "        home_betas = np.linspace(beta_and_psi_plausible_range_for_aggregate_mobility['min_home_beta'],\n",
    "                                 beta_and_psi_plausible_range_for_aggregate_mobility['max_home_beta'], 10)\n",
    "        poi_psis = np.linspace(beta_and_psi_plausible_range_for_aggregate_mobility['min_poi_psi'],\n",
    "                               beta_and_psi_plausible_range_for_aggregate_mobility['max_poi_psi'], 15)\n",
    "        for home_beta in home_betas:\n",
    "            for poi_psi in poi_psis:\n",
    "                for p_sick in p_sicks:\n",
    "                    configs_with_changing_params.append({'home_beta':home_beta, 'poi_psi':poi_psi, 'p_sick_at_t0':p_sick})\n",
    "\n",
    "    elif experiment_to_run == 'grid_search_home_proportion_beta':\n",
    "        p_sicks = P0_SICK_RANGE\n",
    "        home_betas = np.linspace(BETA_AND_PSI_PLAUSIBLE_RANGE['min_home_beta'],\n",
    "            BETA_AND_PSI_PLAUSIBLE_RANGE['max_home_beta'], 10)\n",
    "        poi_psis = np.linspace(BETA_AND_PSI_PLAUSIBLE_RANGE['min_poi_psi'], BETA_AND_PSI_PLAUSIBLE_RANGE['max_poi_psi'], 15)\n",
    "        for home_beta in home_betas:\n",
    "            for poi_psi in poi_psis:\n",
    "                for p_sick in p_sicks:\n",
    "                    configs_with_changing_params.append({'home_beta':home_beta, 'poi_psi':poi_psi, 'p_sick_at_t0':p_sick})\n",
    "    \n",
    "    elif experiment_to_run == 'grid_search_inter_cbg_gamma':\n",
    "        if min_datetime < datetime.datetime(2020, 4, 1):   # try different p0's if near the beginning of pandemic\n",
    "            p_sicks = P0_SICK_RANGE\n",
    "        else:\n",
    "            p_sicks = [None]  # must rely on reported cases / deaths to initialize\n",
    "        home_betas = np.linspace(BETA_AND_PSI_PLAUSIBLE_RANGE['min_home_beta'],\n",
    "            BETA_AND_PSI_PLAUSIBLE_RANGE['max_home_beta'], 5)\n",
    "        poi_psis = np.linspace(BETA_AND_PSI_PLAUSIBLE_RANGE['min_poi_psi'], BETA_AND_PSI_PLAUSIBLE_RANGE['max_poi_psi'], 5)\n",
    "        leak_gammas = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "        for home_beta in home_betas:\n",
    "            for poi_psi in poi_psis:\n",
    "                for p_sick in p_sicks:\n",
    "                    for gamma in leak_gammas:\n",
    "                        configs_with_changing_params.append({'home_beta':home_beta, \n",
    "                                                             'poi_psi':poi_psi, \n",
    "                                                             'p_sick_at_t0':p_sick,\n",
    "                                                             'inter_cbg_gamma':gamma})\n",
    "        \n",
    "    elif experiment_to_run == 'calibrate_r0':\n",
    "        # home_betas = [5e-1, 2e-1, 1e-1, 5e-2, 2e-2, 1e-2, 5e-3, 2e-3, 1e-3, 5e-4, 2e-4, 1e-4]\n",
    "        # poi_psis = [20000, 16000, 13000, 10000, 7500, 6000, 5000, 4500, 4000,\n",
    "        #             3500, 3000, 2500, 2000, 1500, 1000, 500, 250, 100]\n",
    "\n",
    "        home_betas = [0.00001,  0.00002, 0.00005, 0.001, 0.005,  0.01]\n",
    "        poi_psis = [10, 100, 200, 300, 400, 500, 50]\n",
    "                    \n",
    "        # for home_beta in home_betas:\n",
    "        #     configs_with_changing_params.append({'home_beta':home_beta, 'poi_psi':2500, 'p_sick_at_t0':1e-4})\n",
    "        # for poi_psi in poi_psis:\n",
    "        #     configs_with_changing_params.append({'home_beta':0.001, 'poi_psi':poi_psi, 'p_sick_at_t0':1e-4})\n",
    "\n",
    "        # Huan\n",
    "        for home_beta in home_betas:\n",
    "            # configs_with_changing_params.append({'home_beta':home_beta, 'poi_psi':2500, 'p_sick_at_t0':1e-4})\n",
    "            for poi_psi in poi_psis:\n",
    "                configs_with_changing_params.append({'home_beta':home_beta, 'poi_psi':poi_psi, 'p_sick_at_t0':1e-4})\n",
    "\n",
    "    elif experiment_to_run == 'calibrate_r0_aggregate_mobility':\n",
    "        # home beta range will be the same as normal experiment\n",
    "        poi_psis = [50, 25, 10, 5,  1, 0.5, 0.1, 0.005, 0.001]\n",
    "        for poi_psi in poi_psis:\n",
    "            configs_with_changing_params.append({'home_beta':0.001, 'poi_psi':poi_psi, 'p_sick_at_t0':1e-4})\n",
    "\n",
    "    # experiments that require the best fit models\n",
    "    best_models_experiments = {\n",
    "        'test_interventions',\n",
    "        'test_retrospective_counterfactuals',\n",
    "        'test_max_capacity_clipping',\n",
    "        'test_uniform_proportion_of_full_reopening',\n",
    "        'rerun_best_models_and_save_cases_per_poi',\n",
    "        'test_reopening_category_combinations',\n",
    "        'test_category_combinations_for_dashboard'}\n",
    "    if experiment_to_run in best_models_experiments:\n",
    "        # here model and data kwargs are entwined, so we can't just take the outer product of model_kwargs and data_kwargs.\n",
    "        # this is because we load the best fitting model for each MSA.\n",
    "        list_of_data_and_model_kwargs = []\n",
    "        poi_categories_to_examine = 20\n",
    "        if how_to_select_best_grid_search_models == 'daily_cases_rmse':\n",
    "            key_to_sort_by = 'loss_dict_daily_cases_RMSE'\n",
    "        elif how_to_select_best_grid_search_models == 'daily_deaths_rmse':\n",
    "            key_to_sort_by = 'loss_dict_daily_deaths_RMSE'\n",
    "        elif how_to_select_best_grid_search_models == 'daily_cases_poisson':\n",
    "            key_to_sort_by = 'loss_dict_daily_cases_poisson_NLL_thres-10_sum'\n",
    "        elif how_to_select_best_grid_search_models == 'daily_cases_rmse_time_varying_cdr':\n",
    "            key_to_sort_by = 'loss_dict_daily_cases_RMSE_time_varying_cdr'\n",
    "        elif how_to_select_best_grid_search_models == 'smoothed_daily_cases_rmse_time_varying_cdr':\n",
    "            key_to_sort_by = 'loss_dict_smoothed_daily_cases_RMSE_time_varying_cdr'\n",
    "        else:\n",
    "            raise Exception(\"Not a valid means of selecting best-fit models\")\n",
    "        print(\"selecting best grid search models using criterion %s\" % how_to_select_best_grid_search_models)\n",
    "\n",
    "        # get list of all fitted models -- need this for any of the \"best fit models\" experiments\n",
    "        model_timestrings, model_msas = filter_timestrings_for_properties(\n",
    "            min_timestring=min_timestring_to_load_best_fit_models,\n",
    "            max_timestring=max_timestring_to_load_best_fit_models,\n",
    "            required_properties={'experiment_to_run':'normal_grid_search'},\n",
    "            return_msa_names=True)\n",
    "        print(\"Found %i models\" % len(model_timestrings))\n",
    "        timestring_msa_df = pd.DataFrame({'model_timestring':model_timestrings, 'model_msa':model_msas})\n",
    "        n_models_for_msa_prior_to_quality_filter = None\n",
    "\n",
    "        # get experiment-specific stuff\n",
    "        if experiment_to_run == 'test_interventions':\n",
    "            most_visited_poi_subcategories = get_list_of_poi_subcategories_with_most_visits(n_poi_categories=poi_categories_to_examine)\n",
    "        else:\n",
    "            most_visited_poi_subcategories = None\n",
    "        if experiment_to_run == 'test_uniform_proportion_of_full_reopening':\n",
    "            # need to match visits lost from max capacity clipping experiments\n",
    "            msa2proportions = get_uniform_proportions_per_msa(\n",
    "                min_timestring=min_timestring_to_load_best_fit_models)\n",
    "        else:\n",
    "            msa2proportions = None\n",
    "\n",
    "        for row in data_kwargs:   # {}\n",
    "            msa_t0 = time.time()\n",
    "            msa_name = row['MSA_name']\n",
    "            timestrings_for_msa = list(\n",
    "                timestring_msa_df.loc[timestring_msa_df['model_msa'] == msa_name, 'model_timestring'].values)\n",
    "            print(\"Evaluating %i timestrings for %s\" % (len(timestrings_for_msa), msa_name))\n",
    "\n",
    "            # sort the models\n",
    "            best_msa_models = evaluate_all_fitted_models_for_msa(msa_name, timestrings=timestrings_for_msa)\n",
    "\n",
    "            # select the normal_grid_search results\n",
    "            best_msa_models = best_msa_models.loc[(best_msa_models['experiment_to_run'] == 'normal_grid_search') &\n",
    "            (best_msa_models['poi_psi'] > 0)].sort_values(by=key_to_sort_by)\n",
    "\n",
    "            # number of models (?)\n",
    "            if n_models_for_msa_prior_to_quality_filter is None:\n",
    "                n_models_for_msa_prior_to_quality_filter = len(best_msa_models) # make sure nothing weird happening / no duplicate models.\n",
    "            else:\n",
    "                assert len(best_msa_models) == n_models_for_msa_prior_to_quality_filter\n",
    "\n",
    "            best_loss = float(best_msa_models.iloc[0][key_to_sort_by])\n",
    "            print(\"After filtering for normal_grid_search models, %i models for MSA\" % (len(best_msa_models)))\n",
    "            best_msa_models = best_msa_models.loc[best_msa_models[key_to_sort_by] <= acceptable_loss_tolerance * best_loss]\n",
    "            # Select several models, not only one.\n",
    "\n",
    "            # limit the number of models.\n",
    "            best_msa_models = best_msa_models.iloc[:max_models_to_take_per_msa]\n",
    "            print(\"After filtering for models with %s within factor %2.3f of best loss, and taking max %i models, %i models\" %\n",
    "                (key_to_sort_by, acceptable_loss_tolerance, max_models_to_take_per_msa, len(best_msa_models)))\n",
    "\n",
    "            # for each model\n",
    "            for i in range(len(best_msa_models)):\n",
    "\n",
    "                loss_ratio = best_msa_models.iloc[i][key_to_sort_by]/best_loss   # check the loss if in the range [1, 1.2 or other]\n",
    "                assert loss_ratio >= 1 and loss_ratio <= acceptable_loss_tolerance\n",
    "                model_quality_dict = {'model_fit_rank_for_msa':i,\n",
    "                                      'how_to_select_best_grid_search_models':how_to_select_best_grid_search_models,\n",
    "                                      'ratio_of_%s_to_that_of_best_fitting_model' % key_to_sort_by:loss_ratio,\n",
    "                                      'model_timestring':best_msa_models.iloc[i]['timestring']}\n",
    "                _, kwargs_i, _, _, _ = load_model_and_data_from_timestring(best_msa_models.iloc[i]['timestring'], load_fast_results_only=True)\n",
    "                kwargs_i['experiment_to_run'] = experiment_to_run\n",
    "                del kwargs_i['model_kwargs']['counties_to_track']\n",
    "\n",
    "                if experiment_to_run == 'test_retrospective_counterfactuals':\n",
    "                    # LOOKING AT THE PAST.\n",
    "                    # what if we had only done x% of social distancing?\n",
    "                    # degree represents what percentage of social distancing to keep - we don't need to test 1\n",
    "                    # because that is what actually happened\n",
    "                    for degree in [0, 0.25, 0.5, 0.75]:\n",
    "                        kwarg_copy = copy.deepcopy(kwargs_i) # don't modify by mistake in pass-by-reference.\n",
    "                        counterfactual_retrospective_experiment_kwargs = {'distancing_degree':degree}\n",
    "                        kwarg_copy['model_kwargs']['model_quality_dict'] = model_quality_dict.copy()\n",
    "                        kwarg_copy['model_kwargs']['counterfactual_retrospective_experiment_kwargs'] = counterfactual_retrospective_experiment_kwargs\n",
    "                        list_of_data_and_model_kwargs.append(kwarg_copy)\n",
    "                    # above: prepare parameters\n",
    "\n",
    "                    # what if we shifted the timeseries by x days?\n",
    "                    for shift in [-7, -3, 3, 7]:  # how many days to shift\n",
    "                        kwarg_copy = copy.deepcopy(kwargs_i) # don't modify by mistake in pass-by-reference.\n",
    "                        counterfactual_retrospective_experiment_kwargs = {'shift_in_days':shift}\n",
    "                        kwarg_copy['model_kwargs']['model_quality_dict'] = model_quality_dict.copy()\n",
    "                        kwarg_copy['model_kwargs']['counterfactual_retrospective_experiment_kwargs'] = counterfactual_retrospective_experiment_kwargs\n",
    "                        list_of_data_and_model_kwargs.append(kwarg_copy)  # the same list as degree\n",
    "                    # above: do not know what is shift. Huan\n",
    "\n",
    "                elif experiment_to_run == 'test_interventions':\n",
    "                    # FUTURE EXPERIMENTS: reopen each category of POI.\n",
    "                    for cat_idx in range(len(most_visited_poi_subcategories)):\n",
    "                        for alpha in [0, 1]:\n",
    "                            kwarg_copy = copy.deepcopy(kwargs_i) # don't modify by mistake in pass-by-reference.\n",
    "                            counterfactual_poi_opening_experiment_kwargs = {'alpha':alpha,\n",
    "                                                   'extra_weeks_to_simulate':4,\n",
    "                                                   'intervention_datetime':datetime.datetime(2020, 5, 1, 0),\n",
    "                                                   'top_category':None,\n",
    "                                                   'sub_category':most_visited_poi_subcategories[cat_idx]}\n",
    "                            kwarg_copy['model_kwargs']['model_quality_dict'] = model_quality_dict.copy()\n",
    "\n",
    "                            kwarg_copy['model_kwargs']['counterfactual_poi_opening_experiment_kwargs'] = counterfactual_poi_opening_experiment_kwargs\n",
    "                            list_of_data_and_model_kwargs.append(kwarg_copy)\n",
    "\n",
    "                elif experiment_to_run == 'test_max_capacity_clipping':\n",
    "                    # FUTURE EXPERIMENTS: reopen fully but clip at alpha-proportion of max capacity\n",
    "                    for alpha in np.arange(.1, 1.1, .1):\n",
    "                        kwarg_copy = copy.deepcopy(kwargs_i) # don't modify by mistake in pass-by-reference.\n",
    "                        counterfactual_poi_opening_experiment_kwargs = {\n",
    "                                               'extra_weeks_to_simulate':4,\n",
    "                                               'intervention_datetime':datetime.datetime(2020, 5, 1, 0),\n",
    "                                               'alpha':1,  # assume full activity before clipping\n",
    "                                               'max_capacity_alpha':alpha}\n",
    "                        kwarg_copy['model_kwargs']['model_quality_dict'] = model_quality_dict.copy()\n",
    "                        kwarg_copy['model_kwargs']['counterfactual_poi_opening_experiment_kwargs'] = counterfactual_poi_opening_experiment_kwargs\n",
    "                        list_of_data_and_model_kwargs.append(kwarg_copy)\n",
    "\n",
    "                elif experiment_to_run == 'test_uniform_proportion_of_full_reopening':\n",
    "                    # FUTURE EXPERIMENTS: test uniform reopening on all pois, simple proportion of pre-lockdown activity\n",
    "                    for alpha in msa2proportions[msa_name]:\n",
    "                        kwarg_copy = copy.deepcopy(kwargs_i) # don't modify by mistake in pass-by-reference.\n",
    "                        counterfactual_poi_opening_experiment_kwargs = {\n",
    "                                               'extra_weeks_to_simulate':4,\n",
    "                                               'intervention_datetime':datetime.datetime(2020, 5, 1, 0),\n",
    "                                               'full_activity_alpha':alpha}\n",
    "                        kwarg_copy['model_kwargs']['model_quality_dict'] = model_quality_dict.copy()\n",
    "                        kwarg_copy['model_kwargs']['counterfactual_poi_opening_experiment_kwargs'] = counterfactual_poi_opening_experiment_kwargs\n",
    "                        list_of_data_and_model_kwargs.append(kwarg_copy)\n",
    "\n",
    "                elif experiment_to_run == 'rerun_best_models_and_save_cases_per_poi':\n",
    "                    # Rerun best fit models so that we can track the infection contribution of each POI,\n",
    "                    # overall and for each income decile.\n",
    "                    kwarg_copy = copy.deepcopy(kwargs_i) # don't modify by mistake in pass-by-reference.\n",
    "                    simulation_kwargs = {\n",
    "                        'groups_to_track_num_cases_per_poi':['all',\n",
    "                            'median_household_income_bottom_decile',\n",
    "                            'median_household_income_top_decile']}\n",
    "                    kwarg_copy['model_kwargs']['simulation_kwargs'] = simulation_kwargs\n",
    "                    kwarg_copy['model_kwargs']['model_quality_dict'] = model_quality_dict.copy()\n",
    "                    list_of_data_and_model_kwargs.append(kwarg_copy)\n",
    "               \n",
    "                elif experiment_to_run == 'test_reopening_category_combinations':\n",
    "                    if msa_name in {'Chicago_Naperville_Elgin_IL_IN_WI',\n",
    "                                    'New_York_Newark_Jersey_City_NY_NJ_PA'\n",
    "                                    'San_Francisco_Oakland_Hayward_CA'}:  # only run this experiment for these MSAs\n",
    "                        # pre-determined categories to test\n",
    "                        categories = ['Full-Service Restaurants', 'Fitness and Recreational Sports Centers',\n",
    "                                      'Limited-Service Restaurants', 'Religious Organizations',\n",
    "                                      'Pharmacies and Drug Stores', 'Department Stores',\n",
    "                                      'Supermarkets and Other Grocery (except Convenience) Stores']    \n",
    "                        for combination in range(2 ** len(categories)):\n",
    "                            binary_str = '{0:b}'.format(combination).zfill(len(categories))\n",
    "                            cats_to_open = []\n",
    "                            for i, digit in enumerate(binary_str):\n",
    "                                if digit == '1':\n",
    "                                    cats_to_open.append(categories[i])\n",
    "                            kwarg_copy = copy.deepcopy(kwargs_i) # don't modify by mistake in pass-by-reference.\n",
    "                            if len(cats_to_open) > 0:  # return at least one category to pre-pandemic levels\n",
    "                                counterfactual_poi_opening_experiment_kwargs = {'alpha':1,\n",
    "                                                       'extra_weeks_to_simulate':0,\n",
    "                                                       'intervention_datetime':datetime.datetime(2020, 3, 8, 0),\n",
    "                                                       'top_category':None,\n",
    "                                                       'sub_category':cats_to_open}\n",
    "                            else:  # keep everything at actual levels\n",
    "                                counterfactual_poi_opening_experiment_kwargs = {'alpha':0,\n",
    "                                                       'extra_weeks_to_simulate':0,\n",
    "                                                       'intervention_datetime':datetime.datetime(2020, 3, 8, 0),\n",
    "                                                       'top_category':None,\n",
    "                                                       'sub_category':None}\n",
    "                            kwarg_copy['model_kwargs']['model_quality_dict'] = model_quality_dict.copy()\n",
    "                            kwarg_copy['model_kwargs']['counterfactual_poi_opening_experiment_kwargs'] = counterfactual_poi_opening_experiment_kwargs\n",
    "                            list_of_data_and_model_kwargs.append(kwarg_copy)\n",
    "                \n",
    "                elif experiment_to_run == 'test_category_combinations_for_dashboard':\n",
    "                    opening_settings = [-1, 0, 0.5, 1]\n",
    "                    exp_base = len(opening_settings)\n",
    "                    groups = list(CATEGORY_GROUPS.keys())\n",
    "                    num_configs = exp_base ** len(groups)\n",
    "                    for config_idx in range(num_configs):\n",
    "                        digits = []\n",
    "                        remainder = config_idx\n",
    "                        while remainder > 0:  # convert into string with base exp_base\n",
    "                            digits.append(remainder % exp_base)\n",
    "                            remainder = remainder // exp_base\n",
    "                        while len(digits) < len(groups):  # pad with 0's\n",
    "                            digits.append(0)\n",
    "                        category2alpha = {}\n",
    "                        for digit, group in zip(digits, groups):\n",
    "                            alpha = opening_settings[digit]\n",
    "                            if alpha >= 0:  # if -1, we want current state\n",
    "                                cats = CATEGORY_GROUPS[group]\n",
    "                                for c in cats:\n",
    "                                    category2alpha[c] = alpha\n",
    "                        kwarg_copy = copy.deepcopy(kwargs_i)\n",
    "                        intervention_datetime = kwarg_copy['model_kwargs']['max_datetime'] + datetime.timedelta(hours=1)\n",
    "                        counterfactual_poi_opening_experiment_kwargs = {'category_to_alpha':category2alpha,\n",
    "                                                       'extra_weeks_to_simulate':4,\n",
    "                                                       'intervention_datetime':intervention_datetime}\n",
    "                        kwarg_copy['model_kwargs']['model_quality_dict'] = model_quality_dict.copy()\n",
    "                        kwarg_copy['model_kwargs']['counterfactual_poi_opening_experiment_kwargs'] = counterfactual_poi_opening_experiment_kwargs\n",
    "                        list_of_data_and_model_kwargs.append(kwarg_copy)\n",
    "\n",
    "            print(\"In total, it took %2.3f seconds to generate configs for MSA\" % (time.time() - msa_t0))\n",
    "\n",
    "        # sanity check to make sure nothing strange - number of parameters we expect.\n",
    "        expt_params = []\n",
    "        for row in list_of_data_and_model_kwargs:   # number = best-models * degree (4) * shift (4)  / 2 (?, seems no need to /2 . Huan)\n",
    "            expt_params.append(\n",
    "                {'home_beta':row['model_kwargs']['exogenous_model_kwargs']['home_beta'],\n",
    "                 'poi_psi':row['model_kwargs']['exogenous_model_kwargs']['poi_psi'],\n",
    "                 'p_sick_at_t0':row['model_kwargs']['exogenous_model_kwargs']['p_sick_at_t0'],\n",
    "                 'MSA_name':row['data_kwargs']['MSA_name']})\n",
    "        expt_params = pd.DataFrame(expt_params)   # expt_params is not use again, strange! Huan\n",
    "\n",
    "    else:  # if experiment_to_run is not in best_models_experiments\n",
    "        if experiment_to_run != 'just_save_ipf_output':  # model_kwargs is already set for ipf experiment\n",
    "            # create model kwargs.\n",
    "            model_kwargs = []\n",
    "            for config in configs_with_changing_params:\n",
    "                model_kwargs.append({'min_datetime':min_datetime,\n",
    "                                     'max_datetime':max_datetime,\n",
    "                                     'exogenous_model_kwargs': {\n",
    "                                        'home_beta':config['home_beta'],\n",
    "                                        'poi_psi':config['poi_psi'],\n",
    "                                        'p_sick_at_t0':config['p_sick_at_t0'],\n",
    "                                        'inter_cbg_gamma':config['inter_cbg_gamma'] if 'inter_cbg_gamma' in config else None,\n",
    "                                        'just_compute_r0':'calibrate_r0' in experiment_to_run,\n",
    "                                      },\n",
    "                                     'model_init_kwargs':{\n",
    "                                         'num_seeds':num_seeds,\n",
    "                                     },\n",
    "                                     'simulation_kwargs':{\n",
    "                                         'use_aggregate_mobility': 'aggregate_mobility' in experiment_to_run,\n",
    "                                         'use_home_proportion_beta': 'home_proportion_beta' in experiment_to_run,\n",
    "                                         'use_inter_cbg_leak_factor': 'inter_cbg_gamma' in experiment_to_run,\n",
    "                                     },\n",
    "                                     'poi_attributes_to_clip':{\n",
    "                                         'clip_areas':True,\n",
    "                                         'clip_dwell_times':True,\n",
    "                                         'clip_visits':True\n",
    "                                     },\n",
    "                                     'include_cbg_prop_out':'home_proportion_beta' in experiment_to_run,\n",
    "                                     'include_inter_cbg_travel': 'inter_cbg_gamma' in experiment_to_run,\n",
    "                                     'include_mask_use': not 'no_mask_data' in experiment_to_run,\n",
    "                                    })\n",
    "\n",
    "        list_of_data_and_model_kwargs = [{'data_kwargs':copy.deepcopy(a), 'model_kwargs':copy.deepcopy(b), 'experiment_to_run':experiment_to_run, 'version':version} for b in model_kwargs for a in data_kwargs]\n",
    "\n",
    "    # remove previously fitted kwargs\n",
    "    if len(previously_fitted_data_and_model_kwargs) > 0:\n",
    "        print(\"Prior to filtering out previously fitted kwargs, %i kwargs\" % len(list_of_data_and_model_kwargs))\n",
    "        for i in range(len(previously_fitted_data_and_model_kwargs)):\n",
    "            # remove stuff that is added when saved so configs are comparable.\n",
    "            if 'counties_to_track' in previously_fitted_data_and_model_kwargs[i]['model_kwargs']:\n",
    "                del previously_fitted_data_and_model_kwargs[i]['model_kwargs']['counties_to_track']\n",
    "            #if 'preload_poi_visits_list_filename' in previously_fitted_data_and_model_kwargs[i]['model_kwargs']:\n",
    "            #    del previously_fitted_data_and_model_kwargs[i]['model_kwargs']['preload_poi_visits_list_filename']\n",
    "\n",
    "        old_len = len(list_of_data_and_model_kwargs)\n",
    "        list_of_data_and_model_kwargs = [a for a in list_of_data_and_model_kwargs if a not in previously_fitted_data_and_model_kwargs]\n",
    "        assert old_len != len(list_of_data_and_model_kwargs)\n",
    "        print(\"After removing previously fitted kwargs, %i kwargs\" % (len(list_of_data_and_model_kwargs)))\n",
    "\n",
    "    print(\"Total data/model configs to fit: %i; randomly shuffling order\" % len(list_of_data_and_model_kwargs))\n",
    "    random.Random(0).shuffle(list_of_data_and_model_kwargs)   # do not know why shuffle. Huan\n",
    "    if config_idx_to_start_at is not None:\n",
    "        print(\"Skipping first %i configs\" % config_idx_to_start_at)\n",
    "        list_of_data_and_model_kwargs = list_of_data_and_model_kwargs[config_idx_to_start_at:]\n",
    "    print(\"Total time to generate configs: %2.3f seconds\" % (time.time() - config_generation_start_time))\n",
    "    return list_of_data_and_model_kwargs\n",
    "\n",
    "\n",
    "def get_list_of_poi_subcategories_with_most_visits(n_poi_categories, n_chunks=5, return_df_without_filtering_or_sorting=False):\n",
    "    \"\"\"\n",
    "    Return n_poi_categories subcategories with the most visits in \"normal times\" (Jan 2019 - Feb 2020)\n",
    "    \"\"\"\n",
    "    normal_times = helper.list_datetimes_in_range(datetime.datetime(2019, 1, 1),\n",
    "                                              datetime.datetime(2020, 2, 29))\n",
    "    normal_time_cols = ['%i.%i.%i' % (a.year, a.month, a.day) for a in normal_times]\n",
    "    must_have_cols = normal_time_cols + ['sub_category', 'top_category']\n",
    "    d = helper.load_multiple_chunks(range(n_chunks), cols=must_have_cols)\n",
    "    d['visits_in_normal_times'] = d[normal_time_cols].sum(axis=1)\n",
    "    if return_df_without_filtering_or_sorting:\n",
    "        d = d[['sub_category', 'visits_in_normal_times']]\n",
    "        grouped_d = d.groupby(['sub_category']).agg(['sum', 'size']).reset_index()\n",
    "        grouped_d.columns = ['Original Name', 'N visits', 'N POIs']\n",
    "        grouped_d['% POIs'] = 100 * grouped_d['N POIs'] / grouped_d['N POIs'].sum()\n",
    "        grouped_d['% visits'] = 100 * grouped_d['N visits'] / grouped_d['N visits'].sum()\n",
    "        grouped_d['Category'] = grouped_d['Original Name'].map(lambda x:SUBCATEGORIES_TO_PRETTY_NAMES[x] if x in SUBCATEGORIES_TO_PRETTY_NAMES else x)\n",
    "        grouped_d = grouped_d.sort_values(by='% visits')[::-1].head(n=n_poi_categories)[['Category', '% visits', '% POIs', 'N visits', 'N POIs']]\n",
    "        print('Percent of POIs: %2.3f; percent of visits: %2.3f' %\n",
    "              (grouped_d['% POIs'].sum(),\n",
    "               grouped_d['% visits'].sum()))\n",
    "        return grouped_d\n",
    "    assert((d.groupby('sub_category')['top_category'].nunique().values == 1).all()) # Make sure that each sub_category only maps to one top category (and so it's safe to just look at sub categories).\n",
    "    d = d.loc[d['sub_category'].map(lambda x:x not in REMOVED_SUBCATEGORIES)]\n",
    "    grouped_d = d.groupby('sub_category')['visits_in_normal_times'].sum().sort_values()[::-1].iloc[:n_poi_categories]\n",
    "    print(\"Returning the list of %i POI subcategories with the most visits, collectively accounting for percentage %2.1f%% of visits\" %\n",
    "        (n_poi_categories, 100*grouped_d.values.sum()/d['visits_in_normal_times'].sum()))\n",
    "    return list(grouped_d.index)\n",
    "\n",
    "def filter_timestrings_for_properties(required_properties=None,\n",
    "                                      required_model_kwargs=None,\n",
    "                                      required_data_kwargs=None,\n",
    "                                      min_timestring=None,\n",
    "                                      max_timestring=None,\n",
    "                                      return_msa_names=False,\n",
    "                                      old_directory=False):\n",
    "    \"\"\"\n",
    "    required_properties refers to params that are defined in data_and_model_kwargs, outside of ‘model_kwargs’ and ‘data_kwargs\n",
    "    \"\"\"\n",
    "    if required_properties is None:\n",
    "        required_properties = {}\n",
    "    if required_model_kwargs is None:\n",
    "        required_model_kwargs = {}\n",
    "    if required_data_kwargs is None:\n",
    "        required_data_kwargs = {}\n",
    "    if max_timestring is None:\n",
    "        max_timestring = str(datetime.datetime.now()).replace(' ', '_').replace('-', '_').replace('.', '_').replace(':', '_')\n",
    "    print(\"Loading models with timestrings between %s and %s\" % (str(min_timestring), max_timestring))\n",
    "    if old_directory:\n",
    "        config_dir = os.path.join(OLD_FITTED_MODEL_DIR, 'data_and_model_configs')\n",
    "    else:\n",
    "        config_dir = os.path.join(FITTED_MODEL_DIR, 'data_and_model_configs')\n",
    "    matched_timestrings = []\n",
    "    msa_names = []\n",
    "    configs_to_evaluate = os.listdir(config_dir)\n",
    "    print(\"%i files in directory %s\" % (len(configs_to_evaluate), config_dir))\n",
    "    for fn in configs_to_evaluate:\n",
    "        if fn.startswith('config_'):\n",
    "            timestring = fn.lstrip('config_').rstrip('.pkl')\n",
    "            if (timestring <= max_timestring) and (min_timestring is None or timestring >= min_timestring):\n",
    "                f = open(os.path.join(config_dir, fn), 'rb')\n",
    "                data_and_model_kwargs = pickle.load(f)\n",
    "                f.close()\n",
    "                if test_if_kwargs_match(required_properties,\n",
    "                                        required_data_kwargs,\n",
    "                                        required_model_kwargs,\n",
    "                                        data_and_model_kwargs):\n",
    "                    matched_timestrings.append(timestring)\n",
    "                    msa_names.append(data_and_model_kwargs['data_kwargs']['MSA_name'])\n",
    "    if not return_msa_names:\n",
    "        return matched_timestrings\n",
    "    else:\n",
    "        return matched_timestrings, msa_names\n",
    "\n",
    "    return matched_timestrings\n",
    "\n",
    "def test_if_kwargs_match(req_properties, req_data_kwargs,\n",
    "                         req_model_kwargs, test_data_and_model_kwargs):\n",
    "    # check whether direct properties in test_data_and_model_kwargs match\n",
    "    prop_match = all([req_properties[key] == test_data_and_model_kwargs[key] for key in req_properties if key not in ['data_kwargs', 'model_kwargs']])\n",
    "    if not prop_match:\n",
    "        return False\n",
    "\n",
    "    # check whether data kwargs match\n",
    "    test_data_kwargs = test_data_and_model_kwargs['data_kwargs']\n",
    "    data_match = all([req_data_kwargs[key] == test_data_kwargs[key] for key in req_data_kwargs])\n",
    "    if not data_match:\n",
    "        return False\n",
    "\n",
    "    # check if non-dictionary model kwargs match\n",
    "    kwargs_keys = set([key for key in req_model_kwargs if 'kwargs' in key])\n",
    "    test_model_kwargs = test_data_and_model_kwargs['model_kwargs']\n",
    "    model_match = all([req_model_kwargs[key] == test_model_kwargs[key] for key in req_model_kwargs if key not in kwargs_keys])\n",
    "    if not model_match:\n",
    "        return False\n",
    "\n",
    "    # check if elements within dictionary model kwargs match\n",
    "    for kw_key in kwargs_keys:\n",
    "        req_kwargs = req_model_kwargs[kw_key]\n",
    "        test_kwargs = test_model_kwargs[kw_key]\n",
    "        kw_match = all([req_kwargs[k] == test_kwargs[k] for k in req_kwargs])\n",
    "        if not kw_match:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def check_memory_usage():\n",
    "    virtual_memory = psutil.virtual_memory()\n",
    "    total_memory = getattr(virtual_memory, 'total')\n",
    "    available_memory = getattr(virtual_memory, 'available')\n",
    "    free_memory = getattr(virtual_memory, 'free')\n",
    "    available_memory_percentage = 100. * available_memory / total_memory\n",
    "    # Free memory is the amount of memory which is currently not used for anything. This number should be small, because memory which is not used is simply wasted.\n",
    "    # Available memory is the amount of memory which is available for allocation to a new process or to existing processes.\n",
    "    print('Total memory: %s; free memory: %s; available memory %s; available memory %2.3f%%' % (\n",
    "        bytes2human(total_memory),\n",
    "        bytes2human(free_memory),\n",
    "        bytes2human(available_memory),\n",
    "        available_memory_percentage))\n",
    "    return available_memory_percentage\n",
    "\n",
    "def run_many_models_in_parallel(configs_to_fit):\n",
    "    max_processes_for_user = int(multiprocessing.cpu_count() / 1.2)\n",
    "    print(\"Maximum number of processes to run: %i\" % max_processes_for_user)\n",
    "    # for config_idx in range(len(configs_to_fit) - 0):  # Huan\n",
    "    for config_idx in range(len(configs_to_fit)):\n",
    "        t0 = time.time()\n",
    "        # Check how many processes user is running.\n",
    "        n_processes_running = int(subprocess.check_output('ps -fA | grep model_experiments.py | wc -l', shell=True))\n",
    "        print(\"Current processes running for user: %i\" % n_processes_running)\n",
    "        while n_processes_running > max_processes_for_user:\n",
    "            print(\"Current processes are %i, above threshold of %i; waiting.\" % (n_processes_running, max_processes_for_user))\n",
    "            time.sleep(SECONDS_TO_WAIT_AFTER_EXCEEDING_COMP_THRESHOLD)\n",
    "            n_processes_running = int(subprocess.check_output('ps -fA | grep model_experiments.py | wc -l', shell=True))\n",
    "\n",
    "        # don't swamp cluster. Check CPU usage.\n",
    "        cpu_usage = psutil.cpu_percent()\n",
    "        print(\"Current CPU usage: %2.3f%%\" % cpu_usage)\n",
    "        while cpu_usage > CPU_USAGE_THRESHOLD:\n",
    "            print(\"Current CPU usage is %2.3f, above threshold of %2.3f; waiting.\" % (cpu_usage, CPU_USAGE_THRESHOLD))\n",
    "            time.sleep(SECONDS_TO_WAIT_AFTER_EXCEEDING_COMP_THRESHOLD)\n",
    "            cpu_usage = psutil.cpu_percent()\n",
    "\n",
    "        # Also check memory.\n",
    "        available_memory_percentage = check_memory_usage()\n",
    "        while available_memory_percentage < 100 - MEM_USAGE_THRESHOLD:\n",
    "            print(\"Current memory usage is above threshold of %2.3f; waiting.\" % (MEM_USAGE_THRESHOLD))\n",
    "            time.sleep(SECONDS_TO_WAIT_AFTER_EXCEEDING_COMP_THRESHOLD)\n",
    "            available_memory_percentage = check_memory_usage()\n",
    "\n",
    "        # If we pass these checks, start a job.\n",
    "        timestring = str(datetime.datetime.now()).replace(' ', '_').replace('-', '_').replace('.', '_').replace(':', '_')\n",
    "\n",
    "        experiment_to_run = configs_to_fit[config_idx]['experiment_to_run']\n",
    "        timestring = f'{timestring}_{experiment_to_run}'\n",
    "        print(\"Starting job %i/%i\" % (config_idx + 1, len(configs_to_fit)))\n",
    "        outfile_path = os.path.join(FITTED_MODEL_DIR, 'model_fitting_logfiles/%s.out' % timestring)\n",
    "        # cmd = 'python model_experiments.py fit_and_save_one_model %s --timestring %s --config_idx %i > %s 2>&1 &' % (experiment_to_run, timestring, config_idx, outfile_path)\n",
    "        # Huan\n",
    "        # fit_and_save_one_model(timestring=timestring, model_kwargs=model_kwargs,   data_kwargs=data_kwargs, outfile_path)\n",
    "        # cmd = 'nohup python -u model_experiments.py fit_and_save_one_model %s --timestring %s --config_idx %i > %s 2>&1 &' % (experiment_to_run, timestring, config_idx, outfile_path)\n",
    "        cmd = 'python model_experiments.py fit_and_save_one_model %s --timestring %s --config_idx %i > %s 2>&1 &' \\\n",
    "              % (experiment_to_run, timestring, config_idx, outfile_path)\n",
    "        print(\"Command: %s\" % cmd)\n",
    "        print(\"Current Python:\")\n",
    "        # os.system('source activate /media/gpu/easystore/covid-mobility-tool/env_dir/')\n",
    "        os.system('which python')\n",
    "        os.system(cmd)\n",
    "        time.sleep(SECONDS_TO_WAIT_BETWEEN_JOBS)\n",
    "        print(\"Time between job submissions: %2.3f\" % (time.time() - t0))\n",
    "\n",
    "def get_config_as_json(data_and_model_config):\n",
    "    data_and_model_config = copy.deepcopy(data_and_model_config)\n",
    "    for k in data_and_model_config:\n",
    "        if type(data_and_model_config[k]) is dict:\n",
    "            for k1 in data_and_model_config[k]:\n",
    "                data_and_model_config[k][k1] = str(data_and_model_config[k][k1])\n",
    "        else:\n",
    "            data_and_model_config[k] = str(data_and_model_config[k])\n",
    "    return json.dumps(data_and_model_config, indent=4, sort_keys=True)\n",
    "\n",
    "def print_config_as_json(data_and_model_config):\n",
    "    print(get_config_as_json(data_and_model_config))\n",
    "\n",
    "def partition_jobs_across_computers(computer_name, configs_to_fit):\n",
    "    computer_name = computer_name.replace('.stanford.edu', '')\n",
    "    username = getpass.getuser()\n",
    "    if username in USERNAME2COMPUTERS:\n",
    "        computers_to_use = USERNAME2COMPUTERS[username]\n",
    "    else:\n",
    "        #computers_to_use = ['rambo']\n",
    "        computers_to_use = ['Deep']  # Huan\n",
    "    #computer_stats = {'rambo':288, 'trinity':144, 'furiosa':144, 'madmax':64, 'madmax2':80,  'madmax3':80, 'madmax4':80, 'madmax5':80,  'madmax6':80,  'madmax7':80}\n",
    "    computer_stats = {'Deep':12}  # HUan\n",
    "\n",
    "    total_cores = sum([computer_stats[a] for a in computers_to_use])\n",
    "    computer_loads = dict([(k, computer_stats[k]/total_cores) for k in computers_to_use])\n",
    "    print('Partitioning up jobs among computers as follows', computer_loads)\n",
    "    assert computer_name in computer_loads\n",
    "    assert np.allclose(sum(computer_loads.values()), 1)\n",
    "    start_idx = 0\n",
    "    computers_to_configs = {}\n",
    "    for computer_idx, computer in enumerate(sorted(computer_loads.keys())):\n",
    "        if computer_idx == len(computer_loads) - 1:\n",
    "            computers_to_configs[computer] = configs_to_fit[start_idx:]\n",
    "        else:\n",
    "            end_idx = start_idx + int(len(configs_to_fit) * computer_loads[computer])\n",
    "            computers_to_configs[computer] = configs_to_fit[start_idx:end_idx]\n",
    "            start_idx = end_idx\n",
    "    assert sum([len(a) for a in computers_to_configs.values()]) == len(configs_to_fit)\n",
    "    print(\"Assigning %i configs to %s\" % (len(computers_to_configs[computer_name]), computer_name))\n",
    "    return computers_to_configs[computer_name]\n",
    "\n",
    "def print_failed_logs(min_timestring, max_timestring=None):\n",
    "    log_dir = os.path.join(FITTED_MODEL_DIR, 'model_fitting_logfiles')\n",
    "    fns = os.listdir(log_dir)\n",
    "    total = 0\n",
    "    success = 0\n",
    "    failed_fns = []\n",
    "    for fn in fns:\n",
    "        if fn >= min_timestring:\n",
    "            if max_timestring is None or fn <= max_timestring:\n",
    "                total += 1\n",
    "                full_fn = os.path.join(log_dir, fn)\n",
    "                content = open(full_fn, 'r').readlines()\n",
    "                if content[-1].startswith('Successfully fitted and saved model'):\n",
    "                    success += 1\n",
    "                else:\n",
    "                    failed_fns.append(fn)\n",
    "    print('Found %d timestrings in total, succeeded on %d' % (total, success))\n",
    "    return failed_fns\n",
    "    \n",
    "#########################################################\n",
    "# Functions to evaluate model fit and basic results\n",
    "#########################################################\n",
    "def plot_slir_over_time(mdl,\n",
    "    ax,\n",
    "    plot_logarithmic=True,\n",
    "    timesteps_to_plot=None,\n",
    "    groups_to_plot=None,\n",
    "    lines_to_plot=None,\n",
    "    title=None):\n",
    "    \"\"\"\n",
    "    Plot SLIR fractions over time.\n",
    "    \"\"\"\n",
    "    if groups_to_plot is None:\n",
    "        groups_to_plot = ['all']\n",
    "    history = copy.deepcopy(mdl.history)\n",
    "    for group in history.keys():\n",
    "        history[group]['L+I+R'] = history[group]['latent'] + history[group]['infected'] + history[group]['removed']\n",
    "\n",
    "    if lines_to_plot is None:\n",
    "        lines_to_plot = ['susceptible', 'latent', 'infected', 'removed']\n",
    "\n",
    "    linestyles = ['-', '--', '-.', ':']\n",
    "    colors = ['black', 'orange', 'blue', 'green', 'red']\n",
    "    lines_to_return = {}\n",
    "\n",
    "    for line_idx, k in enumerate(lines_to_plot):\n",
    "        for group_idx, group in enumerate(groups_to_plot):\n",
    "            total_population = history[group]['total_pop']\n",
    "            time_in_days = np.arange(history[group][k].shape[1]) / 24.\n",
    "            x = time_in_days\n",
    "            y = (history[group][k].T / total_population).T\n",
    "            assert y.shape[1] == x.shape[0]\n",
    "            mean_Y, lower_CI_Y, upper_CI_Y = mean_and_CIs_of_timeseries_matrix(y)\n",
    "            assert len(mean_Y) == len(x)\n",
    "\n",
    "            color = colors[line_idx % len(colors)]\n",
    "            linestyle = linestyles[group_idx % len(linestyles)]\n",
    "            n_cbgs = history[group]['num_cbgs']\n",
    "            if timesteps_to_plot is not None:\n",
    "                x = x[:timesteps_to_plot]\n",
    "                mean_Y = mean_Y[:timesteps_to_plot]\n",
    "                lower_CI_Y = lower_CI_Y[:timesteps_to_plot]\n",
    "                upper_CI_Y = upper_CI_Y[:timesteps_to_plot]\n",
    "\n",
    "            states_to_legend_labels = {'latent':'E (exposed)',\n",
    "                                        'infected':'I (infectious)',\n",
    "                                        'removed':'R (removed)',\n",
    "                                        'susceptible':'S (susceptible)',\n",
    "                                        'L+I+R':'E+I+R'}\n",
    "            if group != 'all':\n",
    "                ax.plot(x, mean_Y, label='%s, %s' % (states_to_legend_labels[k], group), color=color, linestyle=linestyle)\n",
    "            else:\n",
    "                ax.plot(x, mean_Y, label='%s' % (states_to_legend_labels[k]), color=color, linestyle=linestyle)\n",
    "            ax.fill_between(x, lower_CI_Y, upper_CI_Y, color=color, alpha=.2)\n",
    "\n",
    "            if plot_logarithmic:\n",
    "                ax.set_yscale('log')\n",
    "\n",
    "            lines_to_return['%s, %s' % (k, group)] = mean_Y\n",
    "    ax.legend(fontsize=16) # Removed for now because we need to handle multiple labels\n",
    "    logarithmic_string = ' (logarithmic)' if plot_logarithmic else ''\n",
    "    ax.set_xlabel('Time (in days)', fontsize=16)\n",
    "    ax.set_ylabel(\"Fraction of population%s\" % logarithmic_string, fontsize=16)\n",
    "    ax.set_xticks(range(0, math.ceil(max(time_in_days)) + 1, 7))\n",
    "    plt.xlim(0, math.ceil(max(time_in_days)))\n",
    "    if plot_logarithmic:\n",
    "        ax.set_ylim([1e-6, 1])\n",
    "    else:\n",
    "        ax.set_ylim([-.01, 1])\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "    ax.grid(alpha=.5)\n",
    "    return lines_to_return\n",
    "\n",
    "def make_slir_plot_stratified_by_demographic_attribute(mdl, ax, attribute, median_or_decile,\n",
    "                                                       slir_lines_to_plot=None):\n",
    "    \"\"\"\n",
    "    Given a demographic attribute, plot SLIR curves for people above and below median\n",
    "    if median_or_decile = median, or top and bottom decile, if median_or_decile = decile.\n",
    "    \"\"\"\n",
    "    if slir_lines_to_plot is None:\n",
    "        slir_lines_to_plot = ['L+I+R']\n",
    "    assert attribute in ['p_black', 'p_white', 'median_household_income']\n",
    "\n",
    "    if median_or_decile not in ['median', 'decile', 'above_median_in_own_county']:\n",
    "        raise Exception(\"median_or_decile should be 'median' or 'decile' or 'above_median_in_own_county'\")\n",
    "    if median_or_decile == 'median':\n",
    "        groups_to_plot = [f'{attribute}_above_median', f'{attribute}_below_median']\n",
    "        title = 'above and below median for %s' % attribute\n",
    "    elif median_or_decile == 'decile':\n",
    "        groups_to_plot = [f'{attribute}_top_decile', f'{attribute}_bottom_decile']\n",
    "        title = 'top and bottom decile for %s' % attribute\n",
    "    elif median_or_decile == 'above_median_in_own_county':\n",
    "        groups_to_plot = [f'{attribute}_above_median_in_own_county', f'{attribute}_below_median_in_own_county']\n",
    "        title = 'above and below COUNTY median for %s' % attribute\n",
    "\n",
    "    if attribute != 'p_black':\n",
    "        groups_to_plot = groups_to_plot[::-1] # keep underserved population consistent. Should always be solid line (first line plotted)\n",
    "\n",
    "    lines_to_return = plot_slir_over_time(\n",
    "        mdl,\n",
    "        ax,\n",
    "        groups_to_plot=groups_to_plot,\n",
    "        lines_to_plot=slir_lines_to_plot,\n",
    "        title=title)\n",
    "    return lines_to_return\n",
    "\n",
    "def make_slir_race_ses_plot(mdl, path_to_save=None, title_string=None, slir_lines_to_plot=None):\n",
    "    \"\"\"\n",
    "    Plot SLIR curves stratified by race and SES.\n",
    "    Returns a dictionary which stores the values for each SLIR curve.\n",
    "    \"\"\"\n",
    "    all_results = {}\n",
    "    fig = plt.figure(figsize=[30, 20])\n",
    "    subplot_idx = 1\n",
    "    for demographic_attribute in ['p_black', 'p_white', 'median_household_income']:\n",
    "        for median_or_decile in ['median', 'decile', 'above_median_in_own_county']:\n",
    "            ax = fig.add_subplot(3, 3, subplot_idx)\n",
    "            results = make_slir_plot_stratified_by_demographic_attribute(\n",
    "                mdl=mdl,\n",
    "                ax=ax,\n",
    "                attribute=demographic_attribute,\n",
    "                median_or_decile=median_or_decile,\n",
    "                slir_lines_to_plot=slir_lines_to_plot)\n",
    "            for k in results:\n",
    "                assert k not in all_results\n",
    "                all_results[k] = results[k]\n",
    "            subplot_idx += 1\n",
    "    if title_string is not None:\n",
    "        fig.suptitle(title_string)\n",
    "    if path_to_save is not None:\n",
    "        fig.savefig(path_to_save)\n",
    "    else:\n",
    "        plt.show()\n",
    "    return all_results\n",
    "\n",
    "def match_msa_name_to_msas_in_acs_data(msa_name, acs_msas):\n",
    "    '''\n",
    "    Matches the MSA name from our annotated SafeGraph data to the\n",
    "    MSA name in the external datasource in MSA_COUNTY_MAPPING\n",
    "    '''\n",
    "    msa_pieces = msa_name.split('_')\n",
    "    query_states = set()\n",
    "    i = len(msa_pieces) - 1\n",
    "    while True:\n",
    "        piece = msa_pieces[i]\n",
    "        if len(piece) == 2 and piece.upper() == piece:\n",
    "            query_states.add(piece)\n",
    "            i -= 1\n",
    "        else:\n",
    "            break\n",
    "    query_cities = set(msa_pieces[:i+1])\n",
    "\n",
    "    for msa in acs_msas:\n",
    "        if ', ' in msa:\n",
    "            city_string, state_string = msa.split(', ')\n",
    "            states = set(state_string.split('-'))\n",
    "            if states == query_states:\n",
    "                cities = city_string.split('-')\n",
    "                overlap = set(cities).intersection(query_cities)\n",
    "                if len(overlap) > 0:  # same states and at least one city matched\n",
    "                    return msa\n",
    "    return None\n",
    "\n",
    "def get_fips_codes_from_state_and_county_fp(state_vec, county_vec):\n",
    "    fips_codes = []\n",
    "    for state, county in zip(state_vec, county_vec):\n",
    "        state = str(state)\n",
    "        if len(state) == 1:\n",
    "            state = '0' + state\n",
    "        county = str(county)\n",
    "        if len(county) == 1:\n",
    "            county = '00' + county\n",
    "        elif len(county) == 2:\n",
    "            county = '0' + county\n",
    "        fips_codes.append(np.int64(state + county))\n",
    "    return fips_codes\n",
    "\n",
    "def get_nyt_outcomes_over_counties(counties=None):\n",
    "    outcomes = pd.read_csv(r'H:\\extra_safegraph_aggregate_models\\all_aggregate_data\\us-counties.csv')\n",
    "    outcomes['fips'] = outcomes.fillna(0)['fips'].astype(float).astype(int).astype(str).str.zfill(5)  # \"Huan\n",
    "    if counties is not None:\n",
    "        outcomes = outcomes[outcomes['fips'].isin(counties)]\n",
    "    return outcomes\n",
    "\n",
    "def get_datetimes_and_totals_from_nyt_outcomes(nyt_outcomes):\n",
    "    date_groups = nyt_outcomes.groupby('date').indices\n",
    "    dates = sorted(date_groups.keys())\n",
    "    datetimes = []\n",
    "    total_cases = []\n",
    "    total_deaths = []\n",
    "    for date in dates:\n",
    "        year, month, day = date.split('-')\n",
    "        curr_datetime = datetime.datetime(int(year), int(month), int(day))\n",
    "        if len(datetimes) > 0:\n",
    "            assert(curr_datetime > datetimes[-1])\n",
    "        datetimes.append(curr_datetime)\n",
    "        rows = nyt_outcomes.iloc[date_groups[date]]\n",
    "        total_cases.append(np.sum(rows['cases'].to_numpy()))\n",
    "        total_deaths.append(np.sum(rows['deaths'].to_numpy()))\n",
    "    return datetimes, np.array(total_cases), np.array(total_deaths)\n",
    "\n",
    "def find_model_and_real_overlap_for_eval(real_dates, real_cases, mdl_hours, mdl_cases,\n",
    "                                         compare_start_time=None, compare_end_time=None):\n",
    "    overlap = set(real_dates).intersection(set(mdl_hours))\n",
    "    if len(overlap) == 0:\n",
    "        raise Exception('No overlap in dates between real COVID counts and model predictions')\n",
    "    if compare_start_time is None:\n",
    "        compare_start_time = min(overlap)\n",
    "    if compare_end_time is None:\n",
    "        compare_end_time = max(overlap)\n",
    "    try:\n",
    "        comparable_period = helper.list_hours_in_range(compare_start_time, compare_end_time)\n",
    "        overlap = sorted(overlap.intersection(set(comparable_period)))\n",
    "        real_date2case = dict(zip(real_dates, real_cases))\n",
    "        mdl_date2case = dict(zip(mdl_hours, mdl_cases.T)) # mdl_cases has an extra random_seed first dim\n",
    "        real_vec = []\n",
    "        mdl_mat = np.zeros((len(mdl_cases), len(overlap)))  # num_seed x num_time\n",
    "        for idx, date in enumerate(overlap):\n",
    "            real_vec.append(real_date2case[date])\n",
    "            mdl_mat[:, idx] = mdl_date2case[date]\n",
    "        return np.array(real_vec), mdl_mat, overlap[0], overlap[-1]\n",
    "    except Exception as e:\n",
    "        print(\"Error in find_model_and_real_overlap_for_eval():\", e, mdl_hours, mdl_cases,\n",
    "                                         compare_start_time, compare_end_time)\n",
    "\n",
    "PATH_TO_ACS_5YR_DATA =r'H:\\covid_mobility_results\\new_census_data\\ACS_2019_5YR_BG\\ACS_race_cbsa_income_2019.csv   \n",
    "def get_variables_for_evaluating_msa_model(msa_name, verbose=False):\n",
    "    PATH_TO_ACS_5YR_DATA =r'H:\\covid_mobility_results\\new_census_data\\ACS_2019_5YR_BG\\ACS_race_cbsa_income_2019.csv   \n",
    "\n",
    "    acs_data = pd.read_csv(r'H:\\covid_mobility_results\\new_census_data\\ACS_2019_5YR_BG\\ACS_race_cbsa_income_2019.csv )\n",
    "    acs_msas = [msa for msa in acs_data['CBSA Title'].unique() if type(msa) == str]\n",
    "    msa_matches = []\n",
    "    for single_msa in msa_name.split('+'):  # may be combination of multiple MSAs\n",
    "        msa_match = match_msa_name_to_msas_in_acs_data(single_msa, acs_msas)\n",
    "        if msa_match is None:\n",
    "            raise Exception('Could not find ACS match for %s' % single_msa)\n",
    "        if verbose: \n",
    "            print('Found MSA %s in ACS 5-year data' % msa_match)\n",
    "        msa_matches.append(msa_match)\n",
    "    msa_data = acs_data[acs_data['CBSA Title'].isin(msa_matches)].copy()\n",
    "    msa_data['id_to_match_to_safegraph_data'] = msa_data['GEOID'].map(lambda x:x.split(\"US\")[1]).astype(np.int64)\n",
    "    msa_data['id_to_match_to_safegraph_data'] = msa_data['id_to_match_to_safegraph_data'].astype(str).str.zfill(12)\n",
    "    msa_cbgs = msa_data['id_to_match_to_safegraph_data'].values\n",
    "    msa_data['fips'] = get_fips_codes_from_state_and_county_fp(msa_data.STATEFP, msa_data.COUNTYFP)\n",
    "    msa_data['fips'] = msa_data['fips'].astype(str)\n",
    "    msa_counties = list(set(msa_data['fips'].values))\n",
    "    if verbose:\n",
    "        print('Found %d counties and %d CBGs in MSA' % (len(msa_counties), len(msa_cbgs)))\n",
    "    nyt_outcomes = get_nyt_outcomes_over_counties(msa_counties)\n",
    "    nyt_counties = set(nyt_outcomes.fips.unique())\n",
    "    nyt_cbgs = msa_data[msa_data['fips'].isin(nyt_counties)]['id_to_match_to_safegraph_data'].values\n",
    "    if verbose:\n",
    "        print('Found NYT data matching %d counties and %d CBGs' % (len(nyt_counties), len(nyt_cbgs)))\n",
    "    return nyt_outcomes, nyt_counties, nyt_cbgs, msa_counties, msa_cbgs\n",
    "\n",
    "def resave_fast_to_load_results_for_timestring(ts, old_directory, nyt_outcomes, expect_same=True):\n",
    "    \"\"\"\n",
    "    Overwrite old loss if we want to add additional features.\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    model, kwargs, _, model_results, fast_to_load_results = load_model_and_data_from_timestring(\n",
    "         ts,\n",
    "         verbose=False,\n",
    "         load_fast_results_only=False,\n",
    "         load_full_model=True,\n",
    "         old_directory=old_directory)\n",
    "    model_kwargs = kwargs['model_kwargs']\n",
    "    data_kwargs = kwargs['data_kwargs']\n",
    "    train_test_partition = fast_to_load_results['train_test_date_cutoff']\n",
    "    keys_to_rewrite = ['loss_dict', 'train_loss_dict', 'test_loss_dict']\n",
    "\n",
    "\n",
    "    for key_to_rewrite in keys_to_rewrite:\n",
    "        old_loss_dict = None\n",
    "        new_loss_dict = None\n",
    "        old_loss_dict = fast_to_load_results[key_to_rewrite]\n",
    "        if key_to_rewrite == 'loss_dict':\n",
    "            new_loss_dict = compare_model_vs_real_num_cases(nyt_outcomes,\n",
    "                                               model_kwargs['min_datetime'],\n",
    "                                               model=model,\n",
    "                                               make_plot=False)\n",
    "        elif key_to_rewrite == 'train_loss_dict':\n",
    "            train_max = train_test_partition + datetime.timedelta(hours=-1)\n",
    "            new_loss_dict = compare_model_vs_real_num_cases(nyt_outcomes,\n",
    "                                           model_kwargs['min_datetime'],\n",
    "                                           compare_end_time = train_max,\n",
    "                                           model=model)\n",
    "        elif key_to_rewrite == 'test_loss_dict':\n",
    "            new_loss_dict = compare_model_vs_real_num_cases(nyt_outcomes,\n",
    "                                           model_kwargs['min_datetime'],\n",
    "                                           compare_start_time = train_test_partition,\n",
    "                                           model=model)\n",
    "\n",
    "        common_keys = [a for a in new_loss_dict.keys() if a in old_loss_dict.keys()]\n",
    "        assert len(common_keys) > 0\n",
    "        if expect_same:  # we expect values of same keys to stay the same\n",
    "            for k in common_keys:\n",
    "                if type(new_loss_dict[k]) is not np.ndarray:\n",
    "                    assert new_loss_dict[k] == old_loss_dict[k]\n",
    "                else:\n",
    "                    assert np.allclose(new_loss_dict[k], old_loss_dict[k])\n",
    "\n",
    "        fast_to_load_results[key_to_rewrite] = new_loss_dict\n",
    "\n",
    "    if old_directory:\n",
    "        model_dir = OLD_FITTED_MODEL_DIR\n",
    "    else:\n",
    "        model_dir = FITTED_MODEL_DIR\n",
    "    path_to_save = os.path.join(model_dir, 'fast_to_load_results_only', 'fast_to_load_results_%s.pkl' % ts)\n",
    "    assert os.path.exists(path_to_save)\n",
    "    file = open(path_to_save, 'wb')\n",
    "    pickle.dump(fast_to_load_results, file)\n",
    "    file.close()\n",
    "    print(\"Time to save model: %2.3f seconds\" % (time.time() - t0))    \n",
    "\n",
    "def list_datetimes_in_range(min_day, max_day):\n",
    "    \"\"\"\n",
    "    Return a list of datetimes in a range from min_day to max_day, inclusive. Increment is one day. \n",
    "    \"\"\"\n",
    "    assert(min_day <= max_day)\n",
    "    days = []\n",
    "    while min_day <= max_day:\n",
    "        days.append(min_day)\n",
    "        min_day = min_day + datetime.timedelta(days=1)\n",
    "    return days \n",
    "\n",
    "def list_hours_in_range(min_hour, max_hour):\n",
    "    \"\"\"\n",
    "    Return a list of datetimes in a range from min_hour to max_hour, inclusive. Increment is one hour. \n",
    "    \"\"\"\n",
    "    assert(min_hour <= max_hour), f'min_hour: {min_hour}, max_hour: {max_hour}'\n",
    "    hours = []\n",
    "    while min_hour <= max_hour:\n",
    "        hours.append(min_hour)\n",
    "        min_hour = min_hour + datetime.timedelta(hours=1)\n",
    " \n",
    "                           return hours    \n",
    "def compare_model_vs_real_num_cases(nyt_outcomes,\n",
    "                                    mdl_start_time,\n",
    "                                    compare_start_time=None,\n",
    "                                    compare_end_time=None,\n",
    "                                    model=None,\n",
    "                                    model_results=None,\n",
    "                                    mdl_prediction=None,\n",
    "                                    projected_hrs=None,\n",
    "                                    detection_rate=DETECTION_RATE,\n",
    "                                    detection_lag=DETECTION_LAG,\n",
    "                                    death_rate=DEATH_RATE,\n",
    "                                    death_lag=DEATH_LAG,\n",
    "                                    prediction_mode='deterministic',\n",
    "                                    verbose=False,\n",
    "                                    make_plot=False,\n",
    "                                    ax=None,\n",
    "                                    title=None,\n",
    "                                    plot_log=False,\n",
    "                                    plot_mode='cases',\n",
    "                                    plot_errorbars=True,\n",
    "                                    plot_real_data=True,\n",
    "                                    plot_daily_not_cumulative=False,\n",
    "                                    only_plot_intersection=True,\n",
    "                                    model_line_label=None,\n",
    "                                    true_line_label=None,\n",
    "                                    x_interval=None,\n",
    "                                    add_smoothed_real_data_line=True,\n",
    "                                    title_fontsize=20,\n",
    "                                    legend_fontsize=16,\n",
    "                                    tick_label_fontsize=16,\n",
    "                                    marker_size=5,\n",
    "                                    plot_legend=True,\n",
    "                                    real_data_color='black',\n",
    "                                    model_color='tab:blue',\n",
    "                                    xticks=None,\n",
    "                                    x_range=None,\n",
    "                                    y_range=None,\n",
    "                                    only_two_yticks=False,\n",
    "                                    return_mdl_pred_and_hours=False):\n",
    "    assert plot_daily_not_cumulative in [True, False]\n",
    "    assert prediction_mode in {'deterministic', 'exponential', 'gamma', 'model_history'}\n",
    "    if model is not None:\n",
    "        cbgs_to_idxs = model.CBGS_TO_IDXS\n",
    "        history = model.history\n",
    "        assert('nyt' in history)\n",
    "        assert model_results is None\n",
    "        assert mdl_prediction is None\n",
    "        assert projected_hrs is None\n",
    "    elif model_results is not None:\n",
    "        cbgs_to_idxs = model_results['CBGS_TO_IDXS']\n",
    "        history = model_results['history']\n",
    "        assert('nyt' in history)\n",
    "        assert mdl_prediction is None\n",
    "        assert projected_hrs is None\n",
    "    else:\n",
    "        assert mdl_prediction is not None\n",
    "        assert projected_hrs is not None\n",
    "\n",
    "\n",
    "    real_dates, real_cases, real_deaths = get_datetimes_and_totals_from_nyt_outcomes(nyt_outcomes)\n",
    "    score_dict = {}\n",
    "\n",
    "    if mdl_prediction is not None:\n",
    "        mdl_prediction_provided = True\n",
    "    else:\n",
    "        mdl_prediction_provided = False\n",
    "\n",
    "    if not mdl_prediction_provided:\n",
    "        # align cases with datetimes\n",
    "        mdl_IR = (history['nyt']['infected'] + history['nyt']['removed'])\n",
    "        # should think of this as a cumulative count because once you enter the removed state, you never leave.\n",
    "        # So mdl_cases is the number of people who have _ever_ been infectious or removed (ie, in states I or R).\n",
    "        num_hours = mdl_IR.shape[1]\n",
    "        mdl_end_time = mdl_start_time + datetime.timedelta(hours=num_hours-1)\n",
    "        mdl_hours = list_hours_in_range(mdl_start_time, mdl_end_time)\n",
    "        mdl_dates = list_datetimes_in_range(mdl_start_time, mdl_end_time)\n",
    "        assert(mdl_start_time < mdl_end_time)\n",
    "    else:\n",
    "        mdl_IR = None\n",
    "\n",
    "    modes = ['cases', 'deaths']\n",
    "\n",
    "    for mode in modes:\n",
    "\n",
    "        if mode == 'cases':\n",
    "            real_data = real_cases\n",
    "        else:\n",
    "            real_data = real_deaths\n",
    "\n",
    "        if not mdl_prediction_provided:\n",
    "            # note: mdl_prediction should always represent an hourly *cumulative* count per seed x hour\n",
    "            if mode == 'cases':\n",
    "                min_thresholds = [1, 10, 20, 50, 100]\n",
    "                # don't evaluate LL on very small numbers -- too noisy\n",
    "\n",
    "                if prediction_mode == 'deterministic':  # assume constant detection rate and delay\n",
    "                    mdl_prediction = mdl_IR * detection_rate\n",
    "                    projected_hrs = [hr + datetime.timedelta(days=detection_lag) for hr in mdl_hours]\n",
    "                elif prediction_mode == 'exponential':  # draw delays from exponential distribution\n",
    "                    mdl_hourly_new_cases, _ = draw_cases_and_deaths_from_exponential_distribution(mdl_IR,\n",
    "                                                detection_rate, detection_lag, death_rate, death_lag)\n",
    "                    mdl_prediction = get_cumulative(mdl_hourly_new_cases)\n",
    "                    projected_hrs = mdl_hours\n",
    "                elif prediction_mode == 'gamma':  # draw delays from gamma distribution\n",
    "                    mdl_hourly_new_cases, _ = draw_cases_and_deaths_from_gamma_distribution(mdl_IR,\n",
    "                                                detection_rate, death_rate)\n",
    "                    mdl_prediction = get_cumulative(mdl_hourly_new_cases)\n",
    "                    projected_hrs = mdl_hours\n",
    "                else:  # modeled confirmed cases during simulation\n",
    "                    assert 'new_confirmed_cases' in history['nyt']\n",
    "                    mdl_hourly_new_cases = history['nyt']['new_confirmed_cases']\n",
    "                    mdl_prediction = get_cumulative(mdl_hourly_new_cases)\n",
    "                    projected_hrs = mdl_hours\n",
    "            else:\n",
    "                min_thresholds = [1, 2, 3, 5, 10]  # don't evaluate LL on very small numbers -- too noisy\n",
    "                if prediction_mode == 'deterministic':  # assume constant detection rate and delay\n",
    "                    mdl_prediction = mdl_IR * death_rate\n",
    "                    projected_hrs = [hr + datetime.timedelta(days=death_lag) for hr in mdl_hours]\n",
    "                elif prediction_mode == 'exponential':  # draw delays from exponential distribution\n",
    "                    _, mdl_hourly_new_deaths = draw_cases_and_deaths_from_exponential_distribution(mdl_IR,\n",
    "                                                detection_rate, detection_lag, death_rate, death_lag)\n",
    "                    mdl_prediction = get_cumulative(mdl_hourly_new_deaths)\n",
    "                    projected_hrs = mdl_hours\n",
    "                elif prediction_mode == 'gamma':  # draw delays from gamma distribution\n",
    "                    _, mdl_hourly_new_deaths = draw_cases_and_deaths_from_gamma_distribution(mdl_IR,\n",
    "                                                detection_rate, death_rate)\n",
    "                    mdl_prediction = get_cumulative(mdl_hourly_new_deaths)\n",
    "                    projected_hrs = mdl_hours\n",
    "                else:  # modeled confirmed deaths during simulation\n",
    "                    assert 'new_confirmed_cases' in history['nyt']\n",
    "                    mdl_hourly_new_deaths = history['nyt']['new_deaths']\n",
    "                    mdl_prediction = get_cumulative(mdl_hourly_new_deaths)\n",
    "                    projected_hrs = mdl_hours\n",
    "\n",
    "            if not make_plot:\n",
    "                # note: y_pred is also cumulative, but represents seed x day, instead of hour\n",
    "                y_true, y_pred, eval_start, eval_end = find_model_and_real_overlap_for_eval(\n",
    "                    real_dates, real_data, projected_hrs, mdl_prediction, compare_start_time, compare_end_time)\n",
    "                if len(y_true) < 5:\n",
    "                    print(\"Fewer than 5 days of overlap between model predictions and observed %s data; not scoring\" % mode)\n",
    "                else:\n",
    "                    score_dict['eval_start_time_%s' % mode] = eval_start\n",
    "                    score_dict['eval_end_time_%s' % mode] = eval_end\n",
    "                    score_dict['cumulative_predicted_%s' % mode] = y_pred\n",
    "                    score_dict['cumulative_true_%s' % mode] = y_true\n",
    "                    score_dict['cumulative_%s_RMSE' % mode] = compute_loss(y_true, y_pred, metric='RMSE', min_threshold=None, compare_daily_not_cumulative=False)\n",
    "                    score_dict['cumulative_%s_MSE' % mode] = compute_loss(y_true, y_pred, metric='MSE', min_threshold=None, compare_daily_not_cumulative=False)\n",
    "                    score_dict['daily_%s_RMSE' % mode] = compute_loss(y_true, y_pred, metric='RMSE', min_threshold=None, compare_daily_not_cumulative=True)\n",
    "                    score_dict['daily_%s_MSE' % mode] = compute_loss(y_true, y_pred, metric='MSE', min_threshold=None, compare_daily_not_cumulative=True)\n",
    "                    \n",
    "                    if mode == 'cases':  # special case to test time-varying case detection rate\n",
    "                        y_pred_IR = y_pred / detection_rate  # convert back to cumulative IR\n",
    "                        daily_IR = get_daily_from_cumulative(y_pred_IR) \n",
    "                        eval_start_infectious = eval_start + datetime.timedelta(days=-detection_lag)\n",
    "                        eval_end_infectious = eval_end + datetime.timedelta(days=-detection_lag)\n",
    "                        _, daily_cdr = helper.get_daily_case_detection_rate(min_datetime=eval_start_infectious, max_datetime=eval_end_infectious)\n",
    "                        assert len(daily_cdr) == daily_IR.shape[1]\n",
    "                        daily_y_pred = (daily_IR * daily_cdr)[:, 1:]\n",
    "                        \n",
    "                        daily_real_data = get_daily_from_cumulative(real_data)\n",
    "                        # want to smooth on original timeseries so we get smoothing on both ends of evaluated timeseries\n",
    "                        smoothed_daily_real_data = apply_smoothing(daily_real_data, before=7, after=7)\n",
    "                        new_eval_start = eval_start + datetime.timedelta(days=1)\n",
    "                        assert new_eval_start in real_dates\n",
    "                        daily_y_true = daily_real_data[real_dates.index(new_eval_start):real_dates.index(eval_end)+1]\n",
    "                        assert len(daily_y_true) == daily_y_pred.shape[1]\n",
    "                        score_dict['daily_cases_RMSE_time_varying_cdr'] = compute_loss(daily_y_true, daily_y_pred, metric='RMSE', min_threshold=None, compare_daily_not_cumulative=False)  # don't need to convert to daily bc already daily\n",
    "                        smoothed_daily_y_true = smoothed_daily_real_data[real_dates.index(new_eval_start):real_dates.index(eval_end)+1]  # compute loss on smoothed data\n",
    "                        assert len(smoothed_daily_y_true) == daily_y_pred.shape[1]\n",
    "                        score_dict['smoothed_daily_cases_RMSE_time_varying_cdr'] = compute_loss(smoothed_daily_y_true, daily_y_pred, metric='RMSE', min_threshold=None, compare_daily_not_cumulative=False)\n",
    "                        \n",
    "                    if prediction_mode == 'deterministic':  # LL metrics assume constant delay and rate for predictions\n",
    "                        threshold_metrics = [\n",
    "                            'MRE',\n",
    "                            'poisson_NLL']\n",
    "                        rate = detection_rate if mode == 'cases' else death_rate\n",
    "                        for threshold_metric in threshold_metrics:\n",
    "                            for min_threshold in min_thresholds:\n",
    "                                for do_logsumexp in [True, False]:\n",
    "                                    if do_logsumexp:\n",
    "                                        agg_str = 'logsumexp'\n",
    "                                    else:\n",
    "                                        agg_str = 'sum'\n",
    "\n",
    "                                    # Skip logsumexp for MRE since it has no LL interpretation\n",
    "                                    if threshold_metric == 'MRE' and do_logsumexp:\n",
    "                                        continue\n",
    "\n",
    "                                    dict_str = f'daily_{mode}_{threshold_metric}_thres-{min_threshold}_{agg_str}'\n",
    "                                    score_dict[dict_str] = compute_loss(\n",
    "                                        y_true=y_true,\n",
    "                                        y_pred=y_pred,\n",
    "                                        rate=rate,\n",
    "                                        metric=threshold_metric,\n",
    "                                        min_threshold=min_threshold,\n",
    "                                        compare_daily_not_cumulative=True,\n",
    "                                        do_logsumexp=do_logsumexp)\n",
    "\n",
    "        if return_mdl_pred_and_hours and plot_mode == mode:\n",
    "            return mdl_prediction, projected_hrs\n",
    "\n",
    "        if make_plot and plot_mode == mode:\n",
    "            assert(ax is not None and title is not None)\n",
    "            if plot_daily_not_cumulative:\n",
    "                new_projected_hrs = []\n",
    "                new_mdl_prediction = []\n",
    "                for hr, prediction in zip(projected_hrs, mdl_prediction.T):\n",
    "                    if hr.hour == 0:\n",
    "                        new_projected_hrs.append(hr)\n",
    "                        new_mdl_prediction.append(prediction)\n",
    "                # truncate the first day bc it could be an accumulation from multiple days, \n",
    "                # not the number of new cases on this day\n",
    "                projected_hrs = new_projected_hrs[1:]\n",
    "                mdl_prediction = np.array(new_mdl_prediction).T\n",
    "                mdl_prediction = get_daily_from_cumulative(mdl_prediction)[:, 1:]\n",
    "                assert len(projected_hrs) == mdl_prediction.shape[1]\n",
    "                \n",
    "                if mode == 'cases':  # special case for time-varying case detection rate\n",
    "                    print('Applying time-varying CDR to visualization')\n",
    "                    daily_IR = mdl_prediction / detection_rate  # convert back to daily IR\n",
    "                    eval_start_infectious = projected_hrs[0] + datetime.timedelta(days=-detection_lag)\n",
    "                    eval_end_infectious = projected_hrs[-1] + datetime.timedelta(days=-detection_lag)\n",
    "                    _, daily_cdr = helper.get_daily_case_detection_rate(min_datetime=eval_start_infectious, max_datetime=eval_end_infectious)\n",
    "                    assert len(daily_cdr) == daily_IR.shape[1]\n",
    "                    mdl_prediction = daily_IR * daily_cdr\n",
    "                real_data = get_daily_from_cumulative(real_data)\n",
    "\n",
    "            score_dict['pred_dates'] = projected_hrs\n",
    "            score_dict['pred_data'] = mdl_prediction\n",
    "            num_seeds, _ = mdl_prediction.shape\n",
    "            if num_seeds > 1:\n",
    "                mean, lower_CI, upper_CI = mean_and_CIs_of_timeseries_matrix(mdl_prediction)\n",
    "                model_max = max(upper_CI)\n",
    "                if plot_errorbars:\n",
    "                    ax.fill_between(projected_hrs, lower_CI, upper_CI, alpha=.5, color=model_color)\n",
    "            else:\n",
    "                mean = mdl_prediction[0]\n",
    "                model_max = max(mean)\n",
    "            real_max = max(real_data)\n",
    "            daily_or_cumulative_string = 'daily' if plot_daily_not_cumulative else 'cumulative'\n",
    "            if model_line_label is None:\n",
    "                model_line_label = 'modeled %s %s' % (daily_or_cumulative_string, mode)\n",
    "            if true_line_label is None:\n",
    "                true_line_label = 'true %s %s' % (daily_or_cumulative_string, mode)\n",
    "            ax.plot_date(projected_hrs, mean, linestyle='-', label=model_line_label, c=model_color,\n",
    "                         markersize=marker_size)\n",
    "                \n",
    "            score_dict['real_dates'] = real_dates\n",
    "            score_dict['real_data'] = real_data\n",
    "            if plot_real_data:         \n",
    "                if add_smoothed_real_data_line:  # plot non-smoothed as grey x's and smoothed as real_data_color line      \n",
    "                    smoothed_real_data = apply_smoothing(real_data, before=7, after=7)\n",
    "                    ax.plot_date(real_dates, smoothed_real_data, linestyle='-',\n",
    "                                 label='%s (smoothed)' % true_line_label, c=real_data_color, markersize=marker_size)\n",
    "                    score_dict['smoothed_real_data'] = smoothed_real_data\n",
    "                    ax.plot_date(real_dates, real_data, marker='x', c='grey', alpha=0.8,\n",
    "                              markersize=marker_size+1, markeredgewidth=2, label='%s (raw)' % true_line_label)                 \n",
    "                else:  \n",
    "                    if plot_daily_not_cumulative:  # plot non-smoothed as real_data_color x's\n",
    "                        ax.plot_date(real_dates, real_data, label=true_line_label, marker='x', c=real_data_color, markersize=marker_size+1, markeredgewidth=2)\n",
    "                    else:\n",
    "                        ax.plot_date(real_dates, real_data, label=true_line_label, linestyle='-', c=real_data_color, markersize=marker_size)\n",
    "\n",
    "            interval = int(len(real_dates) / 6)\n",
    "            ax.xaxis.set_major_locator(mdates.DayLocator(interval=interval))\n",
    "            ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))\n",
    "            if only_plot_intersection:\n",
    "                left = max(min(projected_hrs), min(real_dates))\n",
    "                right = min(max(projected_hrs), max(real_dates))\n",
    "                ax.set_xlim([left, right])  # only plot place where both lines intersect.\n",
    "                model_max_idx = projected_hrs.index(right)\n",
    "                if num_seeds > 1:\n",
    "                    model_max = max(upper_CI[:model_max_idx+1])\n",
    "                else:\n",
    "                    model_max = max(mean[:model_max_idx+1])\n",
    "                for real_max_idx, date in enumerate(real_dates):\n",
    "                    if date > right:\n",
    "                        break\n",
    "                real_max_idx -= 1  # real_max_idx is the last index in timeseries to be plotted\n",
    "                real_max = max(real_data[:real_max_idx+1])\n",
    "\n",
    "            if plot_log:\n",
    "                ax.set_yscale('log')\n",
    "                ax.set_ylim([1, max(model_max, real_max)])\n",
    "            else:\n",
    "                ax.set_ylim([0, max(model_max, real_max)])\n",
    "\n",
    "            if plot_legend:\n",
    "                ax.legend(fontsize=legend_fontsize, loc='upper left')\n",
    "\n",
    "            if xticks is None:\n",
    "                if x_interval is None:\n",
    "                    x_interval = int(len(real_dates) / 6)\n",
    "                ax.xaxis.set_major_locator(mdates.DayLocator(interval=x_interval))\n",
    "            else:\n",
    "                ax.set_xticks(xticks)\n",
    "\n",
    "            ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))\n",
    "            ax.tick_params(labelsize=tick_label_fontsize)\n",
    "            if y_range is not None:\n",
    "                ax.set_ylim(*y_range)\n",
    "            if x_range is not None:\n",
    "                ax.set_xlim(*x_range)\n",
    "\n",
    "            if only_two_yticks:\n",
    "\n",
    "                bot, top = ax.get_ylim()\n",
    "                if plot_mode == 'cases':\n",
    "                    # Round to nearest hundred\n",
    "                    top = (top // 100) * 100\n",
    "                elif plot_mode == 'deaths':\n",
    "                    # Round to nearest 20\n",
    "                    top = (top // 20) * 20\n",
    "                ax.set_yticks([bot, top])\n",
    "\n",
    "            if plot_mode == 'cases':\n",
    "                if ax.get_ylim()[1] > 5000:\n",
    "                    ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '0' if x == 0 else '{:.1f}'.format(x/1000) + 'k'))\n",
    "\n",
    "            ax.grid(alpha=.5)\n",
    "            ax.set_title(title, fontsize=title_fontsize)\n",
    "\n",
    "    return score_dict\n",
    "\n",
    "def compare_model_vs_real_num_cases_per_county(nyt_outcomes, model, mdl_start_time,\n",
    "                                               detection_rate=DETECTION_RATE,\n",
    "                                               detection_lag=DETECTION_LAG,\n",
    "                                               death_rate=DEATH_RATE,\n",
    "                                               death_lag=DEATH_LAG):\n",
    "    nyt_counties = nyt_outcomes.fips.unique()\n",
    "    history = model.history\n",
    "    mdl_hours = model.all_hours\n",
    "    county2scores = {}\n",
    "    county2preds = {}\n",
    "    for i, county in enumerate(nyt_counties):\n",
    "        county_outcomes = nyt_outcomes[nyt_outcomes.fips == county]\n",
    "        real_dates, real_cases, real_deaths = get_datetimes_and_totals_from_nyt_outcomes(county_outcomes)\n",
    "        county_name = county_outcomes.iloc[0]['county']\n",
    "        if county not in history:\n",
    "            print('Missing %s in model' % county_name)\n",
    "        else:\n",
    "            pop_size = history[county]['total_pop']\n",
    "            assert pop_size > 0\n",
    "            mdl_IR = history[county]['infected'] + history[county]['removed']  # cumulative, n_seeds x n_hours\n",
    "            assert mdl_IR.shape[1] == len(mdl_hours)\n",
    "\n",
    "            score_dict = {}\n",
    "            pred_dict = {}\n",
    "            for mode in ['cases', 'deaths']:\n",
    "                if mode == 'cases':\n",
    "                    real_data = real_cases\n",
    "                    mdl_prediction = mdl_IR * detection_rate\n",
    "                    projected_hrs = [hr + datetime.timedelta(days=detection_lag) for hr in mdl_hours]\n",
    "                else:\n",
    "                    real_data = real_deaths\n",
    "                    mdl_prediction = mdl_IR * death_rate\n",
    "                    projected_hrs = [hr + datetime.timedelta(days=death_lag) for hr in mdl_hours]\n",
    "                pred_dict[mode] = (projected_hrs, mdl_prediction, pop_size)\n",
    "                \n",
    "                y_true, y_pred, eval_start, eval_end = find_model_and_real_overlap_for_eval(\n",
    "                        real_dates, real_data, projected_hrs, mdl_prediction)\n",
    "                score_dict['eval_start_time_%s' % mode] = eval_start\n",
    "                score_dict['eval_end_time_%s' % mode] = eval_end\n",
    "                score_dict['true_cumulative_%s' % mode] = y_true[-1]\n",
    "                pred_mean = np.mean(y_pred[:, -1])\n",
    "                pred_lower = np.percentile(y_pred[:, -1], LOWER_PERCENTILE)\n",
    "                pred_upper = np.percentile(y_pred[:, -1], UPPER_PERCENTILE)\n",
    "                score_dict['pred_cumulative_%s' % mode] = (pred_mean, pred_lower, pred_upper)\n",
    "                score_dict['true_cumulative_%s_per_capita' % mode] = y_true[-1] / pop_size\n",
    "                score_dict['pred_cumulative_%s_per_capita' % mode] = (pred_mean / pop_size, pred_lower / pop_size, pred_upper / pop_size)\n",
    "                try:\n",
    "                    score_dict['daily_%s_RMSE' % mode] = compute_loss(y_true, y_pred, metric='RMSE', min_threshold=None, compare_daily_not_cumulative=True)\n",
    "                    score_dict['daily_%s_per_capita_RMSE' % mode] = compute_loss(y_true / pop_size, y_pred / pop_size, \n",
    "                                                                                 metric='RMSE', min_threshold=None, \n",
    "                                                                                 compare_daily_not_cumulative=True)\n",
    "                    \n",
    "                    if mode == 'cases':  # special case to test time-varying case detection rate\n",
    "                        y_pred_IR = y_pred / detection_rate  # convert back to cumulative IR\n",
    "                        daily_IR = get_daily_from_cumulative(y_pred_IR) \n",
    "                        eval_start_infectious = eval_start + datetime.timedelta(days=-detection_lag)\n",
    "                        eval_end_infectious = eval_end + datetime.timedelta(days=-detection_lag)\n",
    "                        _, daily_cdr = helper.get_daily_case_detection_rate(min_datetime=eval_start_infectious, max_datetime=eval_end_infectious)\n",
    "                        assert len(daily_cdr) == daily_IR.shape[1]\n",
    "                        daily_y_pred = (daily_IR * daily_cdr)[:, 1:]\n",
    "                        daily_y_true = get_daily_from_cumulative(y_true)[1:]\n",
    "                        assert len(daily_y_true) == daily_y_pred.shape[1]\n",
    "                        score_dict['daily_cases_RMSE_time_varying_cdr'] = compute_loss(daily_y_true, daily_y_pred, metric='RMSE', min_threshold=None, compare_daily_not_cumulative=False)  # don't need to convert to daily bc already daily\n",
    "                        score_dict['daily_cases_per_capita_RMSE_time_varying_cdr'] = compute_loss(daily_y_true / pop_size, daily_y_pred / pop_size, metric='RMSE', min_threshold=None, compare_daily_not_cumulative=False)\n",
    "                except:\n",
    "                    print('Failed to compute daily RMSE for %s' % county_name)\n",
    "            county2scores[county] = score_dict\n",
    "            county2preds[county] = pred_dict\n",
    "            \n",
    "    model_summary = {}\n",
    "    pop_sizes_per_county = []\n",
    "    for county in nyt_counties:\n",
    "        try:\n",
    "            pop_sizes_per_county.append(history[county]['total_pop'] )\n",
    "        except Exception as e:\n",
    "            print(e, county, \"missing, huan\")\n",
    "            # nyt_counties.remove(county)  # Huan\n",
    "\n",
    "    pop_sizes_per_county = np.array(pop_sizes_per_county)\n",
    "\n",
    "    for mode in ['daily_cases_RMSE', 'daily_cases_per_capita_RMSE', \n",
    "                 'daily_cases_RMSE_time_varying_cdr', 'daily_cases_per_capita_RMSE_time_varying_cdr',\n",
    "                 'daily_deaths_RMSE', 'daily_deaths_per_capita_RMSE']:\n",
    "        # results_per_county = np.array([county2scores[county][mode] for county in nyt_counties])\n",
    "        results_per_county = []\n",
    "        for county in nyt_counties:\n",
    "            try:\n",
    "                results_per_county.append(county2scores[county][mode])\n",
    "            except Exception as e:\n",
    "                print(e, county, \" huan\")\n",
    "\n",
    "        results_per_county = np.array(results_per_county)\n",
    "                # nyt_counties.remove(county)  # Huan\n",
    "        model_summary['unweighted_avg_%s' % mode] = np.mean(results_per_county)\n",
    "        model_summary['weighted_avg_%s' % mode] = np.sum(pop_sizes_per_county * results_per_county) / np.sum(pop_sizes_per_county)\n",
    "        model_summary['max_%s' % mode] = np.max(results_per_county)\n",
    "    return model_summary, county2scores, county2preds\n",
    "\n",
    "def make_per_county_plot(nyt_outcomes, county2preds, axes,\n",
    "                         plot_mode='cases', plot_daily_not_cumulative=True, plot_per_capita=True):\n",
    "    nyt_counties = nyt_outcomes.fips.unique()\n",
    "    assert len(axes) >= len(nyt_counties), 'only provided %d axes for %d counties' % (len(axes), len(nyt_counties))\n",
    "    county_and_pop_size = [(c, county2preds[c][plot_mode][2]) for c in nyt_counties]\n",
    "    sorted_counties = [t[0] for t in sorted(county_and_pop_size, key=lambda x:x[1], reverse=True)]\n",
    "    states_to_codes = {codes_to_states[c]:c for c in codes_to_states}\n",
    "    states_to_codes['District of Columbia'] = 'DC'\n",
    "    for i, county in enumerate(sorted_counties):\n",
    "        county_outcomes = nyt_outcomes[nyt_outcomes.fips == county]\n",
    "        county_name = '%s, %s' % (county_outcomes.iloc[0]['county'], states_to_codes[county_outcomes.iloc[0]['state']])\n",
    "        real_dates, real_cases, real_deaths = get_datetimes_and_totals_from_nyt_outcomes(county_outcomes)\n",
    "        if plot_mode == 'cases':\n",
    "            real_data = real_cases\n",
    "            projected_hrs, mdl_prediction, pop_size = county2preds[county]['cases']\n",
    "        else:\n",
    "            real_data = real_deaths\n",
    "            projected_hrs, mdl_prediction, pop_size = county2preds[county]['deaths']\n",
    "        if plot_daily_not_cumulative:\n",
    "            mdl_prediction = get_daily_from_cumulative(mdl_prediction)  # new counts per hour\n",
    "            # can't get deltas for first hour, so assume it's the same as the second hour\n",
    "            mdl_prediction[:, 0] = mdl_prediction[:, 1]  \n",
    "            num_days = int(mdl_prediction.shape[1] / 24)\n",
    "            daily_mdl_prediction = np.zeros((len(mdl_prediction), num_days))\n",
    "            new_projected_hrs = []\n",
    "            for t, (hr, prediction) in enumerate(zip(projected_hrs, mdl_prediction.T)):\n",
    "                if hr.hour == 0:\n",
    "                    new_projected_hrs.append(hr)\n",
    "                d = int(t/24)\n",
    "                daily_mdl_prediction[:, d] = daily_mdl_prediction[:, d] + prediction  # 1 x S \n",
    "            projected_hrs = new_projected_hrs \n",
    "            mdl_prediction = daily_mdl_prediction\n",
    "            \n",
    "            if plot_mode == 'cases':\n",
    "                daily_IR = mdl_prediction / DETECTION_RATE  # convert back to daily IR\n",
    "                eval_start_infectious = projected_hrs[0] + datetime.timedelta(days=-DETECTION_LAG)\n",
    "                eval_end_infectious = projected_hrs[-1] + datetime.timedelta(days=-DETECTION_LAG)\n",
    "                _, daily_cdr = helper.get_daily_case_detection_rate(min_datetime=eval_start_infectious, max_datetime=eval_end_infectious)\n",
    "                assert len(daily_cdr) == daily_IR.shape[1]\n",
    "                mdl_prediction = daily_IR * daily_cdr\n",
    "            real_data = get_daily_from_cumulative(real_data)\n",
    "        if plot_per_capita:\n",
    "            mdl_prediction = INCIDENCE_POP * mdl_prediction / pop_size\n",
    "            real_data = INCIDENCE_POP * real_data / pop_size\n",
    "        mean, lower_CI, upper_CI = mean_and_CIs_of_timeseries_matrix(mdl_prediction)\n",
    "        \n",
    "        ax = axes[i]\n",
    "        print(min(projected_hrs), min(real_dates))\n",
    "        ax.plot_date(projected_hrs, mean, linestyle='-', label='Model predictions', c='tab:blue')\n",
    "        ax.fill_between(projected_hrs, lower_CI, upper_CI, alpha=.5, color='tab:blue')\n",
    "        kept_idx = [i for i in np.arange(len(real_dates)) if real_dates[i] >= projected_hrs[0] and real_dates[i] <= projected_hrs[-1]]\n",
    "        real_dates = [real_dates[i] for i in kept_idx]\n",
    "        real_data = [real_data[i] for i in kept_idx]\n",
    "        ax.plot_date(real_dates, real_data, marker='x', c='grey', alpha=0.8, markeredgewidth=2)       \n",
    "        smoothed_real_data = apply_smoothing(real_data, before=6, after=6)\n",
    "        ax.plot_date(real_dates, smoothed_real_data, linestyle='-',\n",
    "                     label='Reported %s' % plot_mode, c='tab:orange')\n",
    "        ax.set_title('%s\\n[total pop=%dk]' % (county_name, int(round(pop_size / 1000, 0))), fontsize=16)\n",
    "        interval = int(len(projected_hrs) / 4)\n",
    "        ax.xaxis.set_major_locator(mdates.DayLocator(interval=interval))\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))\n",
    "        ax.tick_params(labelsize=12)\n",
    "        if i == 0:\n",
    "            ax.legend(loc='upper left', fontsize=14)\n",
    "                        \n",
    "def draw_cases_and_deaths_from_exponential_distribution(model_IR, detection_rate, detection_lag_in_days,\n",
    "                                                        death_rate, death_lag_in_days, random_seed=0):\n",
    "    # model_IR should be a matrix of seed x hour, where each entry represents the *cumulative* number\n",
    "    # of people in infectious or removed for that seed and hour\n",
    "    # eg mdl_IR = (model.history['nyt']['infected'] + model.history['nyt']['removed'])\n",
    "    np.random.seed(random_seed)\n",
    "    detection_lag = detection_lag_in_days * 24  # want the lags in hours\n",
    "    death_lag = death_lag_in_days * 24\n",
    "    num_seeds, num_hours = model_IR.shape\n",
    "    assert num_hours % 24 == 0\n",
    "    hourly_new_infectious = get_daily_from_cumulative(model_IR)\n",
    "\n",
    "    predicted_cases = np.zeros((num_seeds, num_hours))\n",
    "    predicted_deaths = np.zeros((num_seeds, num_hours))\n",
    "    cases_to_confirm = np.zeros(num_seeds)\n",
    "    deaths_to_happen = np.zeros(num_seeds)\n",
    "    for hr in range(num_hours):\n",
    "        new_infectious = hourly_new_infectious[:, hr]\n",
    "        new_confirmed_cases = np.random.binomial(cases_to_confirm.astype(int), 1/detection_lag)\n",
    "        predicted_cases[:, hr] = new_confirmed_cases\n",
    "        new_cases_to_confirm = np.random.binomial(new_infectious.astype(int), detection_rate)\n",
    "        cases_to_confirm = cases_to_confirm + new_cases_to_confirm - new_confirmed_cases\n",
    "        new_deaths = np.random.binomial(deaths_to_happen.astype(int), 1/death_lag)\n",
    "        predicted_deaths[:, hr] = new_deaths\n",
    "        new_deaths_to_happen = np.random.binomial(new_infectious.astype(int), death_rate)\n",
    "        deaths_to_happen = deaths_to_happen + new_deaths_to_happen - new_deaths\n",
    "    return predicted_cases, predicted_deaths\n",
    "\n",
    "def draw_cases_and_deaths_from_gamma_distribution(model_IR, detection_rate, death_rate,\n",
    "                                                  detection_delay_shape=1.85,  # Li et al. (Science 2020)\n",
    "                                                  detection_delay_scale=3.57,\n",
    "                                                  death_delay_shape=1.85,\n",
    "                                                  death_delay_scale=9.72,\n",
    "                                                  random_seed=0):\n",
    "    # model_IR should be a matrix of seed x hour, where each entry represents the *cumulative* number\n",
    "    # of people in infectious or removed for that seed and hour\n",
    "    # eg mdl_IR = (model.history['nyt']['infected'] + model.history['nyt']['removed'])\n",
    "    np.random.seed(random_seed)\n",
    "    num_seeds, num_hours = model_IR.shape\n",
    "    assert num_hours % 24 == 0\n",
    "    hourly_new_infectious = get_daily_from_cumulative(model_IR)\n",
    "\n",
    "    predicted_cases = np.zeros((num_seeds, num_hours))\n",
    "    predicted_deaths = np.zeros((num_seeds, num_hours))\n",
    "    for hr in range(num_hours):\n",
    "        new_infectious = hourly_new_infectious[:, hr]  # 1 x S\n",
    "        cases_to_confirm = np.random.binomial(new_infectious.astype(int), detection_rate)\n",
    "        deaths_to_happen = np.random.binomial(new_infectious.astype(int), death_rate)\n",
    "        for seed in range(num_seeds):\n",
    "            num_cases = cases_to_confirm[seed]\n",
    "            confirmation_delays = np.random.gamma(detection_delay_shape, detection_delay_scale, int(num_cases))\n",
    "            confirmation_delays = confirmation_delays * 24  # convert delays from days to hours\n",
    "            counts = Counter(confirmation_delays).most_common()\n",
    "            for delay, count in counts:\n",
    "                projected_hr = int(hr + delay)\n",
    "                if projected_hr < num_hours:\n",
    "                    predicted_cases[seed, projected_hr] = predicted_cases[seed, projected_hr] + count\n",
    "\n",
    "            num_deaths = deaths_to_happen[seed]\n",
    "            death_delays = np.random.gamma(death_delay_shape, death_delay_scale, int(num_deaths))\n",
    "            death_delays = death_delays * 24  # convert delays from days to hours\n",
    "            counts = Counter(death_delays).most_common()\n",
    "            for delay, count in counts:\n",
    "                projected_hr = int(hr + delay)\n",
    "                if projected_hr < num_hours:\n",
    "                    predicted_deaths[seed, projected_hr] = predicted_deaths[seed, projected_hr] + count\n",
    "    return predicted_cases, predicted_deaths\n",
    "\n",
    "def compute_loss(y_true, y_pred, rate=None,\n",
    "                 metric='RMSE',\n",
    "                 min_threshold=None,\n",
    "                 compare_daily_not_cumulative=True,\n",
    "                 do_logsumexp=False):\n",
    "    \"\"\"\n",
    "    This assumes that y_true and y_pred are cumulative counts.\n",
    "    y_true: 1D array, the true case/death counts\n",
    "    y_pred: 2D array, the predicted case/death counts over all seeds\n",
    "    rate: the detection or death rate used in computing y_pred;\n",
    "          only required when metric is poisson_NLL\n",
    "    metric: RMSE or MRE, the loss metric\n",
    "    min_threshold: the minimum number of true case/deaths that a day must have\n",
    "                   to be included in eval\n",
    "    compare_daily_not_cumulative: converts y_true and y_pred into daily counts\n",
    "                                  and does the comparison on those instead\n",
    "    do_logsumexp: whether to sum or logsumexp over seeds for LL metrics\n",
    "    \"\"\"\n",
    "    assert metric in {\n",
    "        'RMSE',\n",
    "        'MRE',\n",
    "        'MSE',\n",
    "        'poisson_NLL'}\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    if compare_daily_not_cumulative:\n",
    "        # we leave out the first entry when converting from cumulative to daily because we don't know\n",
    "        # if the first entry is an accumulation from multiple prior days\n",
    "        y_true = get_daily_from_cumulative(y_true)[1:]\n",
    "        y_pred = get_daily_from_cumulative(y_pred)[:, 1:]\n",
    "        assert len(y_true) == y_pred.shape[1]\n",
    "    else:\n",
    "        assert metric not in ['poisson_NLL']\n",
    "\n",
    "    if do_logsumexp:\n",
    "        sum_or_logsumexp = logsumexp\n",
    "    else:\n",
    "        sum_or_logsumexp = np.sum\n",
    "\n",
    "    if min_threshold is not None:\n",
    "        orig_len = len(y_true)\n",
    "        idxs = y_true >= min_threshold\n",
    "        if not idxs.sum() > 0:\n",
    "            print(y_true)\n",
    "            print(\"Warning: NOT ENOUGH VALUES ABOVE THRESHOLD %s\" % min_threshold)\n",
    "            return np.nan\n",
    "        y_true = y_true[idxs]\n",
    "        y_pred = y_pred[:, idxs]\n",
    "        num_dropped = orig_len - len(y_true)\n",
    "        if num_dropped > 30:\n",
    "            print('Warning: dropped %d dates after applying min_threshold %d' % (num_dropped, min_threshold))\n",
    "\n",
    "    if metric == 'RMSE':\n",
    "        return RMSE(y_true=y_true, y_pred=y_pred)\n",
    "    elif metric == 'MRE':\n",
    "        return MRE(y_true=y_true, y_pred=y_pred)\n",
    "    elif metric == 'MSE':\n",
    "        return MSE(y_true=y_true, y_pred=y_pred)\n",
    "    elif metric == 'poisson_NLL':\n",
    "        return poisson_NLL(\n",
    "            y_true=y_true,\n",
    "            y_pred=y_pred,\n",
    "            sum_or_logsumexp=sum_or_logsumexp)\n",
    "\n",
    "def evaluate_all_fitted_models_for_msa(msa_name, min_timestring=None,\n",
    "                                        max_timestring=None,\n",
    "                                        timestrings=None,\n",
    "                                       required_properties=None,\n",
    "                                       required_model_kwargs=None,\n",
    "                                       recompute_losses=False,\n",
    "                                       key_to_sort_by=None,\n",
    "                                       old_directory=False):\n",
    "\n",
    "    \"\"\"\n",
    "    required_properties refers to params that are defined in data_and_model_kwargs, outside of ‘model_kwargs’ and ‘data_kwargs`\n",
    "    E.g., required_properties={'experiment_to_run':'normal_grid_search'})\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    pd.set_option('max_columns', 50)\n",
    "    pd.set_option('display.width', 500)\n",
    "\n",
    "    if required_model_kwargs is None:\n",
    "        required_model_kwargs = {}\n",
    "    if required_properties is None:\n",
    "        required_properties = {}\n",
    "\n",
    "    if timestrings is None:\n",
    "        timestrings = filter_timestrings_for_properties(\n",
    "            required_properties=required_properties,\n",
    "            required_model_kwargs=required_model_kwargs,\n",
    "            required_data_kwargs={'MSA_name':msa_name},\n",
    "            min_timestring=min_timestring,\n",
    "            max_timestring=max_timestring,\n",
    "            old_directory=old_directory)\n",
    "        print('Found %d fitted models for %s' % (len(timestrings), msa_name))\n",
    "    else:\n",
    "        # sometimes we may wish to pass in a list of timestrings to evaluate models\n",
    "        # so we don't have to call filter_timestrings_for_properties a lot.\n",
    "        assert min_timestring is None\n",
    "        assert max_timestring is None\n",
    "        assert required_model_kwargs == {}\n",
    "\n",
    "    if recompute_losses:\n",
    "        nyt_outcomes, _, _, _, _ = get_variables_for_evaluating_msa_model(msa_name)\n",
    "\n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "    for ts in timestrings:\n",
    "        _, kwargs, _, model_results, fast_to_load_results = load_model_and_data_from_timestring(ts,\n",
    "            verbose=False,\n",
    "            load_fast_results_only=(not recompute_losses), old_directory=old_directory)\n",
    "        model_kwargs = kwargs['model_kwargs']\n",
    "        exo_kwargs = model_kwargs['exogenous_model_kwargs']\n",
    "        data_kwargs = kwargs['data_kwargs']\n",
    "        experiment_to_run = kwargs['experiment_to_run']\n",
    "        assert data_kwargs['MSA_name'] == msa_name\n",
    "\n",
    "        if recompute_losses:\n",
    "            fast_to_load_results['loss_dict'] = compare_model_vs_real_num_cases(nyt_outcomes,\n",
    "                                                   model_kwargs['min_datetime'],\n",
    "                                                   model_results=model_results,\n",
    "                                                   make_plot=False)\n",
    "\n",
    "        results_for_ts = {'timestring':ts,\n",
    "                         'data_kwargs':data_kwargs,\n",
    "                         'model_kwargs':model_kwargs,\n",
    "                         'results':model_results,\n",
    "                         'experiment_to_run':experiment_to_run}\n",
    "\n",
    "        if 'final infected fraction' in fast_to_load_results:\n",
    "            results_for_ts['final infected fraction'] = fast_to_load_results['final infected fraction']\n",
    "\n",
    "        for result_type in ['loss_dict', 'train_loss_dict', 'test_loss_dict', 'ses_race_summary_results', 'estimated_R0', 'clipping_monitor', 'agg_county_loss_dict']:\n",
    "            if (result_type in fast_to_load_results) and (fast_to_load_results[result_type] is not None):\n",
    "                for k in fast_to_load_results[result_type]:\n",
    "                    full_key = result_type + '_' + k\n",
    "                    assert full_key not in results_for_ts\n",
    "                    results_for_ts[full_key] = fast_to_load_results[result_type][k]\n",
    "\n",
    "        for k in exo_kwargs:\n",
    "            assert k not in results_for_ts\n",
    "            results_for_ts[k] = exo_kwargs[k]\n",
    "        for k in model_kwargs:\n",
    "            if k == 'exogenous_model_kwargs':\n",
    "                continue\n",
    "            else:\n",
    "                assert k not in results_for_ts\n",
    "                results_for_ts[k] = model_kwargs[k]\n",
    "        results.append(results_for_ts)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print('Time to load and score all models: %.3fs -> %.3fs per model' %\n",
    "          (end_time-start_time, (end_time-start_time)/len(timestrings)))\n",
    "    results = pd.DataFrame(results)\n",
    "\n",
    "    if key_to_sort_by is not None:\n",
    "        results = results.sort_values(by=key_to_sort_by)\n",
    "    return results\n",
    "\n",
    "def evaluate_all_fitted_models_for_experiment(experiment_to_run,\n",
    "                                              min_timestring=None,\n",
    "                                              max_timestring=None,\n",
    "                                              timestrings=None,\n",
    "                                              required_properties=None,\n",
    "                                              required_model_kwargs=None,\n",
    "                                              required_data_kwargs=None,\n",
    "                                              result_types=None,\n",
    "                                              key_to_sort_by=None,\n",
    "                                              old_directory=False):\n",
    "\n",
    "    \"\"\"\n",
    "    required_properties refers to params that are defined in data_and_model_kwargs, outside of ‘model_kwargs’ and ‘data_kwargs`\n",
    "    \"\"\"\n",
    "    if required_properties is None:\n",
    "        required_properties = {}\n",
    "    required_properties['experiment_to_run'] = experiment_to_run\n",
    "    if required_model_kwargs is None:\n",
    "        required_model_kwargs = {}\n",
    "    if required_data_kwargs is None:\n",
    "        required_data_kwargs = {}\n",
    "\n",
    "    if timestrings is None:\n",
    "        timestrings = filter_timestrings_for_properties(\n",
    "            required_properties=required_properties,\n",
    "            required_model_kwargs=required_model_kwargs,\n",
    "            required_data_kwargs=required_data_kwargs,\n",
    "            min_timestring=min_timestring,\n",
    "            max_timestring=max_timestring,\n",
    "            old_directory=old_directory)\n",
    "        print('Found %d fitted models for %s' % (len(timestrings), experiment_to_run))\n",
    "    else:\n",
    "        # sometimes we may wish to pass in a list of timestrings to evaluate models\n",
    "        # so we don't have to call filter_timestrings_for_properties a lot.\n",
    "        assert min_timestring is None\n",
    "        assert max_timestring is None\n",
    "        assert required_model_kwargs == {}\n",
    "\n",
    "    if result_types is None:\n",
    "        result_types = ['loss_dict', 'train_loss_dict', 'test_loss_dict', 'agg_county_loss_dict']\n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "    for i, ts in enumerate(timestrings):\n",
    "        _, kwargs, _, model_results, fast_to_load_results = load_model_and_data_from_timestring(ts,\n",
    "            verbose=False, load_fast_results_only=True, old_directory=old_directory)\n",
    "        model_kwargs = kwargs['model_kwargs']\n",
    "        exo_kwargs = model_kwargs['exogenous_model_kwargs']\n",
    "        data_kwargs = kwargs['data_kwargs']\n",
    "        experiment_to_run = kwargs['experiment_to_run']\n",
    "\n",
    "        results_for_ts = {'timestring':ts,\n",
    "                         'data_kwargs':data_kwargs,\n",
    "                         'model_kwargs':model_kwargs,\n",
    "                         'results':model_results,\n",
    "                         'experiment_to_run':experiment_to_run}\n",
    "\n",
    "        if 'final infected fraction' in fast_to_load_results:\n",
    "            results_for_ts['final infected fraction'] = fast_to_load_results['final infected fraction']\n",
    "\n",
    "        for result_type in result_types:\n",
    "            if (result_type in fast_to_load_results) and (fast_to_load_results[result_type] is not None):\n",
    "                for k in fast_to_load_results[result_type]:\n",
    "                    full_key = result_type + '_' + k\n",
    "                    assert full_key not in results_for_ts\n",
    "                    results_for_ts[full_key] = fast_to_load_results[result_type][k]\n",
    "\n",
    "        for k in exo_kwargs:\n",
    "            assert k not in results_for_ts\n",
    "            results_for_ts[k] = exo_kwargs[k]\n",
    "        for k in model_kwargs:\n",
    "            if k == 'exogenous_model_kwargs':\n",
    "                continue\n",
    "            else:\n",
    "                assert k not in results_for_ts\n",
    "                results_for_ts[k] = model_kwargs[k]\n",
    "        results.append(results_for_ts)\n",
    "        if i % 1000 == 0:\n",
    "            curr_time = time.time()\n",
    "            print('Loaded %d models so far: %.3fs -> %.3fs per model' %\n",
    "                  (len(results), curr_time-start_time, (curr_time-start_time)/len(results)))\n",
    "\n",
    "    end_time = time.time()\n",
    "    print('Time to load and score all models: %.3fs -> %.3fs per model' %\n",
    "          (end_time-start_time, (end_time-start_time)/len(timestrings)))\n",
    "    results = pd.DataFrame(results)\n",
    "\n",
    "    if key_to_sort_by is not None:\n",
    "        results = results.sort_values(by=key_to_sort_by)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eafb673-57f3-46cf-b5aa-d11c9a1fe82a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Google_street_view",
   "language": "python",
   "name": "google_street_view"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
